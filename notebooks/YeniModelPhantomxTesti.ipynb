{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU0LXWTZI1St"
      },
      "source": [
        "### Generator (U Net) and Discriminator (Critic). This time we are NOT using the Sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuyDVrxlJ6BC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "CHANNELS = 1\n",
        "\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "    if apply_batchnorm:\n",
        "        result.add(layers.BatchNormalization())\n",
        "    result.add(layers.LeakyReLU())\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
        "                                      kernel_initializer=initializer, use_bias=False))\n",
        "    result.add(layers.BatchNormalization())\n",
        "    if apply_dropout:\n",
        "        result.add(layers.Dropout(0.5))\n",
        "    result.add(layers.ReLU())\n",
        "    return result\n",
        "\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, CHANNELS])\n",
        "\n",
        "    # Encoder\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "        downsample(128, 4), # (bs, 64, 64, 128)\n",
        "        downsample(256, 4), # (bs, 32, 32, 256)\n",
        "        downsample(512, 4), # (bs, 16, 16, 512)\n",
        "        downsample(512, 4), # (bs, 8, 8, 512)\n",
        "        downsample(512, 4), # (bs, 4, 4, 512)\n",
        "        downsample(512, 4), # (bs, 2, 2, 512)\n",
        "        downsample(512, 4), # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    # Decoder\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4),\n",
        "        upsample(256, 4),\n",
        "        upsample(128, 4),\n",
        "        upsample(64, 4),\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = layers.Conv2DTranspose(CHANNELS, 4, strides=2, padding='same',\n",
        "                                  kernel_initializer=initializer, activation='tanh')\n",
        "\n",
        "    x = inputs\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "    return keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "def build_discriminator():\n",
        "    #   Sigmoid fonksiyonu yerine WGAN-GP kullanÄ±lmÄ±ÅŸtÄ±r.\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    inp = layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, CHANNELS], name='input_image')\n",
        "    tar = layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, CHANNELS], name='target_image')\n",
        "\n",
        "    x = layers.Concatenate()([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "    down1 = downsample(64, 4, False)(x)\n",
        "    down2 = downsample(128, 4)(down1)\n",
        "    down3 = downsample(256, 4)(down2)\n",
        "\n",
        "    # Zero Padding ve Conv\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3)\n",
        "    conv = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)\n",
        "    batchnorm1 = layers.BatchNormalization()(conv)\n",
        "    leaky_relu = layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n",
        "\n",
        "    last = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)\n",
        "\n",
        "    return keras.Model(inputs=[inp, tar], outputs=last)\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdhHODSPJ9FO"
      },
      "source": [
        "### Hybrid Model Class (Pix2Pix + WGAN GP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9-eAhKMKGiv"
      },
      "outputs": [],
      "source": [
        "class WGAN_GP_Pix2Pix(keras.Model):\n",
        "    def __init__(self, generator, discriminator, lambda_gp=10.0, lambda_l1=100.0):\n",
        "        super(WGAN_GP_Pix2Pix, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.lambda_gp = lambda_gp # Gradient Penalty aÄŸÄ±rlÄ±ÄŸÄ±\n",
        "        self.lambda_l1 = lambda_l1 # L1 (Pix2Pix) aÄŸÄ±rlÄ±ÄŸÄ±\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer):\n",
        "        super(WGAN_GP_Pix2Pix, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_loss_fn = self.wasserstein_loss\n",
        "        self.g_loss_fn = self.wasserstein_loss\n",
        "        self.l1_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "    def wasserstein_loss(self, y_true, y_pred):\n",
        "        return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images, input_images):\n",
        "        \"\"\" GP Hesaplama: Real ve Fake arasÄ± interpolasyon \"\"\"\n",
        "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "        diff = fake_images - real_images\n",
        "        interpolated = real_images + alpha * diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            # Discriminator'a hem input(LD) hem interpolasyon verilir\n",
        "            pred = self.discriminator([input_images, interpolated], training=True)\n",
        "\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        return gp\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        if isinstance(inputs, (list, tuple)):\n",
        "            inputs = inputs[0]\n",
        "        return self.generator(inputs, training=training)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Data Loader'dan gelen veri: (input_image, target_image)\n",
        "        input_image, target_image = data\n",
        "        batch_size = tf.shape(input_image)[0]\n",
        "\n",
        "        # --- DISCRIMINATOR EÄÄ°TÄ°MÄ° ---\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_image = self.generator(input_image, training=True)\n",
        "\n",
        "            fake_pred = self.discriminator([input_image, fake_image], training=True)\n",
        "            real_pred = self.discriminator([input_image, target_image], training=True)\n",
        "\n",
        "            # Wasserstein Loss: D(fake) - D(real)\n",
        "\n",
        "            # Not: Real iÃ§in -1, Fake iÃ§in 1 gibi davranÄ±lÄ±r, formÃ¼l minimize etmek Ã¼zerine kuruludur.\n",
        "\n",
        "            d_cost = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred)\n",
        "\n",
        "            # Gradient Penalty\n",
        "            gp = self.gradient_penalty(batch_size, target_image, fake_image, input_image)\n",
        "\n",
        "            # Toplam D Loss\n",
        "            d_loss = d_cost + (gp * self.lambda_gp)\n",
        "\n",
        "        d_grad = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_optimizer.apply_gradients(zip(d_grad, self.discriminator.trainable_variables))\n",
        "\n",
        "        # --- GENERATOR EÄÄ°TÄ°MÄ° ---\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_image = self.generator(input_image, training=True)\n",
        "            fake_pred = self.discriminator([input_image, fake_image], training=True)\n",
        "\n",
        "            # G Loss (Wasserstein KÄ±smÄ±)\n",
        "            g_wgan_loss = -tf.reduce_mean(fake_pred)\n",
        "\n",
        "            # G Loss (L1 KÄ±smÄ±): Orijinal Pix2Pix yapÄ±sÄ± (GÃ¶rÃ¼ntÃ¼ benzerliÄŸi)\n",
        "            g_l1_loss = self.l1_loss_fn(target_image, fake_image) * self.lambda_l1\n",
        "\n",
        "            g_loss = g_wgan_loss + g_l1_loss\n",
        "\n",
        "        g_grad = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        self.g_optimizer.apply_gradients(zip(g_grad, self.generator.trainable_variables))\n",
        "\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"g_l1\": g_l1_loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Model with New Dataset"
      ],
      "metadata": {
        "id": "JZdOKuJ7Sk18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Huwz6AgXhpPe",
        "outputId": "d0cc5628-85eb-458d-f08b-54fc9feec77c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m\u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.4/2.4 MB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m39.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pydicom\n",
        "from pathlib import Path\n",
        "\n",
        "# ==================== AYARLAR ====================\n",
        "phantomx_base_dir = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/phantomx_abdomen_pelvis_dataset\"\n",
        "output_dir = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output3\"\n",
        "\n",
        "phantoms = [\"D55-01\", \"D55-02\"]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PhantomX DICOM â†’ NPY DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ (3 Seviyeli YapÄ±)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"ğŸ“‚ Ana dizin: {phantomx_base_dir}\")\n",
        "print(f\"ğŸ“‚ Ã‡Ä±ktÄ± dizini: {output_dir}\")\n",
        "\n",
        "# Dizin kontrolÃ¼\n",
        "if not os.path.exists(phantomx_base_dir):\n",
        "    print(f\"\\nâŒ HATA: Ana dizin bulunamadÄ±!\")\n",
        "    print(f\"   Kontrol edin: {phantomx_base_dir}\")\n",
        "    exit()\n",
        "\n",
        "# ==================== KLASÃ–R YAPISINI ANLA ====================\n",
        "\n",
        "def klasor_yapisini_incele(base_dir, phantoms):\n",
        "    \"\"\"3 seviyeli klasÃ¶r yapÄ±sÄ±nÄ± inceler\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ“‹ KLASÃ–R YAPISI ANALÄ°ZÄ°\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    klasor_bilgileri = {}\n",
        "\n",
        "    for phantom in phantoms:\n",
        "        phantom_yolu = os.path.join(base_dir, phantom)\n",
        "\n",
        "        if not os.path.exists(phantom_yolu):\n",
        "            print(f\"\\nâŒ {phantom} klasÃ¶rÃ¼ bulunamadÄ±!\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'â”€'*80}\")\n",
        "        print(f\"ğŸ”¬ Phantom: {phantom}\")\n",
        "        print(f\"{'â”€'*80}\")\n",
        "\n",
        "        # 1. Seviye: Doz klasÃ¶rleri (40, 300)\n",
        "        doz_klasorleri = {}\n",
        "\n",
        "        for doz_adi in sorted(os.listdir(phantom_yolu)):\n",
        "            doz_yolu = os.path.join(phantom_yolu, doz_adi)\n",
        "\n",
        "            if not os.path.isdir(doz_yolu):\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n  ğŸ“ Doz KlasÃ¶rÃ¼: {doz_adi}\")\n",
        "\n",
        "            # Doz bilgisini belirle\n",
        "            if doz_adi == \"40\":\n",
        "                doz_tipi = \"1.4 CTDIvol (DÃ¼ÅŸÃ¼k Doz)\"\n",
        "            elif doz_adi == \"300\":\n",
        "                doz_tipi = \"10.5 CTDIvol (YÃ¼ksek Doz)\"\n",
        "            else:\n",
        "                doz_tipi = \"Bilinmeyen\"\n",
        "\n",
        "            print(f\"      â†’ {doz_tipi}\")\n",
        "\n",
        "            # 2. Seviye: RekonstrÃ¼ksiyon metodu klasÃ¶rleri\n",
        "            rekon_klasorleri = []\n",
        "\n",
        "            for rekon_adi in sorted(os.listdir(doz_yolu)):\n",
        "                rekon_yolu = os.path.join(doz_yolu, rekon_adi)\n",
        "\n",
        "                if not os.path.isdir(rekon_yolu):\n",
        "                    continue\n",
        "\n",
        "                # DICOM sayÄ±sÄ±nÄ± say\n",
        "                dicom_sayisi = len([f for f in os.listdir(rekon_yolu) if f.endswith('.dcm')])\n",
        "\n",
        "                # RekonstrÃ¼ksiyon tipini belirle\n",
        "                if \"FBP\" in rekon_adi.upper():\n",
        "                    rekon_tip = \"FBP (Filtered Back Projection)\"\n",
        "                elif \"AIDR\" in rekon_adi.upper():\n",
        "                    rekon_tip = \"AIDR3D (Iterative Reconstruction)\"\n",
        "                elif \"AICE\" in rekon_adi.upper() or \"ICE\" in rekon_adi.upper():\n",
        "                    rekon_tip = \"AiCE (AI-based Reconstruction)\"\n",
        "                else:\n",
        "                    rekon_tip = \"Bilinmeyen\"\n",
        "\n",
        "                rekon_klasorleri.append({\n",
        "                    'ad': rekon_adi,\n",
        "                    'yol': rekon_yolu,\n",
        "                    'tip': rekon_tip,\n",
        "                    'dicom_sayisi': dicom_sayisi\n",
        "                })\n",
        "\n",
        "                print(f\"      â””â”€â”€ {rekon_tip}\")\n",
        "                print(f\"          KlasÃ¶r: {rekon_adi}\")\n",
        "                print(f\"          DICOM: {dicom_sayisi} dosya\")\n",
        "\n",
        "            doz_klasorleri[doz_adi] = {\n",
        "                'yol': doz_yolu,\n",
        "                'tip': doz_tipi,\n",
        "                'rekonstruksiyonlar': rekon_klasorleri\n",
        "            }\n",
        "\n",
        "        klasor_bilgileri[phantom] = doz_klasorleri\n",
        "\n",
        "    return klasor_bilgileri\n",
        "\n",
        "# KlasÃ¶r yapÄ±sÄ±nÄ± incele\n",
        "klasor_bilgileri = klasor_yapisini_incele(phantomx_base_dir, phantoms)\n",
        "\n",
        "if not klasor_bilgileri:\n",
        "    print(\"\\nâŒ HiÃ§bir phantom klasÃ¶rÃ¼ bulunamadÄ±!\")\n",
        "    exit()\n",
        "\n",
        "# ==================== KULLANICIDAN SEÃ‡Ä°M AL ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ¯ REKONSTRÃœKSÄ°YON METODU SEÃ‡Ä°MÄ°\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nâš ï¸  Ã–NERÄ°: FBP metodu seÃ§in (model eÄŸitiminde kullanÄ±ldÄ±ysa)\")\n",
        "print(\"   â€¢ FBP: Standart, ham gÃ¼rÃ¼ltÃ¼ iÃ§erir (en yaygÄ±n)\")\n",
        "print(\"   â€¢ AIDR3D: Iterative, kÄ±smen gÃ¼rÃ¼ltÃ¼ azaltÄ±lmÄ±ÅŸ\")\n",
        "print(\"   â€¢ AiCE: AI-based, yoÄŸun gÃ¼rÃ¼ltÃ¼ azaltma yapÄ±lmÄ±ÅŸ\")\n",
        "\n",
        "# Hangi rekonstrÃ¼ksiyon metodunu kullanacaÄŸÄ±z?\n",
        "print(\"\\nHangi rekonstrÃ¼ksiyon metodunu kullanmak istersiniz?\")\n",
        "print(\"  [1] FBP (Ã–nerilen)\")\n",
        "print(\"  [2] AIDR3D\")\n",
        "print(\"  [3] AiCE\")\n",
        "\n",
        "rekon_secim = input(\"\\nSeÃ§iminiz (1/2/3): \").strip()\n",
        "\n",
        "rekon_map = {\n",
        "    \"1\": \"FBP\",\n",
        "    \"2\": \"AIDR\",\n",
        "    \"3\": \"AICE\"\n",
        "}\n",
        "\n",
        "if rekon_secim not in rekon_map:\n",
        "    print(\"âŒ GeÃ§ersiz seÃ§im! VarsayÄ±lan olarak FBP kullanÄ±lacak.\")\n",
        "    rekon_secim = \"1\"\n",
        "\n",
        "secilen_rekon = rekon_map[rekon_secim]\n",
        "print(f\"\\nâœ… SeÃ§ilen metod: {secilen_rekon}\")\n",
        "\n",
        "# ==================== FONKSÄ°YONLAR ====================\n",
        "\n",
        "def rekonstruksiyon_klasoru_bul(rekon_klasorleri, aranan_tip):\n",
        "    \"\"\"Belirtilen rekonstrÃ¼ksiyon tipine ait klasÃ¶rÃ¼ bulur\"\"\"\n",
        "    for klasor in rekon_klasorleri:\n",
        "        if aranan_tip.upper() in klasor['ad'].upper():\n",
        "            return klasor\n",
        "    return None\n",
        "\n",
        "def dicom_klasoru_oku(klasor_yolu, klasor_adi):\n",
        "    \"\"\"DICOM dosyalarÄ±nÄ± okur ve 3D numpy array olarak dÃ¶ndÃ¼rÃ¼r\"\"\"\n",
        "    print(f\"      ğŸ“‚ {klasor_adi}\")\n",
        "\n",
        "    dicom_dosyalari = []\n",
        "\n",
        "    for dosya in os.listdir(klasor_yolu):\n",
        "        if dosya.endswith('.dcm'):\n",
        "            dosya_yolu = os.path.join(klasor_yolu, dosya)\n",
        "            dicom_dosyalari.append(dosya_yolu)\n",
        "\n",
        "    if not dicom_dosyalari:\n",
        "        print(f\"      âŒ DICOM dosyasÄ± bulunamadÄ±!\")\n",
        "        return None\n",
        "\n",
        "    print(f\"      âœ“ {len(dicom_dosyalari)} DICOM dosyasÄ± bulundu\")\n",
        "\n",
        "    # DICOM dosyalarÄ±nÄ± oku ve sÄ±rala\n",
        "    dicom_veriler = []\n",
        "    hata_sayisi = 0\n",
        "\n",
        "    for dosya in sorted(dicom_dosyalari):\n",
        "        try:\n",
        "            ds = pydicom.dcmread(dosya)\n",
        "\n",
        "            # Pixel array'i al\n",
        "            pixel_array = ds.pixel_array.astype(np.float32)\n",
        "\n",
        "            # --- DÃœZELTME BAÅLANGIÃ‡: HU DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ---\n",
        "            intercept = ds.RescaleIntercept if 'RescaleIntercept' in ds else 0\n",
        "            slope = ds.RescaleSlope if 'RescaleSlope' in ds else 1\n",
        "\n",
        "            # Ham veriyi HU birimine Ã§evir\n",
        "            pixel_array = pixel_array * slope + intercept\n",
        "            # --- DÃœZELTME BÄ°TÄ°Å ---\n",
        "\n",
        "            # Sonra direkt clip ve normalize iÅŸlemlerine geÃ§\n",
        "            pixel_array = np.clip(pixel_array, -1000, 1000)\n",
        "\n",
        "            # Z koordinatÄ± veya InstanceNumber kullan\n",
        "            if hasattr(ds, 'ImagePositionPatient'):\n",
        "                slice_location = float(ds.ImagePositionPatient[2])\n",
        "            elif hasattr(ds, 'SliceLocation'):\n",
        "                slice_location = float(ds.SliceLocation)\n",
        "            elif hasattr(ds, 'InstanceNumber'):\n",
        "                slice_location = float(ds.InstanceNumber)\n",
        "            else:\n",
        "                slice_location = len(dicom_veriler)\n",
        "\n",
        "            dicom_veriler.append((slice_location, pixel_array))\n",
        "\n",
        "        except Exception as e:\n",
        "            hata_sayisi += 1\n",
        "            if hata_sayisi <= 3:\n",
        "                print(f\"      âš  Hata ({os.path.basename(dosya)}): {e}\")\n",
        "\n",
        "    if hata_sayisi > 3:\n",
        "        print(f\"      âš  Toplam {hata_sayisi} dosya okunamadÄ±\")\n",
        "\n",
        "    if not dicom_veriler:\n",
        "        print(f\"      âŒ HiÃ§bir DICOM dosyasÄ± okunamadÄ±!\")\n",
        "        return None\n",
        "\n",
        "    # Z koordinatÄ±na gÃ¶re sÄ±rala\n",
        "    dicom_veriler.sort(key=lambda x: x[0])\n",
        "\n",
        "    # 3D array oluÅŸtur\n",
        "    volume_3d = np.stack([slice_data for _, slice_data in dicom_veriler])\n",
        "\n",
        "    print(f\"      âœ“ Volume: {volume_3d.shape}\")\n",
        "    print(f\"      âœ“ HU aralÄ±ÄŸÄ±: [{volume_3d.min():.1f}, {volume_3d.max():.1f}]\")\n",
        "\n",
        "    return volume_3d\n",
        "\n",
        "def normalizasyon(volume, min_hu=-1000, max_hu=1000):\n",
        "    \"\"\"HU deÄŸerlerini [-1, 1] aralÄ±ÄŸÄ±na normalize eder\"\"\"\n",
        "    volume = np.clip(volume, min_hu, max_hu)\n",
        "    volume = (volume - min_hu) / (max_hu - min_hu)  # [0, 1]\n",
        "    volume = volume * 2 - 1  # [-1, 1]\n",
        "    return volume.astype(np.float32)\n",
        "\n",
        "# ==================== DÃ–NÃœÅTÃœRMEYÄ° BAÅLAT ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ”„ DÃ–NÃœÅTÃœRME Ä°ÅLEMÄ°\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "basarili_donusumler = []\n",
        "\n",
        "for phantom_name, doz_klasorleri in klasor_bilgileri.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ”¬ Phantom: {phantom_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # 40 = dÃ¼ÅŸÃ¼k doz (INPUT)\n",
        "    # 300 = yÃ¼ksek doz (TARGET)\n",
        "\n",
        "    if \"40\" not in doz_klasorleri or \"300\" not in doz_klasorleri:\n",
        "        print(f\"  âŒ Gerekli doz klasÃ¶rleri bulunamadÄ± (40 veya 300 eksik)\")\n",
        "        continue\n",
        "\n",
        "    # INPUT: 40 klasÃ¶rÃ¼nden seÃ§ilen rekonstrÃ¼ksiyon\n",
        "    input_rekonlar = doz_klasorleri[\"40\"][\"rekonstruksiyonlar\"]\n",
        "    input_rekon = rekonstruksiyon_klasoru_bul(input_rekonlar, secilen_rekon)\n",
        "\n",
        "    # TARGET: 300 klasÃ¶rÃ¼nden seÃ§ilen rekonstrÃ¼ksiyon\n",
        "    target_rekonlar = doz_klasorleri[\"300\"][\"rekonstruksiyonlar\"]\n",
        "    target_rekon = rekonstruksiyon_klasoru_bul(target_rekonlar, secilen_rekon)\n",
        "\n",
        "    if not input_rekon or not target_rekon:\n",
        "        print(f\"  âŒ {secilen_rekon} rekonstrÃ¼ksiyonu bulunamadÄ±!\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n  ğŸ“¥ INPUT (1.4 CTDIvol):\")\n",
        "    print(f\"     {input_rekon['ad']}\")\n",
        "    print(f\"  ğŸ“¤ TARGET (10.5 CTDIvol):\")\n",
        "    print(f\"     {target_rekon['ad']}\")\n",
        "\n",
        "    try:\n",
        "        # DICOM'larÄ± oku\n",
        "        print(f\"\\n  ğŸ“ DICOM dosyalarÄ± okunuyor...\")\n",
        "\n",
        "        print(f\"\\n    INPUT:\")\n",
        "        input_volume = dicom_klasoru_oku(input_rekon['yol'], input_rekon['ad'])\n",
        "\n",
        "        print(f\"\\n    TARGET:\")\n",
        "        target_volume = dicom_klasoru_oku(target_rekon['yol'], target_rekon['ad'])\n",
        "\n",
        "        if input_volume is None or target_volume is None:\n",
        "            print(f\"\\n  âŒ DICOM okuma baÅŸarÄ±sÄ±z!\")\n",
        "            continue\n",
        "\n",
        "        # Shape kontrolÃ¼\n",
        "        if input_volume.shape != target_volume.shape:\n",
        "            print(f\"\\n  âš  UYARI: Boyutlar farklÄ±!\")\n",
        "            print(f\"    Input: {input_volume.shape}\")\n",
        "            print(f\"    Target: {target_volume.shape}\")\n",
        "\n",
        "            min_slices = min(input_volume.shape[0], target_volume.shape[0])\n",
        "            min_height = min(input_volume.shape[1], target_volume.shape[1])\n",
        "            min_width = min(input_volume.shape[2], target_volume.shape[2])\n",
        "\n",
        "            input_volume = input_volume[:min_slices, :min_height, :min_width]\n",
        "            target_volume = target_volume[:min_slices, :min_height, :min_width]\n",
        "\n",
        "            print(f\"    â†’ Uyumlu hale getirildi: {input_volume.shape}\")\n",
        "\n",
        "        # Normalizasyon\n",
        "        print(f\"\\n  ğŸ”„ Normalizasyon: [-1000, 1000] HU â†’ [-1, 1]\")\n",
        "\n",
        "        input_normalized = normalizasyon(input_volume)\n",
        "        target_normalized = normalizasyon(target_volume)\n",
        "\n",
        "        print(f\"    âœ“ Input: [{input_normalized.min():.3f}, {input_normalized.max():.3f}]\")\n",
        "        print(f\"    âœ“ Target: [{target_normalized.min():.3f}, {target_normalized.max():.3f}]\")\n",
        "\n",
        "        # NPY olarak kaydet\n",
        "        print(f\"\\n  ğŸ’¾ Dosyalar kaydediliyor...\")\n",
        "        phantom_output = os.path.join(output_dir, phantom_name)\n",
        "        os.makedirs(phantom_output, exist_ok=True)\n",
        "\n",
        "        input_save_path = os.path.join(phantom_output, \"input_ldct.npy\")\n",
        "        target_save_path = os.path.join(phantom_output, \"target_ndct.npy\")\n",
        "\n",
        "        np.save(input_save_path, input_normalized)\n",
        "        np.save(target_save_path, target_normalized)\n",
        "\n",
        "        print(f\"    âœ“ {input_save_path}\")\n",
        "        print(f\"    âœ“ {target_save_path}\")\n",
        "\n",
        "        # Ä°statistikler\n",
        "        print(f\"\\n  ğŸ“Š Ä°statistikler:\")\n",
        "        print(f\"     Slice: {input_normalized.shape[0]}\")\n",
        "        print(f\"     Boyut: {input_normalized.shape[1]} x {input_normalized.shape[2]}\")\n",
        "        print(f\"     Dosya: {input_normalized.nbytes / (1024**2):.2f} MB (her biri)\")\n",
        "\n",
        "        basarili_donusumler.append(phantom_name)\n",
        "        print(f\"\\n  âœ… BaÅŸarÄ±lÄ±!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n  âŒ HATA: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ==================== Ã–ZET ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "if basarili_donusumler:\n",
        "    print(\"âœ… DÃ–NÃœÅTÃœRME TAMAMLANDI!\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nğŸ“ Ã‡Ä±ktÄ±: {output_dir}\\n\")\n",
        "    for phantom in basarili_donusumler:\n",
        "        print(f\"  {phantom}/\")\n",
        "        print(f\"    â”œâ”€â”€ input_ldct.npy  (1.4 CTDIvol - DÃ¼ÅŸÃ¼k Doz)\")\n",
        "        print(f\"    â””â”€â”€ target_ndct.npy (10.5 CTDIvol - YÃ¼ksek Doz)\")\n",
        "    print(f\"\\nğŸš€ Test kodunu Ã§alÄ±ÅŸtÄ±rabilirsiniz!\")\n",
        "else:\n",
        "    print(\"âŒ HÄ°Ã‡BÄ°R PHANTOM DÃ–NÃœÅTÃœRÃœLEMEDI!\")\n",
        "    print(\"=\"*80)\n",
        "print()"
      ],
      "metadata": {
        "id": "mzxjaNRUSsP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca38bd55-4a52-40f0-eb15-9c5ed225e42b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PhantomX DICOM â†’ NPY DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ (3 Seviyeli YapÄ±)\n",
            "================================================================================\n",
            "ğŸ“‚ Ana dizin: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/phantomx_abdomen_pelvis_dataset\n",
            "ğŸ“‚ Ã‡Ä±ktÄ± dizini: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output3\n",
            "\n",
            "================================================================================\n",
            "ğŸ“‹ KLASÃ–R YAPISI ANALÄ°ZÄ°\n",
            "================================================================================\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ”¬ Phantom: D55-01\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "  ğŸ“ Doz KlasÃ¶rÃ¼: 300\n",
            "      â†’ 10.5 CTDIvol (YÃ¼ksek Doz)\n",
            "      â””â”€â”€ AIDR3D (Iterative Reconstruction)\n",
            "          KlasÃ¶r: AIDR3D_FC08_300_171515.916\n",
            "          DICOM: 376 dosya\n",
            "      â””â”€â”€ AiCE (AI-based Reconstruction)\n",
            "          KlasÃ¶r: AiCE_BODY-SHARP_300_172938.900\n",
            "          DICOM: 376 dosya\n",
            "      â””â”€â”€ FBP (Filtered Back Projection)\n",
            "          KlasÃ¶r: FBP_FC08_300_171515.916\n",
            "          DICOM: 376 dosya\n",
            "\n",
            "  ğŸ“ Doz KlasÃ¶rÃ¼: 40\n",
            "      â†’ 1.4 CTDIvol (DÃ¼ÅŸÃ¼k Doz)\n",
            "      â””â”€â”€ AIDR3D (Iterative Reconstruction)\n",
            "          KlasÃ¶r: AIDR3D_FC08_40_152622.890\n",
            "          DICOM: 376 dosya\n",
            "      â””â”€â”€ AiCE (AI-based Reconstruction)\n",
            "          KlasÃ¶r: AiCE_BODY-SHARP_40_152622.890\n",
            "          DICOM: 376 dosya\n",
            "      â””â”€â”€ FBP (Filtered Back Projection)\n",
            "          KlasÃ¶r: FBP_FC08_40_152622.890\n",
            "          DICOM: 376 dosya\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ”¬ Phantom: D55-02\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "  ğŸ“ Doz KlasÃ¶rÃ¼: 300\n",
            "      â†’ 10.5 CTDIvol (YÃ¼ksek Doz)\n",
            "      â””â”€â”€ AIDR3D (Iterative Reconstruction)\n",
            "          KlasÃ¶r: 49_120_300_FC08_AIDR3D_194917.334\n",
            "          DICOM: 363 dosya\n",
            "      â””â”€â”€ FBP (Filtered Back Projection)\n",
            "          KlasÃ¶r: 51_120_300_FC08_FBP_194917.334\n",
            "          DICOM: 363 dosya\n",
            "      â””â”€â”€ AiCE (AI-based Reconstruction)\n",
            "          KlasÃ¶r: 55_120_300_BODY-SHARP_AICE_174115.432\n",
            "          DICOM: 363 dosya\n",
            "\n",
            "  ğŸ“ Doz KlasÃ¶rÃ¼: 40\n",
            "      â†’ 1.4 CTDIvol (DÃ¼ÅŸÃ¼k Doz)\n",
            "      â””â”€â”€ AIDR3D (Iterative Reconstruction)\n",
            "          KlasÃ¶r: 35_120_40_FC08_AIDR3D_174115.413\n",
            "          DICOM: 363 dosya\n",
            "      â””â”€â”€ FBP (Filtered Back Projection)\n",
            "          KlasÃ¶r: 38_120_40_FC08_FBP_174115.413\n",
            "          DICOM: 363 dosya\n",
            "      â””â”€â”€ AiCE (AI-based Reconstruction)\n",
            "          KlasÃ¶r: 7_120_40_BODY-SHARP_AICE_174115.413\n",
            "          DICOM: 363 dosya\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ REKONSTRÃœKSÄ°YON METODU SEÃ‡Ä°MÄ°\n",
            "================================================================================\n",
            "\n",
            "âš ï¸  Ã–NERÄ°: FBP metodu seÃ§in (model eÄŸitiminde kullanÄ±ldÄ±ysa)\n",
            "   â€¢ FBP: Standart, ham gÃ¼rÃ¼ltÃ¼ iÃ§erir (en yaygÄ±n)\n",
            "   â€¢ AIDR3D: Iterative, kÄ±smen gÃ¼rÃ¼ltÃ¼ azaltÄ±lmÄ±ÅŸ\n",
            "   â€¢ AiCE: AI-based, yoÄŸun gÃ¼rÃ¼ltÃ¼ azaltma yapÄ±lmÄ±ÅŸ\n",
            "\n",
            "Hangi rekonstrÃ¼ksiyon metodunu kullanmak istersiniz?\n",
            "  [1] FBP (Ã–nerilen)\n",
            "  [2] AIDR3D\n",
            "  [3] AiCE\n",
            "\n",
            "SeÃ§iminiz (1/2/3): 1\n",
            "\n",
            "âœ… SeÃ§ilen metod: FBP\n",
            "\n",
            "================================================================================\n",
            "ğŸ”„ DÃ–NÃœÅTÃœRME Ä°ÅLEMÄ°\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¬ Phantom: D55-01\n",
            "================================================================================\n",
            "\n",
            "  ğŸ“¥ INPUT (1.4 CTDIvol):\n",
            "     FBP_FC08_40_152622.890\n",
            "  ğŸ“¤ TARGET (10.5 CTDIvol):\n",
            "     FBP_FC08_300_171515.916\n",
            "\n",
            "  ğŸ“ DICOM dosyalarÄ± okunuyor...\n",
            "\n",
            "    INPUT:\n",
            "      ğŸ“‚ FBP_FC08_40_152622.890\n",
            "      âœ“ 376 DICOM dosyasÄ± bulundu\n",
            "      âœ“ Volume: (376, 512, 512)\n",
            "      âœ“ HU aralÄ±ÄŸÄ±: [-1000.0, 1000.0]\n",
            "\n",
            "    TARGET:\n",
            "      ğŸ“‚ FBP_FC08_300_171515.916\n",
            "      âœ“ 376 DICOM dosyasÄ± bulundu\n",
            "      âœ“ Volume: (376, 512, 512)\n",
            "      âœ“ HU aralÄ±ÄŸÄ±: [-1000.0, 1000.0]\n",
            "\n",
            "  ğŸ”„ Normalizasyon: [-1000, 1000] HU â†’ [-1, 1]\n",
            "    âœ“ Input: [-1.000, 1.000]\n",
            "    âœ“ Target: [-1.000, 1.000]\n",
            "\n",
            "  ğŸ’¾ Dosyalar kaydediliyor...\n",
            "    âœ“ /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output3/D55-01/input_ldct.npy\n",
            "    âœ“ /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output3/D55-01/target_ndct.npy\n",
            "\n",
            "  ğŸ“Š Ä°statistikler:\n",
            "     Slice: 376\n",
            "     Boyut: 512 x 512\n",
            "     Dosya: 376.00 MB (her biri)\n",
            "\n",
            "  âœ… BaÅŸarÄ±lÄ±!\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¬ Phantom: D55-02\n",
            "================================================================================\n",
            "\n",
            "  ğŸ“¥ INPUT (1.4 CTDIvol):\n",
            "     38_120_40_FC08_FBP_174115.413\n",
            "  ğŸ“¤ TARGET (10.5 CTDIvol):\n",
            "     51_120_300_FC08_FBP_194917.334\n",
            "\n",
            "  ğŸ“ DICOM dosyalarÄ± okunuyor...\n",
            "\n",
            "    INPUT:\n",
            "      ğŸ“‚ 38_120_40_FC08_FBP_174115.413\n",
            "      âœ“ 363 DICOM dosyasÄ± bulundu\n",
            "      âœ“ Volume: (363, 512, 512)\n",
            "      âœ“ HU aralÄ±ÄŸÄ±: [-1000.0, 1000.0]\n",
            "\n",
            "    TARGET:\n",
            "      ğŸ“‚ 51_120_300_FC08_FBP_194917.334\n",
            "      âœ“ 363 DICOM dosyasÄ± bulundu\n",
            "      âœ“ Volume: (363, 512, 512)\n",
            "      âœ“ HU aralÄ±ÄŸÄ±: [-1000.0, 1000.0]\n",
            "\n",
            "  ğŸ”„ Normalizasyon: [-1000, 1000] HU â†’ [-1, 1]\n",
            "    âœ“ Input: [-1.000, 1.000]\n",
            "    âœ“ Target: [-1.000, 1.000]\n",
            "\n",
            "  ğŸ’¾ Dosyalar kaydediliyor...\n",
            "    âœ“ /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output3/D55-02/input_ldct.npy\n",
            "    âœ“ /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output3/D55-02/target_ndct.npy\n",
            "\n",
            "  ğŸ“Š Ä°statistikler:\n",
            "     Slice: 363\n",
            "     Boyut: 512 x 512\n",
            "     Dosya: 363.00 MB (her biri)\n",
            "\n",
            "  âœ… BaÅŸarÄ±lÄ±!\n",
            "\n",
            "================================================================================\n",
            "âœ… DÃ–NÃœÅTÃœRME TAMAMLANDI!\n",
            "================================================================================\n",
            "\n",
            "ğŸ“ Ã‡Ä±ktÄ±: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output3\n",
            "\n",
            "  D55-01/\n",
            "    â”œâ”€â”€ input_ldct.npy  (1.4 CTDIvol - DÃ¼ÅŸÃ¼k Doz)\n",
            "    â””â”€â”€ target_ndct.npy (10.5 CTDIvol - YÃ¼ksek Doz)\n",
            "  D55-02/\n",
            "    â”œâ”€â”€ input_ldct.npy  (1.4 CTDIvol - DÃ¼ÅŸÃ¼k Doz)\n",
            "    â””â”€â”€ target_ndct.npy (10.5 CTDIvol - YÃ¼ksek Doz)\n",
            "\n",
            "ğŸš€ Test kodunu Ã§alÄ±ÅŸtÄ±rabilirsiniz!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==================== AYARLAR ====================\n",
        "phantomx_processed_dir = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output3\"\n",
        "checkpoint_dir = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/model_checkpoints\"\n",
        "output_dir = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Results3\"\n",
        "\n",
        "phantoms = [\"D55-01\", \"D55-02\"]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PhantomX Model Test ve DeÄŸerlendirme\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"ğŸ“‚ Processed data: {phantomx_processed_dir}\")\n",
        "print(f\"ğŸ§  Model checkpoints: {checkpoint_dir}\")\n",
        "print(f\"ğŸ“Š Results output: {output_dir}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ==================== FONKSÄ°YONLAR ====================\n",
        "\n",
        "def load_latest_generator(checkpoint_dir, generator_model):\n",
        "    \"\"\"En yÃ¼ksek epoch numaralÄ± generator modelini yÃ¼kler\"\"\"\n",
        "    list_of_files = glob.glob(os.path.join(checkpoint_dir, 'G_epoch_*.h5'))\n",
        "\n",
        "    if not list_of_files:\n",
        "        print(\"âŒ HATA: KaydedilmiÅŸ generator modeli bulunamadÄ±!\")\n",
        "        print(f\"   Kontrol edilen dizin: {checkpoint_dir}\")\n",
        "        print(f\"   Aranan format: G_epoch_*.h5\")\n",
        "\n",
        "        # KlasÃ¶rdeki tÃ¼m dosyalarÄ± gÃ¶ster\n",
        "        if os.path.exists(checkpoint_dir):\n",
        "            all_files = os.listdir(checkpoint_dir)\n",
        "            if all_files:\n",
        "                print(f\"\\n   KlasÃ¶rdeki dosyalar:\")\n",
        "                for f in sorted(all_files)[:10]:  # Ä°lk 10 dosyayÄ± gÃ¶ster\n",
        "                    print(f\"   - {f}\")\n",
        "        return None\n",
        "\n",
        "    # Epoch numarasÄ±na gÃ¶re sÄ±rala ve en bÃ¼yÃ¼ÄŸÃ¼nÃ¼ al\n",
        "    def get_epoch_number(filepath):\n",
        "        \"\"\"Dosya adÄ±ndan epoch numarasÄ±nÄ± Ã§Ä±kar\"\"\"\n",
        "        basename = os.path.basename(filepath)\n",
        "        # G_epoch_50.h5 -> 50\n",
        "        try:\n",
        "            epoch_str = basename.split('_')[-1].replace('.h5', '')\n",
        "            return int(epoch_str)\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    latest_file = max(list_of_files, key=get_epoch_number)\n",
        "    epoch_num = get_epoch_number(latest_file)\n",
        "\n",
        "    print(f\"\\nğŸ“¦ Model bulundu ve yÃ¼kleniyor...\")\n",
        "    print(f\"   Dosya: {os.path.basename(latest_file)}\")\n",
        "    print(f\"   Epoch: {epoch_num}\")\n",
        "    print(f\"   Toplam {len(list_of_files)} checkpoint bulundu\")\n",
        "\n",
        "    try:\n",
        "        generator_model.load_weights(latest_file)\n",
        "        print(\"âœ… Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
        "        return generator_model\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Model yÃ¼kleme hatasÄ±: {e}\")\n",
        "        print(f\"\\nğŸ’¡ Model mimarisi ile checkpoint uyumsuz olabilir.\")\n",
        "        print(f\"   LÃ¼tfen generator modelinin doÄŸru tanÄ±mlandÄ±ÄŸÄ±ndan emin olun.\")\n",
        "        return None\n",
        "\n",
        "def calculate_metrics_3d(target, predicted):\n",
        "    \"\"\"3D volume iÃ§in PSNR ve SSIM hesaplar\"\"\"\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "\n",
        "    # Her slice iÃ§in ayrÄ± ayrÄ± hesapla\n",
        "    for i in range(target.shape[0]):\n",
        "        target_slice = target[i]\n",
        "        pred_slice = predicted[i]\n",
        "\n",
        "        # PSNR hesaplama (data_range=2.0 Ã§Ã¼nkÃ¼ [-1, 1] aralÄ±ÄŸÄ±nda)\n",
        "        psnr_val = psnr(target_slice, pred_slice, data_range=2.0)\n",
        "        psnr_scores.append(psnr_val)\n",
        "\n",
        "        # SSIM hesaplama\n",
        "        ssim_val = ssim(target_slice, pred_slice, data_range=2.0)\n",
        "        ssim_scores.append(ssim_val)\n",
        "\n",
        "    return np.array(psnr_scores), np.array(ssim_scores)\n",
        "\n",
        "def visualize_results(input_vol, target_vol, predicted_vol, phantom_name, output_dir, num_slices=5):\n",
        "    \"\"\"SonuÃ§larÄ± gÃ¶rselleÅŸtirir ve kaydeder\"\"\"\n",
        "    total_slices = input_vol.shape[0]\n",
        "    slice_indices = np.linspace(0, total_slices-1, num_slices, dtype=int)\n",
        "\n",
        "    fig, axes = plt.subplots(num_slices, 4, figsize=(16, 4*num_slices))\n",
        "\n",
        "    for idx, slice_num in enumerate(slice_indices):\n",
        "        # Input (LDCT)\n",
        "        axes[idx, 0].imshow(input_vol[slice_num], cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[idx, 0].set_title(f'Input (LDCT)\\nSlice {slice_num}/{total_slices}', fontsize=10)\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        # Target (NDCT)\n",
        "        axes[idx, 1].imshow(target_vol[slice_num], cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[idx, 1].set_title(f'Target (NDCT)\\nSlice {slice_num}/{total_slices}', fontsize=10)\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Predicted\n",
        "        axes[idx, 2].imshow(predicted_vol[slice_num], cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[idx, 2].set_title(f'Predicted\\nSlice {slice_num}/{total_slices}', fontsize=10)\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "        # Difference map\n",
        "        diff = np.abs(target_vol[slice_num] - predicted_vol[slice_num])\n",
        "        im = axes[idx, 3].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n",
        "        axes[idx, 3].set_title(f'Absolute Error\\nSlice {slice_num}/{total_slices}', fontsize=10)\n",
        "        axes[idx, 3].axis('off')\n",
        "\n",
        "        # Colorbar for error map\n",
        "        plt.colorbar(im, ax=axes[idx, 3], fraction=0.046, pad=0.04)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Kaydet\n",
        "    save_path = os.path.join(output_dir, f\"{phantom_name}_comparison.png\")\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"  ğŸ’¾ GÃ¶rselleÅŸtirme kaydedildi: {os.path.basename(save_path)}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics_distribution(psnr_scores, ssim_scores, phantom_name, output_dir):\n",
        "    \"\"\"Metrik daÄŸÄ±lÄ±mlarÄ±nÄ± Ã§izer\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # PSNR histogram\n",
        "    axes[0].hist(psnr_scores, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "    axes[0].axvline(np.mean(psnr_scores), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Ortalama: {np.mean(psnr_scores):.2f} dB')\n",
        "    axes[0].set_xlabel('PSNR (dB)', fontsize=12)\n",
        "    axes[0].set_ylabel('Slice SayÄ±sÄ±', fontsize=12)\n",
        "    axes[0].set_title(f'{phantom_name} - PSNR DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    # SSIM histogram\n",
        "    axes[1].hist(ssim_scores, bins=30, color='forestgreen', edgecolor='black', alpha=0.7)\n",
        "    axes[1].axvline(np.mean(ssim_scores), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Ortalama: {np.mean(ssim_scores):.4f}')\n",
        "    axes[1].set_xlabel('SSIM', fontsize=12)\n",
        "    axes[1].set_ylabel('Slice SayÄ±sÄ±', fontsize=12)\n",
        "    axes[1].set_title(f'{phantom_name} - SSIM DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Kaydet\n",
        "    save_path = os.path.join(output_dir, f\"{phantom_name}_metrics_distribution.png\")\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"  ğŸ’¾ Metrik daÄŸÄ±lÄ±mÄ± kaydedildi: {os.path.basename(save_path)}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_slice_metrics(psnr_scores, ssim_scores, phantom_name, output_dir):\n",
        "    \"\"\"Slice bazÄ±nda metrik deÄŸiÅŸimini gÃ¶sterir\"\"\"\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "    slice_numbers = np.arange(len(psnr_scores))\n",
        "\n",
        "    # PSNR over slices\n",
        "    axes[0].plot(slice_numbers, psnr_scores, color='steelblue', linewidth=1.5, alpha=0.7)\n",
        "    axes[0].axhline(np.mean(psnr_scores), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Ortalama: {np.mean(psnr_scores):.2f} dB')\n",
        "    axes[0].fill_between(slice_numbers, psnr_scores, alpha=0.3, color='steelblue')\n",
        "    axes[0].set_xlabel('Slice NumarasÄ±', fontsize=12)\n",
        "    axes[0].set_ylabel('PSNR (dB)', fontsize=12)\n",
        "    axes[0].set_title(f'{phantom_name} - Slice BazÄ±nda PSNR', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    # SSIM over slices\n",
        "    axes[1].plot(slice_numbers, ssim_scores, color='forestgreen', linewidth=1.5, alpha=0.7)\n",
        "    axes[1].axhline(np.mean(ssim_scores), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Ortalama: {np.mean(ssim_scores):.4f}')\n",
        "    axes[1].fill_between(slice_numbers, ssim_scores, alpha=0.3, color='forestgreen')\n",
        "    axes[1].set_xlabel('Slice NumarasÄ±', fontsize=12)\n",
        "    axes[1].set_ylabel('SSIM', fontsize=12)\n",
        "    axes[1].set_title(f'{phantom_name} - Slice BazÄ±nda SSIM', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Kaydet\n",
        "    save_path = os.path.join(output_dir, f\"{phantom_name}_slice_metrics.png\")\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"  ğŸ’¾ Slice metrikleri kaydedildi: {os.path.basename(save_path)}\")\n",
        "    plt.close()\n",
        "\n",
        "# ==================== MODEL YÃœKLEME ====================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ”§ MODEL HAZIRLANIYOR\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Generator objesi kontrolÃ¼\n",
        "if 'generator' not in dir():\n",
        "    print(\"\\nâš ï¸  UYARI: 'generator' objesi bulunamadÄ±!\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ“ GENERATOR MODELÄ°NÄ° TANIMLAYIN\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nLÃ¼tfen test kodunu Ã§alÄ±ÅŸtÄ±rmadan Ã–NCE aÅŸaÄŸÄ±daki kodu Ã§alÄ±ÅŸtÄ±rÄ±n:\\n\")\n",
        "    print(\"```python\")\n",
        "    print(\"# SeÃ§enek 1: EÄŸer build_generator fonksiyonunuz varsa\")\n",
        "    print(\"generator = build_generator()\")\n",
        "    print(\"\")\n",
        "    print(\"# SeÃ§enek 2: EÄŸer model tanÄ±mÄ± baÅŸka bir yerdeyse\")\n",
        "    print(\"from your_model_file import build_generator\")\n",
        "    print(\"generator = build_generator()\")\n",
        "    print(\"\")\n",
        "    print(\"# SeÃ§enek 3: Manuel olarak tanÄ±mlayÄ±n\")\n",
        "    print(\"# generator = tf.keras.models.Sequential([...])\")\n",
        "    print(\"```\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"âŒ Test durduruluyor. Ã–nce generator'Ä± tanÄ±mlayÄ±n ve bu kodu tekrar Ã§alÄ±ÅŸtÄ±rÄ±n.\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"âœ… Generator objesi bellekte bulundu.\")\n",
        "    print(f\"   Model tipi: {type(generator)}\")\n",
        "\n",
        "    # Model aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼kle\n",
        "    generator = load_latest_generator(checkpoint_dir, generator)\n",
        "\n",
        "    if generator is None:\n",
        "        print(\"\\nâŒ Generator modeli yÃ¼klenemedi. Test durduruluyor.\")\n",
        "        print(\"\\nğŸ’¡ OlasÄ± nedenler:\")\n",
        "        print(\"   1. Checkpoint dosyasÄ± bulunamadÄ±\")\n",
        "        print(\"   2. Model mimarisi checkpoint ile uyumsuz\")\n",
        "        print(\"   3. Dosya yolu hatalÄ±\")\n",
        "        print(f\"\\n   Kontrol edin: {checkpoint_dir}\")\n",
        "        exit()\n",
        "\n",
        "# ==================== PHANTOM TESTLERÄ° ====================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ§ª PHANTOM TESTLERÄ° BAÅLIYOR\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for phantom_name in phantoms:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ”¬ Phantom: {phantom_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    phantom_dir = os.path.join(phantomx_processed_dir, phantom_name)\n",
        "\n",
        "    # NPY dosyalarÄ±nÄ± yÃ¼kle\n",
        "    input_path = os.path.join(phantom_dir, \"input_ldct.npy\")\n",
        "    target_path = os.path.join(phantom_dir, \"target_ndct.npy\")\n",
        "\n",
        "    if not os.path.exists(input_path) or not os.path.exists(target_path):\n",
        "        print(f\"âŒ NPY dosyalarÄ± bulunamadÄ±: {phantom_dir}\")\n",
        "        print(f\"   Kontrol edilen dosyalar:\")\n",
        "        print(f\"   - {input_path}\")\n",
        "        print(f\"   - {target_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"ğŸ“‚ Veri yÃ¼kleniyor...\")\n",
        "    input_volume = np.load(input_path)\n",
        "    target_volume = np.load(target_path)\n",
        "\n",
        "    print(f\"  âœ“ Input shape: {input_volume.shape}\")\n",
        "    print(f\"  âœ“ Target shape: {target_volume.shape}\")\n",
        "    print(f\"  âœ“ Input range: [{input_volume.min():.3f}, {input_volume.max():.3f}]\")\n",
        "    print(f\"  âœ“ Target range: [{target_volume.min():.3f}, {target_volume.max():.3f}]\")\n",
        "\n",
        "    # Model input boyutunu kontrol et\n",
        "    model_input_shape = generator.input_shape  # (None, H, W, C)\n",
        "    expected_h = model_input_shape[1]\n",
        "    expected_w = model_input_shape[2]\n",
        "\n",
        "    actual_h = input_volume.shape[1]\n",
        "    actual_w = input_volume.shape[2]\n",
        "\n",
        "    print(f\"\\n  ğŸ” Boyut kontrolÃ¼:\")\n",
        "    print(f\"     Model beklenen boyut: {expected_h}x{expected_w}\")\n",
        "    print(f\"     Veri boyutu: {actual_h}x{actual_w}\")\n",
        "\n",
        "    # EÄŸer boyutlar uyuÅŸmuyorsa resize et\n",
        "    needs_resize = (actual_h != expected_h) or (actual_w != expected_w)\n",
        "    original_input = None\n",
        "    original_target = None\n",
        "\n",
        "    if needs_resize:\n",
        "        print(f\"\\n  ğŸ”„ GÃ¶rÃ¼ntÃ¼ler resize ediliyor: {actual_h}x{actual_w} â†’ {expected_h}x{expected_w}\")\n",
        "\n",
        "        from tensorflow.keras.preprocessing.image import array_to_img, img_to_array\n",
        "        import cv2\n",
        "\n",
        "        # Input volume'u resize et\n",
        "        input_resized = np.zeros((input_volume.shape[0], expected_h, expected_w), dtype=np.float32)\n",
        "        for i in range(input_volume.shape[0]):\n",
        "            input_resized[i] = cv2.resize(input_volume[i], (expected_w, expected_h),\n",
        "                                         interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Target volume'u resize et\n",
        "        target_resized = np.zeros((target_volume.shape[0], expected_h, expected_w), dtype=np.float32)\n",
        "        for i in range(target_volume.shape[0]):\n",
        "            target_resized[i] = cv2.resize(target_volume[i], (expected_w, expected_h),\n",
        "                                          interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Orijinal boyutlarÄ± sakla (sonra tekrar bÃ¼yÃ¼tmek iÃ§in)\n",
        "        original_input = input_volume\n",
        "        original_target = target_volume\n",
        "\n",
        "        input_volume = input_resized\n",
        "        target_volume = target_resized\n",
        "\n",
        "        print(f\"     âœ“ Resize tamamlandÄ±: {input_volume.shape}\")\n",
        "\n",
        "    # Model iÃ§in tahmin yap\n",
        "    num_slices = input_volume.shape[0]\n",
        "    predicted_volume = np.zeros_like(input_volume)\n",
        "\n",
        "    print(f\"\\nğŸ”® Model tahminleri yapÄ±lÄ±yor ({num_slices} slice)...\")\n",
        "\n",
        "    # Progress bar iÃ§in\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    for i in tqdm(range(num_slices), desc=f\"  {phantom_name}\"):\n",
        "        # Slice'Ä± al ve reshape et: (1, H, W, 1)\n",
        "        input_slice = input_volume[i:i+1, :, :]\n",
        "        input_slice = np.expand_dims(input_slice, axis=-1)  # Channel dimension ekle\n",
        "\n",
        "        # Tahmin yap\n",
        "        predicted_slice = generator.predict(input_slice, verbose=0)\n",
        "\n",
        "        # Geri Ã§evir: (H, W)\n",
        "        predicted_volume[i] = predicted_slice[0, :, :, 0]\n",
        "\n",
        "    print(f\"  âœ… TÃ¼m tahminler tamamlandÄ±!\")\n",
        "\n",
        "    # EÄŸer resize yaptÄ±ysak, tahminleri orijinal boyuta geri getir\n",
        "    if needs_resize:\n",
        "        print(f\"\\n  ğŸ”„ Tahminler orijinal boyuta geri getiriliyor: {expected_h}x{expected_w} â†’ {actual_h}x{actual_w}\")\n",
        "\n",
        "        predicted_original_size = np.zeros_like(original_input, dtype=np.float32)\n",
        "        for i in range(predicted_volume.shape[0]):\n",
        "            predicted_original_size[i] = cv2.resize(predicted_volume[i], (actual_w, actual_h),\n",
        "                                                    interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Metrikleri hesaplamak iÃ§in orijinal boyutlarÄ± kullan\n",
        "        input_for_metrics = original_input\n",
        "        target_for_metrics = original_target\n",
        "        predicted_for_metrics = predicted_original_size\n",
        "\n",
        "        # GÃ¶rselleÅŸtirme iÃ§in resize edilmiÅŸ versiyonlarÄ± kullan (daha hÄ±zlÄ±)\n",
        "        input_for_viz = input_volume\n",
        "        target_for_viz = target_volume\n",
        "        predicted_for_viz = predicted_volume\n",
        "\n",
        "        print(f\"     âœ“ Geri getirme tamamlandÄ±: {predicted_original_size.shape}\")\n",
        "    else:\n",
        "        input_for_metrics = input_volume\n",
        "        target_for_metrics = target_volume\n",
        "        predicted_for_metrics = predicted_volume\n",
        "\n",
        "        input_for_viz = input_volume\n",
        "        target_for_viz = target_volume\n",
        "        predicted_for_viz = predicted_volume\n",
        "\n",
        "    # Metrikleri hesapla\n",
        "    print(f\"\\nğŸ“Š Metrikler hesaplanÄ±yor...\")\n",
        "    psnr_scores, ssim_scores = calculate_metrics_3d(target_for_metrics, predicted_for_metrics)\n",
        "\n",
        "    # Ä°statistikleri kaydet\n",
        "    results = {\n",
        "        'phantom': phantom_name,\n",
        "        'num_slices': num_slices,\n",
        "        'psnr_mean': np.mean(psnr_scores),\n",
        "        'psnr_std': np.std(psnr_scores),\n",
        "        'psnr_min': np.min(psnr_scores),\n",
        "        'psnr_max': np.max(psnr_scores),\n",
        "        'psnr_median': np.median(psnr_scores),\n",
        "        'ssim_mean': np.mean(ssim_scores),\n",
        "        'ssim_std': np.std(ssim_scores),\n",
        "        'ssim_min': np.min(ssim_scores),\n",
        "        'ssim_max': np.max(ssim_scores),\n",
        "        'ssim_median': np.median(ssim_scores)\n",
        "    }\n",
        "\n",
        "    all_results[phantom_name] = results\n",
        "\n",
        "    # SonuÃ§larÄ± yazdÄ±r\n",
        "    print(f\"\\n{'â”€'*80}\")\n",
        "    print(f\"  ğŸ“ˆ SONUÃ‡LAR - {phantom_name}\")\n",
        "    print(f\"{'â”€'*80}\")\n",
        "    print(f\"  Toplam Slice: {num_slices}\")\n",
        "    print(f\"\\n  PSNR:\")\n",
        "    print(f\"    Ortalama: {results['psnr_mean']:.2f} dB (Â±{results['psnr_std']:.2f})\")\n",
        "    print(f\"    Medyan:   {results['psnr_median']:.2f} dB\")\n",
        "    print(f\"    Min/Max:  {results['psnr_min']:.2f} / {results['psnr_max']:.2f} dB\")\n",
        "    print(f\"\\n  SSIM:\")\n",
        "    print(f\"    Ortalama: {results['ssim_mean']:.4f} (Â±{results['ssim_std']:.4f})\")\n",
        "    print(f\"    Medyan:   {results['ssim_median']:.4f}\")\n",
        "    print(f\"    Min/Max:  {results['ssim_min']:.4f} / {results['ssim_max']:.4f}\")\n",
        "    print(f\"{'â”€'*80}\\n\")\n",
        "\n",
        "    # GÃ¶rselleÅŸtirmeler\n",
        "    print(f\"ğŸ¨ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...\")\n",
        "    visualize_results(input_for_viz, target_for_viz, predicted_for_viz,\n",
        "                     phantom_name, output_dir, num_slices=5)\n",
        "    plot_metrics_distribution(psnr_scores, ssim_scores, phantom_name, output_dir)\n",
        "    plot_slice_metrics(psnr_scores, ssim_scores, phantom_name, output_dir)\n",
        "\n",
        "    # Tahminleri kaydet (orijinal boyutta)\n",
        "    pred_save_path = os.path.join(output_dir, f\"{phantom_name}_predicted.npy\")\n",
        "    np.save(pred_save_path, predicted_for_metrics)\n",
        "    print(f\"  ğŸ’¾ Tahminler kaydedildi: {os.path.basename(pred_save_path)}\\n\")\n",
        "\n",
        "# ==================== GENEL RAPOR ====================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ“‹ GENEL DEÄERLENDIRME RAPORU\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if len(all_results) == 0:\n",
        "    print(\"\\nâŒ HiÃ§bir phantom iÃ§in test yapÄ±lamadÄ±!\")\n",
        "else:\n",
        "    # Her phantom iÃ§in Ã¶zet\n",
        "    for phantom_name, results in all_results.items():\n",
        "        print(f\"\\nğŸ”¬ {phantom_name}:\")\n",
        "        print(f\"  ğŸ“Š PSNR: {results['psnr_mean']:.2f} Â± {results['psnr_std']:.2f} dB\")\n",
        "        print(f\"  ğŸ“Š SSIM: {results['ssim_mean']:.4f} Â± {results['ssim_std']:.4f}\")\n",
        "\n",
        "    # Ortalama sonuÃ§lar (tÃ¼m phantomlar)\n",
        "    if len(all_results) > 1:\n",
        "        avg_psnr = np.mean([r['psnr_mean'] for r in all_results.values()])\n",
        "        avg_ssim = np.mean([r['ssim_mean'] for r in all_results.values()])\n",
        "\n",
        "        print(f\"\\n{'â”€'*80}\")\n",
        "        print(f\"ğŸ“Š TÃœM PHANTOM ORTALAMASI:\")\n",
        "        print(f\"  PSNR: {avg_psnr:.2f} dB\")\n",
        "        print(f\"  SSIM: {avg_ssim:.4f}\")\n",
        "        print(f\"{'â”€'*80}\")\n",
        "\n",
        "    # DosyalarÄ± listele\n",
        "    print(f\"\\nğŸ“ Ã‡IKTI DOSYALARI ({output_dir}):\")\n",
        "    for phantom_name in all_results.keys():\n",
        "        print(f\"\\n  {phantom_name}/\")\n",
        "        print(f\"    â”œâ”€â”€ {phantom_name}_comparison.png          (GÃ¶rsel karÅŸÄ±laÅŸtÄ±rma)\")\n",
        "        print(f\"    â”œâ”€â”€ {phantom_name}_metrics_distribution.png (Metrik daÄŸÄ±lÄ±mlarÄ±)\")\n",
        "        print(f\"    â”œâ”€â”€ {phantom_name}_slice_metrics.png       (Slice bazÄ±nda trend)\")\n",
        "        print(f\"    â””â”€â”€ {phantom_name}_predicted.npy           (Tahmin edilen volume)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ… TEST TAMAMLANDI!\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d1IDjV_Qn2R",
        "outputId": "180490b8-cb95-4c99-dfdd-33a7c2db9ff4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PhantomX Model Test ve DeÄŸerlendirme\n",
            "================================================================================\n",
            "ğŸ“‚ Processed data: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output3\n",
            "ğŸ§  Model checkpoints: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/model_checkpoints\n",
            "ğŸ“Š Results output: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Results3\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸ”§ MODEL HAZIRLANIYOR\n",
            "================================================================================\n",
            "âœ… Generator objesi bellekte bulundu.\n",
            "   Model tipi: <class 'keras.src.models.functional.Functional'>\n",
            "\n",
            "ğŸ“¦ Model bulundu ve yÃ¼kleniyor...\n",
            "   Dosya: G_epoch_45.h5\n",
            "   Epoch: 45\n",
            "   Toplam 1 checkpoint bulundu\n",
            "âœ… Model baÅŸarÄ±yla yÃ¼klendi!\n",
            "\n",
            "================================================================================\n",
            "ğŸ§ª PHANTOM TESTLERÄ° BAÅLIYOR\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¬ Phantom: D55-01\n",
            "================================================================================\n",
            "ğŸ“‚ Veri yÃ¼kleniyor...\n",
            "  âœ“ Input shape: (376, 512, 512)\n",
            "  âœ“ Target shape: (376, 512, 512)\n",
            "  âœ“ Input range: [-1.000, 1.000]\n",
            "  âœ“ Target range: [-1.000, 1.000]\n",
            "\n",
            "  ğŸ” Boyut kontrolÃ¼:\n",
            "     Model beklenen boyut: 256x256\n",
            "     Veri boyutu: 512x512\n",
            "\n",
            "  ğŸ”„ GÃ¶rÃ¼ntÃ¼ler resize ediliyor: 512x512 â†’ 256x256\n",
            "     âœ“ Resize tamamlandÄ±: (376, 256, 256)\n",
            "\n",
            "ğŸ”® Model tahminleri yapÄ±lÄ±yor (376 slice)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  D55-01: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 376/376 [04:10<00:00,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  âœ… TÃ¼m tahminler tamamlandÄ±!\n",
            "\n",
            "  ğŸ”„ Tahminler orijinal boyuta geri getiriliyor: 256x256 â†’ 512x512\n",
            "     âœ“ Geri getirme tamamlandÄ±: (376, 512, 512)\n",
            "\n",
            "ğŸ“Š Metrikler hesaplanÄ±yor...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  ğŸ“ˆ SONUÃ‡LAR - D55-01\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  Toplam Slice: 376\n",
            "\n",
            "  PSNR:\n",
            "    Ortalama: 34.75 dB (Â±1.00)\n",
            "    Medyan:   34.85 dB\n",
            "    Min/Max:  29.44 / 36.60 dB\n",
            "\n",
            "  SSIM:\n",
            "    Ortalama: 0.9473 (Â±0.0152)\n",
            "    Medyan:   0.9486\n",
            "    Min/Max:  0.8406 / 0.9861\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ¨ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...\n",
            "  ğŸ’¾ GÃ¶rselleÅŸtirme kaydedildi: D55-01_comparison.png\n",
            "  ğŸ’¾ Metrik daÄŸÄ±lÄ±mÄ± kaydedildi: D55-01_metrics_distribution.png\n",
            "  ğŸ’¾ Slice metrikleri kaydedildi: D55-01_slice_metrics.png\n",
            "  ğŸ’¾ Tahminler kaydedildi: D55-01_predicted.npy\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¬ Phantom: D55-02\n",
            "================================================================================\n",
            "ğŸ“‚ Veri yÃ¼kleniyor...\n",
            "  âœ“ Input shape: (363, 512, 512)\n",
            "  âœ“ Target shape: (363, 512, 512)\n",
            "  âœ“ Input range: [-1.000, 1.000]\n",
            "  âœ“ Target range: [-1.000, 1.000]\n",
            "\n",
            "  ğŸ” Boyut kontrolÃ¼:\n",
            "     Model beklenen boyut: 256x256\n",
            "     Veri boyutu: 512x512\n",
            "\n",
            "  ğŸ”„ GÃ¶rÃ¼ntÃ¼ler resize ediliyor: 512x512 â†’ 256x256\n",
            "     âœ“ Resize tamamlandÄ±: (363, 256, 256)\n",
            "\n",
            "ğŸ”® Model tahminleri yapÄ±lÄ±yor (363 slice)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  D55-02: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 363/363 [03:59<00:00,  1.52it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  âœ… TÃ¼m tahminler tamamlandÄ±!\n",
            "\n",
            "  ğŸ”„ Tahminler orijinal boyuta geri getiriliyor: 256x256 â†’ 512x512\n",
            "     âœ“ Geri getirme tamamlandÄ±: (363, 512, 512)\n",
            "\n",
            "ğŸ“Š Metrikler hesaplanÄ±yor...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  ğŸ“ˆ SONUÃ‡LAR - D55-02\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  Toplam Slice: 363\n",
            "\n",
            "  PSNR:\n",
            "    Ortalama: 35.66 dB (Â±1.34)\n",
            "    Medyan:   35.80 dB\n",
            "    Min/Max:  27.69 / 38.05 dB\n",
            "\n",
            "  SSIM:\n",
            "    Ortalama: 0.9570 (Â±0.0167)\n",
            "    Medyan:   0.9587\n",
            "    Min/Max:  0.8119 / 0.9886\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ¨ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...\n",
            "  ğŸ’¾ GÃ¶rselleÅŸtirme kaydedildi: D55-02_comparison.png\n",
            "  ğŸ’¾ Metrik daÄŸÄ±lÄ±mÄ± kaydedildi: D55-02_metrics_distribution.png\n",
            "  ğŸ’¾ Slice metrikleri kaydedildi: D55-02_slice_metrics.png\n",
            "  ğŸ’¾ Tahminler kaydedildi: D55-02_predicted.npy\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ“‹ GENEL DEÄERLENDIRME RAPORU\n",
            "================================================================================\n",
            "\n",
            "ğŸ”¬ D55-01:\n",
            "  ğŸ“Š PSNR: 34.75 Â± 1.00 dB\n",
            "  ğŸ“Š SSIM: 0.9473 Â± 0.0152\n",
            "\n",
            "ğŸ”¬ D55-02:\n",
            "  ğŸ“Š PSNR: 35.66 Â± 1.34 dB\n",
            "  ğŸ“Š SSIM: 0.9570 Â± 0.0167\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š TÃœM PHANTOM ORTALAMASI:\n",
            "  PSNR: 35.21 dB\n",
            "  SSIM: 0.9522\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“ Ã‡IKTI DOSYALARI (/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Results3):\n",
            "\n",
            "  D55-01/\n",
            "    â”œâ”€â”€ D55-01_comparison.png          (GÃ¶rsel karÅŸÄ±laÅŸtÄ±rma)\n",
            "    â”œâ”€â”€ D55-01_metrics_distribution.png (Metrik daÄŸÄ±lÄ±mlarÄ±)\n",
            "    â”œâ”€â”€ D55-01_slice_metrics.png       (Slice bazÄ±nda trend)\n",
            "    â””â”€â”€ D55-01_predicted.npy           (Tahmin edilen volume)\n",
            "\n",
            "  D55-02/\n",
            "    â”œâ”€â”€ D55-02_comparison.png          (GÃ¶rsel karÅŸÄ±laÅŸtÄ±rma)\n",
            "    â”œâ”€â”€ D55-02_metrics_distribution.png (Metrik daÄŸÄ±lÄ±mlarÄ±)\n",
            "    â”œâ”€â”€ D55-02_slice_metrics.png       (Slice bazÄ±nda trend)\n",
            "    â””â”€â”€ D55-02_predicted.npy           (Tahmin edilen volume)\n",
            "\n",
            "================================================================================\n",
            "âœ… TEST TAMAMLANDI!\n",
            "================================================================================\n"
          ]
        }
      ]
    }
  ]
}