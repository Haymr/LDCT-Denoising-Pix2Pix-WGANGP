{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Veri Ön İşleme: Mayo LDCT Dataset\n",
                "\n",
                "Bu notebook Mayo Clinic LDCT verisini model eğitimi için hazırlar.\n",
                "\n",
                "**Adımlar:**\n",
                "1. DICOM dosyalarını oku\n",
                "2. HU (Hounsfield Unit) dönüşümü uygula\n",
                "3. [-1000, 1000] aralığına clip\n",
                "4. 256x256 boyutuna resize\n",
                "5. [-1, 1] aralığına normalize\n",
                "6. NPY formatında kaydet\n",
                "\n",
                "---\n",
                "\n",
                "## Neden Bu İşlemler?\n",
                "\n",
                "CT görüntüleri ham halde piksel değerleri olarak gelir. Bunları gerçek radyolojik değerlere (HU) çevirmemiz lazım:\n",
                "\n",
                "```\n",
                "HU = pixel_value × RescaleSlope + RescaleIntercept\n",
                "```\n",
                "\n",
                "Tipik HU değerleri:\n",
                "- Hava: -1000 HU\n",
                "- Su: 0 HU  \n",
                "- Yumuşak doku: 40-80 HU\n",
                "- Kemik: 400-1000 HU"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import pydicom\n",
                "import cv2\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Ayarlar\n",
                "\n",
                "Aşağıdaki yolları kendi sisteminize göre güncelleyin."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === DOSYA YOLLARI ===\n",
                "# Ham DICOM verisinin bulunduğu klasör\n",
                "base_dir = r\"D:\\data\\manifest-1755254936487\\LDCT-and-Projection-data\"\n",
                "\n",
                "# İşlenmiş NPY dosyalarının kaydedileceği klasör\n",
                "output_dir = r\"D:\\Projects\\data\\processed_data_npy\"\n",
                "\n",
                "os.makedirs(os.path.join(output_dir, \"trainA\"), exist_ok=True) # Low Dose\n",
                "os.makedirs(os.path.join(output_dir, \"trainB\"), exist_ok=True) # High Dose\n",
                "\n",
                "# === NORMALIZASYON AYARLARI ===\n",
                "HU_MIN = -1000\n",
                "HU_MAX = 1000\n",
                "IMG_SIZE = (256, 256)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Yardımcı Fonksiyonlar"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def find_dose_folders(patient_path):\n",
                "    \"\"\"\n",
                "    Hasta klasöründe Low Dose ve Full Dose klasörlerini bulur.\n",
                "    Projeksiyon (proj/sino) klasörlerini atlar.\n",
                "    \"\"\"\n",
                "    low_p = None\n",
                "    high_p = None\n",
                "\n",
                "    # Tüm alt klasörleri gez\n",
                "    for root, dirs, files in os.walk(patient_path):\n",
                "        for d in dirs:\n",
                "            d_lower = d.lower()\n",
                "\n",
                "            # --- FİLTRE ---\n",
                "            # 'proj' veya 'sino' içeren klasörleri atla\n",
                "            if \"proj\" in d_lower or \"sino\" in d_lower:\n",
                "                continue\n",
                "\n",
                "            # Klasör eşleştirme\n",
                "            if \"low dose\" in d_lower:\n",
                "                low_p = os.path.join(root, d)\n",
                "            elif \"full dose\" in d_lower or \"high dose\" in d_lower:\n",
                "                high_p = os.path.join(root, d)\n",
                "\n",
                "    return low_p, high_p"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## Ana İşleme Döngüsü\n",
                "\n",
                "Her hasta için:\n",
                "1. Low/High dose klasörlerini bul\n",
                "2. Eşleşen DICOM'ları oku\n",
                "3. HU dönüşümü ve normalizasyon uygula\n",
                "4. NPY olarak kaydet"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# C ve L ile başlayan hastaları alıyoruz\n",
                "try:\n",
                "    phantoms = [d for d in os.listdir(base_dir)\n",
                "                if d.startswith((\"C\", \"L\"))\n",
                "                and os.path.isdir(os.path.join(base_dir, d))]\n",
                "\n",
                "    phantoms = sorted(phantoms)\n",
                "    print(f\"Bulunan Hastalar ({len(phantoms)} adet): {phantoms}\")\n",
                "\n",
                "except FileNotFoundError:\n",
                "    print(f\"HATA: '{base_dir}' yolu bulunamıyor. Lütfen yolu kontrol edin.\")\n",
                "    phantoms = []\n",
                "\n",
                "print(\"\\n--- İşlem Başlıyor ---\\n\")\n",
                "\n",
                "for phantom in phantoms:\n",
                "    patient_path = os.path.join(base_dir, phantom)\n",
                "\n",
                "    # Low ve High Dose klasörlerini bul\n",
                "    low_path, high_path = find_dose_folders(patient_path)\n",
                "\n",
                "    # Klasörler bulundu mu kontrolü\n",
                "    if not low_path or not high_path:\n",
                "        print(f\" Klasörler TAM bulunamadı (Atlanıyor -> Sadece projeksiyon olabilir): {phantom}\")\n",
                "        continue\n",
                "\n",
                "    print(f\" {phantom} işleniyor...\")\n",
                "\n",
                "    low_files = sorted(os.listdir(low_path))\n",
                "    high_files = sorted(os.listdir(high_path))\n",
                "\n",
                "    min_len = min(len(low_files), len(high_files))\n",
                "\n",
                "    for i in range(min_len):\n",
                "        low_f = low_files[i]\n",
                "        high_f = high_files[i]\n",
                "\n",
                "        try:\n",
                "            # DICOM Oku\n",
                "            low_dcm = pydicom.dcmread(os.path.join(low_path, low_f))\n",
                "            high_dcm = pydicom.dcmread(os.path.join(high_path, high_f))\n",
                "\n",
                "            # --- HU DÖNÜŞÜMÜ ---\n",
                "\n",
                "            # Low Dose için katsayılar\n",
                "            intercept_low = low_dcm.RescaleIntercept if 'RescaleIntercept' in low_dcm else 0\n",
                "            slope_low = low_dcm.RescaleSlope if 'RescaleSlope' in low_dcm else 1\n",
                "\n",
                "            # High Dose için katsayılar\n",
                "            intercept_high = high_dcm.RescaleIntercept if 'RescaleIntercept' in high_dcm else 0\n",
                "            slope_high = high_dcm.RescaleSlope if 'RescaleSlope' in high_dcm else 1\n",
                "\n",
                "            # Ham veriyi HU birimine çeviriyoruz\n",
                "            low_img = low_dcm.pixel_array.astype(np.float32) * slope_low + intercept_low\n",
                "            high_img = high_dcm.pixel_array.astype(np.float32) * slope_high + intercept_high\n",
                "\n",
                "            # --- NORMALIZASYON ---\n",
                "\n",
                "            # 1. Windowing\n",
                "            low_img = np.clip(low_img, HU_MIN, HU_MAX)\n",
                "            high_img = np.clip(high_img, HU_MIN, HU_MAX)\n",
                "\n",
                "            # 2. Resize\n",
                "            low_img = cv2.resize(low_img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
                "            high_img = cv2.resize(high_img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
                "\n",
                "            # 3. Normalize (-1 ile 1 arası)\n",
                "            low_img = (low_img - HU_MIN) / (HU_MAX - HU_MIN)\n",
                "            high_img = (high_img - HU_MIN) / (HU_MAX - HU_MIN)\n",
                "\n",
                "            low_img = (low_img * 2) - 1\n",
                "            high_img = (high_img * 2) - 1\n",
                "\n",
                "            # 4. Kaydet\n",
                "            save_name = f\"{phantom}_{i:04d}.npy\"\n",
                "\n",
                "            np.save(os.path.join(output_dir, \"trainA\", save_name), low_img.astype(np.float32))\n",
                "            np.save(os.path.join(output_dir, \"trainB\", save_name), high_img.astype(np.float32))\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"   Hata ({phantom} - {i}): {e}\")\n",
                "            continue\n",
                "\n",
                "    print(f\"{phantom} bitti.\")\n",
                "\n",
                "print(\"\\n TÜM İŞLEMLER BAŞARIYLA TAMAMLANDI.\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}