{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Model Eğitimi\n",
                "\n",
                "Bu notebook hazırlanan verileri kullanarak modeli eğitir.\n",
                "\n",
                "**İçerik:**\n",
                "1. Veri yükleme (NPY Dataset)\n",
                "2. Model oluşturma\n",
                "3. Eğitim döngüsü\n",
                "4. Checkpoint kaydetme\n",
                "\n",
                "---\n",
                "\n",
                "## Önkoşullar\n",
                "\n",
                "Çalıştırmadan önce:\n",
                "- `02_data_preprocessing.ipynb` ile verileri hazırlayın\n",
                "- GPU kullanımı önerilir (Colab Pro veya lokal GPU)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Google Colab için\n",
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Model Mimarisini Yükle\n",
                "\n",
                "İlk notebook'tan model tanımlarını alıyoruz."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%run 01_model_architecture.ipynb"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Veri Yükleyici\n",
                "\n",
                "Keras Sequence API kullanarak memory-efficient veri yükleme. Her epoch sonunda shuffle yapılıyor."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "import numpy as np\n",
                "import tensorflow as tf\n",
                "from sklearn.model_selection import train_test_split\n",
                "\n",
                "# === AYARLAR ===\n",
                "# Batch size'ı 4 yaptık, hem hızlı olur hem de RAM'i patlatmaz.\n",
                "BATCH_SIZE = 4\n",
                "IMG_WIDTH = 256\n",
                "IMG_HEIGHT = 256\n",
                "\n",
                "# === GÜNCELLENMİŞ SHUFFLE ÖZELLİKLİ DATASET SINIFI ===\n",
                "class NPYDataset(tf.keras.utils.Sequence):\n",
                "    def __init__(self, file_list_A, file_list_B, batch_size=1, shuffle=True):\n",
                "        self.files_A = np.array(file_list_A) # Numpy array yapıyoruz\n",
                "        self.files_B = np.array(file_list_B)\n",
                "        self.batch_size = batch_size\n",
                "        self.shuffle = shuffle\n",
                "        self.indexes = np.arange(len(self.files_A)) # İndeks listesi\n",
                "\n",
                "        print(f\"Veri listesi hazırlandı... ({len(self.files_A)} adet dosya)\")\n",
                "\n",
                "        # Başlarken bir kere karıştıralım\n",
                "        if self.shuffle:\n",
                "            self.on_epoch_end()\n",
                "\n",
                "    def __len__(self):\n",
                "        return int(np.floor(len(self.files_A) / self.batch_size))\n",
                "\n",
                "    def __getitem__(self, index):\n",
                "        # O anki batch için indeksleri seç\n",
                "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
                "\n",
                "        batch_A = []\n",
                "        batch_B = []\n",
                "\n",
                "        for k in indexes:\n",
                "            # Dosyayı yükle\n",
                "            imgA = np.load(self.files_A[k])\n",
                "            imgB = np.load(self.files_B[k])\n",
                "\n",
                "            # Boyut Kontrolü (H, W) -> (H, W, 1)\n",
                "            if imgA.ndim == 2:\n",
                "                imgA = np.expand_dims(imgA, axis=-1)\n",
                "            if imgB.ndim == 2:\n",
                "                imgB = np.expand_dims(imgB, axis=-1)\n",
                "\n",
                "            batch_A.append(imgA)\n",
                "            batch_B.append(imgB)\n",
                "\n",
                "        return np.array(batch_A), np.array(batch_B)\n",
                "\n",
                "    def on_epoch_end(self):\n",
                "        # Her epoch sonunda verileri karıştır (Shuffle)\n",
                "        if self.shuffle:\n",
                "            np.random.shuffle(self.indexes)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Verileri Yükle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === VERİYİ OKUMA VE AYIRMA ===\n",
                "dataset_path = r\"/content/drive/MyDrive/Tasarım Dersi/Projects/data/processed_data_npy\"\n",
                "\n",
                "# Tüm dosya yollarını diskten alıyoruz\n",
                "all_files_A = sorted(glob.glob(os.path.join(dataset_path, 'trainA') + '/*.npy'))\n",
                "all_files_B = sorted(glob.glob(os.path.join(dataset_path, 'trainB') + '/*.npy'))\n",
                "\n",
                "print(f\"Toplam bulunan dosya (trainA): {len(all_files_A)}\")\n",
                "print(f\"Toplam bulunan dosya (trainB): {len(all_files_B)}\")\n",
                "\n",
                "# Dosya isimlerini al\n",
                "filenames_A = {os.path.basename(f) for f in all_files_A}\n",
                "filenames_B = {os.path.basename(f) for f in all_files_B}\n",
                "\n",
                "# Eksik dosyaları bul\n",
                "missing_in_B = filenames_A - filenames_B\n",
                "missing_in_A = filenames_B - filenames_A\n",
                "\n",
                "if missing_in_A:\n",
                "    print(f\"HATA: trainA içinde karşılığı olmayan dosya(lar) (trainB'de var): {missing_in_A}\")\n",
                "if missing_in_B:\n",
                "    print(f\"HATA: trainB içinde karşılığı olmayan dosya(lar) (trainA'da var): {missing_in_B}\")\n",
                "\n",
                "# Sadece eşleşen dosyaları al\n",
                "common_filenames = sorted(list(filenames_A.intersection(filenames_B)))\n",
                "\n",
                "# Yeni dosya listelerini oluştur\n",
                "filtered_files_A = [os.path.join(dataset_path, 'trainA', f) for f in common_filenames]\n",
                "filtered_files_B = [os.path.join(dataset_path, 'trainB', f) for f in common_filenames]\n",
                "\n",
                "print(f\"Eşleşen dosya sayısı: {len(filtered_files_A)}\")\n",
                "\n",
                "# %90 Train, %10 Validation ayırma\n",
                "train_A, val_A, train_B, val_B = train_test_split(filtered_files_A, filtered_files_B, test_size=0.10, random_state=42)\n",
                "\n",
                "print(f\"Eğitim Seti: {len(train_A)} adet\")\n",
                "print(f\"Test/Doğrulama Seti: {len(val_A)} adet\")\n",
                "\n",
                "# === DATASETLERİ OLUŞTURMA (Shuffle Eklendi) ===\n",
                "# Train setini karıştırıyoruz (True), Validation sabit kalabilir (False)\n",
                "train_dataset = NPYDataset(train_A, train_B, batch_size=BATCH_SIZE, shuffle=True)\n",
                "val_dataset = NPYDataset(val_A, val_B, batch_size=BATCH_SIZE, shuffle=False)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. GPU Kontrolü"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# === GPU KONTROL ===\n",
                "\n",
                "import tensorflow as tf\n",
                "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Eğitim Ayarları ve Callback\n",
                "\n",
                "Eğitim sırasında:\n",
                "- Her epoch sonunda örnek görseller kaydedilir\n",
                "- Her 5 epoch'ta model checkpoint'u alınır"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import keras\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import tensorflow as tf\n",
                "\n",
                "# === AYARLAR ===\n",
                "LEARNING_RATE_G = 2e-4\n",
                "LEARNING_RATE_D = 2e-4\n",
                "\n",
                "# === 1. ADIM: KLASÖRLER ===\n",
                "project_root = r\"D:\\Projects\"\n",
                "results_dir = os.path.join(project_root, \"results\")\n",
                "checkpoint_dir = os.path.join(project_root, \"model_checkpoints\")\n",
                "\n",
                "os.makedirs(results_dir, exist_ok=True)\n",
                "os.makedirs(checkpoint_dir, exist_ok=True)\n",
                "\n",
                "# === 2. ADIM: MODELİ DERLE ===\n",
                "hybrid_gan = WGAN_GP_Pix2Pix(generator=generator, discriminator=discriminator)\n",
                "\n",
                "hybrid_gan.compile(\n",
                "    d_optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_D, beta_1=0.5, beta_2=0.9),\n",
                "    g_optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_G, beta_1=0.5, beta_2=0.9)\n",
                ")\n",
                "\n",
                "# === 3. ADIM: GÖRSEL İZLEME VE KAYIT ===\n",
                "class GANMonitor(keras.callbacks.Callback):\n",
                "    def __init__(self, val_dataset, num_img=3):\n",
                "        self.val_dataset = val_dataset\n",
                "        self.num_img = num_img\n",
                "\n",
                "    def on_epoch_end(self, epoch, logs=None):\n",
                "        # --- A. GÖRSEL KAYIT (HER EPOCH) ---\n",
                "        print(f\"\\nEpoch {epoch+1} bitti. Görseller işleniyor...\")\n",
                "\n",
                "        try:\n",
                "            # Rastgele veri çek\n",
                "            idx = np.random.randint(0, len(self.val_dataset))\n",
                "            inp, tar = self.val_dataset[idx]\n",
                "            prediction = self.model.generator(inp, training=False)\n",
                "\n",
                "            title = ['Input (LD)', 'Generated (AI)', 'Target (HD)']\n",
                "            display_count = min(self.num_img, inp.shape[0])\n",
                "\n",
                "            for i in range(display_count):\n",
                "                img_list = [inp[i], prediction[i], tar[i]]\n",
                "\n",
                "                plt.figure(figsize=(12, 4))\n",
                "                for j in range(3):\n",
                "                    plt.subplot(1, 3, j+1)\n",
                "                    plt.title(title[j])\n",
                "\n",
                "                    # Veriyi alıp en küçük ve en büyüğe göre 0-1 arasına çekiyoruz.\n",
                "                    # Bu sayede veri bozuk bile olsa ekranda gri gözükür.\n",
                "                    img_data = img_list[j][:, :, 0]\n",
                "                    _min, _max = np.min(img_data), np.max(img_data)\n",
                "\n",
                "                    if _max - _min > 0:\n",
                "                        show_img = (img_data - _min) / (_max - _min)\n",
                "                    else:\n",
                "                        show_img = img_data\n",
                "\n",
                "                    plt.imshow(show_img, cmap='gray')\n",
                "                    plt.axis('off')\n",
                "\n",
                "                filename = f\"epoch_{epoch+1}_{i}.png\"\n",
                "                save_path = os.path.join(results_dir, filename)\n",
                "                plt.savefig(save_path)\n",
                "                plt.close()\n",
                "\n",
                "            # --- B. MODEL KAYIT (HER 5 EPOCH) ---\n",
                "            if (epoch+1) % 5 == 0:\n",
                "                model_name = f\"G_epoch_{epoch+1}.h5\"\n",
                "                ckpt_path = os.path.join(checkpoint_dir, model_name)\n",
                "                self.model.generator.save_weights(ckpt_path)\n",
                "                print(f\"✅ Model Kaydedildi: {ckpt_path}\")\n",
                "\n",
                "        except Exception as e:\n",
                "            print(f\"Kayıt hatası: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Eğitimi Başlat\n",
                "\n",
                "50 epoch eğitim yaklaşık 4-6 saat sürer (GPU'ya bağlı)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"Eğitim başlıyor...\")\n",
                "\n",
                "hybrid_gan.fit(\n",
                "    train_dataset,\n",
                "    validation_data=val_dataset,\n",
                "    epochs=50,\n",
                "    callbacks=[GANMonitor(val_dataset)]\n",
                ")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}