{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1L9n-S2R8Kx"
      },
      "source": [
        "# (1) New Dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYlunfAQqEVl"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# ANA YOLU BELÄ°RLE\n",
        "base_dir = r\"D:\\data\\manifest-1755254936487\\LDCT-and-Projection-data\"\n",
        "\n",
        "print(f\"Ana yol: {base_dir}\")\n",
        "\n",
        "# C ile baÅŸlayan hastalarÄ± bul ve ilk hastanÄ±n iÃ§eriÄŸini listele\n",
        "if os.path.exists(base_dir):\n",
        "    hastalar = [d for d in os.listdir(base_dir) if d.startswith(\"C\")]\n",
        "\n",
        "    if len(hastalar) > 0:\n",
        "        ilk_hasta = hastalar[0] # Ä°lk C ile baÅŸlayan hasta\n",
        "        hasta_yolu = os.path.join(base_dir, ilk_hasta)\n",
        "\n",
        "        print(f\"\\n1. SEVÄ°YE ({ilk_hasta} iÃ§i):\")\n",
        "        print(os.listdir(hasta_yolu))\n",
        "\n",
        "        # Ä°Ã§erideki ilk alt klasÃ¶re bak\n",
        "        icindekiler = os.listdir(hasta_yolu)\n",
        "        if icindekiler:\n",
        "            ilk_alt = icindekiler[0]\n",
        "            alt_yol = os.path.join(hasta_yolu, ilk_alt)\n",
        "\n",
        "            if os.path.isdir(alt_yol):\n",
        "                print(f\"\\n2. SEVÄ°YE ({ilk_alt} iÃ§i):\")\n",
        "                print(os.listdir(alt_yol))\n",
        "\n",
        "                # Daha derine in\n",
        "                derin_icindekiler = os.listdir(alt_yol)\n",
        "                if derin_icindekiler:\n",
        "                    derin_alt = derin_icindekiler[0]\n",
        "                    derin_yol = os.path.join(alt_yol, derin_alt)\n",
        "                    if os.path.isdir(derin_yol):\n",
        "                         print(f\"\\n3. SEVÄ°YE ({derin_alt} iÃ§i):\")\n",
        "                         print(os.listdir(derin_yol))\n",
        "    else:\n",
        "        print(\"C ile baÅŸlayan klasÃ¶r bulunamadÄ±!\")\n",
        "else:\n",
        "    print(\"Ana yol yanlÄ±ÅŸ!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0C-vuIMEAxb7"
      },
      "source": [
        "###Â This time we are going to use the DICOM format directly instead of converting it to PNG. Otherwise the images will be scientifically useless."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U10xP5RZqEVm"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pydicom\n",
        "import cv2\n",
        "import numpy as np\n",
        "\n",
        "# === GÃœNCELLENMÄ°Å DOSYA YOLLARI ===\n",
        "# Senin verdiÄŸin yeni doÄŸru adres:\n",
        "base_dir = r\"D:\\data\\manifest-1755254936487\\LDCT-and-Projection-data\"\n",
        "\n",
        "# Ã‡Ä±ktÄ± klasÃ¶rÃ¼nÃ¼ de kontrol et istersen, buraya kaydedecek:\n",
        "output_dir = r\"D:\\Projects\\data\\processed_data_npy\"\n",
        "\n",
        "os.makedirs(os.path.join(output_dir, \"trainA\"), exist_ok=True) # Low Dose\n",
        "os.makedirs(os.path.join(output_dir, \"trainB\"), exist_ok=True) # High Dose\n",
        "\n",
        "# === AYARLAR ===\n",
        "HU_MIN = -1000\n",
        "HU_MAX = 1000\n",
        "IMG_SIZE = (256, 256)\n",
        "\n",
        "def find_dose_folders(patient_path):\n",
        "    low_p = None\n",
        "    high_p = None\n",
        "\n",
        "    # TÃ¼m alt klasÃ¶rleri gez\n",
        "    for root, dirs, files in os.walk(patient_path):\n",
        "        for d in dirs:\n",
        "            d_lower = d.lower()\n",
        "\n",
        "            # --- FÄ°LTRE ---\n",
        "            # 'proj' veya 'sino' iÃ§eren klasÃ¶rleri atla\n",
        "            if \"proj\" in d_lower or \"sino\" in d_lower:\n",
        "                continue\n",
        "\n",
        "            # KlasÃ¶r eÅŸleÅŸtirme\n",
        "            if \"low dose\" in d_lower:\n",
        "                low_p = os.path.join(root, d)\n",
        "            elif \"full dose\" in d_lower or \"high dose\" in d_lower:\n",
        "                high_p = os.path.join(root, d)\n",
        "\n",
        "    return low_p, high_p\n",
        "\n",
        "\n",
        "# C ve L ile baÅŸlayan hastalarÄ± alÄ±yoruz\n",
        "try:\n",
        "    phantoms = [d for d in os.listdir(base_dir)\n",
        "                if d.startswith((\"C\", \"L\"))\n",
        "                and os.path.isdir(os.path.join(base_dir, d))]\n",
        "\n",
        "    phantoms = sorted(phantoms)\n",
        "    print(f\"Bulunan Hastalar ({len(phantoms)} adet): {phantoms}\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(f\"HATA: '{base_dir}' yolu hala bulunamÄ±yor. LÃ¼tfen D sÃ¼rÃ¼cÃ¼sÃ¼nÃ¼ kontrol et.\")\n",
        "    phantoms = []\n",
        "\n",
        "print(\"\\n--- Ä°ÅŸlem BaÅŸlÄ±yor ---\\n\")\n",
        "\n",
        "for phantom in phantoms:\n",
        "    patient_path = os.path.join(base_dir, phantom)\n",
        "\n",
        "    # Low ve High Dose klasÃ¶rlerini bul\n",
        "    low_path, high_path = find_dose_folders(patient_path)\n",
        "\n",
        "    # KlasÃ¶rler bulundu mu kontrolÃ¼\n",
        "    if not low_path or not high_path:\n",
        "        print(f\" KlasÃ¶rler TAM bulunamadÄ± (AtlanÄ±yor -> Sadece projeksiyon olabilir): {phantom}\")\n",
        "        continue\n",
        "\n",
        "    print(f\" {phantom} iÅŸleniyor...\")\n",
        "\n",
        "    low_files = sorted(os.listdir(low_path))\n",
        "    high_files = sorted(os.listdir(high_path))\n",
        "\n",
        "    min_len = min(len(low_files), len(high_files))\n",
        "\n",
        "    for i in range(min_len):\n",
        "        low_f = low_files[i]\n",
        "        high_f = high_files[i]\n",
        "\n",
        "        try:\n",
        "            # DICOM Oku\n",
        "            low_dcm = pydicom.dcmread(os.path.join(low_path, low_f))\n",
        "            high_dcm = pydicom.dcmread(os.path.join(high_path, high_f))\n",
        "\n",
        "            # --- DÃœZELTME BAÅLANGIÃ‡: HU DÃ¶nÃ¼ÅŸÃ¼mÃ¼ (Intercept & Slope Eklendi) ---\n",
        "\n",
        "            # Low Dose iÃ§in katsayÄ±lar\n",
        "            intercept_low = low_dcm.RescaleIntercept if 'RescaleIntercept' in low_dcm else 0\n",
        "            slope_low = low_dcm.RescaleSlope if 'RescaleSlope' in low_dcm else 1\n",
        "\n",
        "            # High Dose iÃ§in katsayÄ±lar\n",
        "            intercept_high = high_dcm.RescaleIntercept if 'RescaleIntercept' in high_dcm else 0\n",
        "            slope_high = high_dcm.RescaleSlope if 'RescaleSlope' in high_dcm else 1\n",
        "\n",
        "            # Ham veriyi HU birimine Ã§eviriyoruz\n",
        "            low_img = low_dcm.pixel_array.astype(np.float32) * slope_low + intercept_low\n",
        "            high_img = high_dcm.pixel_array.astype(np.float32) * slope_high + intercept_high\n",
        "\n",
        "            # --- DÃœZELTME BÄ°TÄ°Å ---\n",
        "\n",
        "            # 1. Windowing\n",
        "            low_img = np.clip(low_img, HU_MIN, HU_MAX)\n",
        "            high_img = np.clip(high_img, HU_MIN, HU_MAX)\n",
        "\n",
        "            # 2. Resize\n",
        "            low_img = cv2.resize(low_img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
        "            high_img = cv2.resize(high_img, IMG_SIZE, interpolation=cv2.INTER_AREA)\n",
        "\n",
        "            # 3. Normalize (-1 ile 1 arasÄ±)\n",
        "            low_img = (low_img - HU_MIN) / (HU_MAX - HU_MIN)\n",
        "            high_img = (high_img - HU_MIN) / (HU_MAX - HU_MIN)\n",
        "\n",
        "            low_img = (low_img * 2) - 1\n",
        "            high_img = (high_img * 2) - 1\n",
        "\n",
        "            # 4. Kaydet\n",
        "            save_name = f\"{phantom}_{i:04d}.npy\"\n",
        "\n",
        "            np.save(os.path.join(output_dir, \"trainA\", save_name), low_img.astype(np.float32))\n",
        "            np.save(os.path.join(output_dir, \"trainB\", save_name), high_img.astype(np.float32))\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"   Hata ({phantom} - {i}): {e}\")\n",
        "            continue\n",
        "\n",
        "    print(f\"{phantom} bitti.\")\n",
        "\n",
        "print(\"\\n TÃœM Ä°ÅLEMLER BAÅARIYLA TAMAMLANDI.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8h_JzqdH5bl"
      },
      "source": [
        "# (2) Model Building."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CdSqLSJ4H826"
      },
      "source": [
        "##Â This time the model will be a hybrid Pix2Pix + WGAN GP Model. Changes have been made accordingly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eWj7eAQKIVH1"
      },
      "source": [
        "### This part reads the data from the disk in .npy format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q-tUVH7rqEVn",
        "outputId": "23894126-aa65-47d7-eeb4-49f9d0228d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Toplam bulunan dosya (trainA): 25473\n",
            "Toplam bulunan dosya (trainB): 25472\n",
            "HATA: trainB iÃ§inde karÅŸÄ±lÄ±ÄŸÄ± olmayan dosya(lar) (trainA'da var): {'C261_0000 (1).npy'}\n",
            "EÅŸleÅŸen dosya sayÄ±sÄ±: 25472\n",
            "EÄŸitim Seti: 22924 adet\n",
            "Test/DoÄŸrulama Seti: 2548 adet\n",
            "Veri listesi hazÄ±rlandÄ±... (22924 adet dosya)\n",
            "Veri listesi hazÄ±rlandÄ±... (2548 adet dosya)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# === AYARLAR ===\n",
        "# Batch size'Ä± 4 yaptÄ±k, hem hÄ±zlÄ± olur hem de RAM'i patlatmaz.\n",
        "BATCH_SIZE = 4\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "\n",
        "# === GÃœNCELLENMÄ°Å SHUFFLE Ã–ZELLÄ°KLÄ° DATASET SINIFI ===\n",
        "class NPYDataset(tf.keras.utils.Sequence):\n",
        "    def __init__(self, file_list_A, file_list_B, batch_size=1, shuffle=True):\n",
        "        self.files_A = np.array(file_list_A) # Numpy array yapÄ±yoruz\n",
        "        self.files_B = np.array(file_list_B)\n",
        "        self.batch_size = batch_size\n",
        "        self.shuffle = shuffle\n",
        "        self.indexes = np.arange(len(self.files_A)) # Ä°ndeks listesi\n",
        "\n",
        "        print(f\"Veri listesi hazÄ±rlandÄ±... ({len(self.files_A)} adet dosya)\")\n",
        "\n",
        "        # BaÅŸlarken bir kere karÄ±ÅŸtÄ±ralÄ±m\n",
        "        if self.shuffle:\n",
        "            self.on_epoch_end()\n",
        "\n",
        "    def __len__(self):\n",
        "        return int(np.floor(len(self.files_A) / self.batch_size))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # O anki batch iÃ§in indeksleri seÃ§\n",
        "        indexes = self.indexes[index * self.batch_size:(index + 1) * self.batch_size]\n",
        "\n",
        "        batch_A = []\n",
        "        batch_B = []\n",
        "\n",
        "        for k in indexes:\n",
        "            # DosyayÄ± yÃ¼kle\n",
        "            imgA = np.load(self.files_A[k])\n",
        "            imgB = np.load(self.files_B[k])\n",
        "\n",
        "            # Boyut KontrolÃ¼ (H, W) -> (H, W, 1)\n",
        "            if imgA.ndim == 2:\n",
        "                imgA = np.expand_dims(imgA, axis=-1)\n",
        "            if imgB.ndim == 2:\n",
        "                imgB = np.expand_dims(imgB, axis=-1)\n",
        "\n",
        "            batch_A.append(imgA)\n",
        "            batch_B.append(imgB)\n",
        "\n",
        "        return np.array(batch_A), np.array(batch_B)\n",
        "\n",
        "    def on_epoch_end(self):\n",
        "        # Her epoch sonunda verileri karÄ±ÅŸtÄ±r (Shuffle)\n",
        "        if self.shuffle:\n",
        "            np.random.shuffle(self.indexes)\n",
        "\n",
        "# === VERÄ°YÄ° OKUMA VE AYIRMA ===\n",
        "dataset_path = r\"/content/drive/MyDrive/TasarÄ±m Dersi/Projects/data/processed_data_npy\"\n",
        "\n",
        "# TÃ¼m dosya yollarÄ±nÄ± diskten alÄ±yoruz\n",
        "all_files_A = sorted(glob.glob(os.path.join(dataset_path, 'trainA') + '/*.npy'))\n",
        "all_files_B = sorted(glob.glob(os.path.join(dataset_path, 'trainB') + '/*.npy'))\n",
        "\n",
        "print(f\"Toplam bulunan dosya (trainA): {len(all_files_A)}\")\n",
        "print(f\"Toplam bulunan dosya (trainB): {len(all_files_B)}\")\n",
        "\n",
        "# Dosya isimlerini al\n",
        "filenames_A = {os.path.basename(f) for f in all_files_A}\n",
        "filenames_B = {os.path.basename(f) for f in all_files_B}\n",
        "\n",
        "# Eksik dosyalarÄ± bul\n",
        "missing_in_B = filenames_A - filenames_B\n",
        "missing_in_A = filenames_B - filenames_A\n",
        "\n",
        "if missing_in_A:\n",
        "    print(f\"HATA: trainA iÃ§inde karÅŸÄ±lÄ±ÄŸÄ± olmayan dosya(lar) (trainB'de var): {missing_in_A}\")\n",
        "if missing_in_B:\n",
        "    print(f\"HATA: trainB iÃ§inde karÅŸÄ±lÄ±ÄŸÄ± olmayan dosya(lar) (trainA'da var): {missing_in_B}\")\n",
        "\n",
        "# Sadece eÅŸleÅŸen dosyalarÄ± al\n",
        "common_filenames = sorted(list(filenames_A.intersection(filenames_B)))\n",
        "\n",
        "# Yeni dosya listelerini oluÅŸtur\n",
        "filtered_files_A = [os.path.join(dataset_path, 'trainA', f) for f in common_filenames]\n",
        "filtered_files_B = [os.path.join(dataset_path, 'trainB', f) for f in common_filenames]\n",
        "\n",
        "print(f\"EÅŸleÅŸen dosya sayÄ±sÄ±: {len(filtered_files_A)}\")\n",
        "\n",
        "# %90 Train, %10 Validation ayÄ±rma\n",
        "train_A, val_A, train_B, val_B = train_test_split(filtered_files_A, filtered_files_B, test_size=0.10, random_state=42)\n",
        "\n",
        "print(f\"EÄŸitim Seti: {len(train_A)} adet\")\n",
        "print(f\"Test/DoÄŸrulama Seti: {len(val_A)} adet\")\n",
        "\n",
        "# === DATASETLERÄ° OLUÅTURMA (Shuffle Eklendi) ===\n",
        "# Train setini karÄ±ÅŸtÄ±rÄ±yoruz (True), Validation sabit kalabilir (False)\n",
        "train_dataset = NPYDataset(train_A, train_B, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_dataset = NPYDataset(val_A, val_B, batch_size=BATCH_SIZE, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ek3IO7pn4MwR",
        "outputId": "60096c85-04b5-4bcd-b5b4-9d39844cadeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU0LXWTZI1St"
      },
      "source": [
        "### Generator (U Net) and Discriminator (Critic). This time we are NOT using the Sigmoid function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yuyDVrxlJ6BC"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "IMG_WIDTH = 256\n",
        "IMG_HEIGHT = 256\n",
        "CHANNELS = 1\n",
        "\n",
        "def downsample(filters, size, apply_batchnorm=True):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2D(filters, size, strides=2, padding='same',\n",
        "                             kernel_initializer=initializer, use_bias=False))\n",
        "    if apply_batchnorm:\n",
        "        result.add(layers.BatchNormalization())\n",
        "    result.add(layers.LeakyReLU())\n",
        "    return result\n",
        "\n",
        "def upsample(filters, size, apply_dropout=False):\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    result = keras.Sequential()\n",
        "    result.add(layers.Conv2DTranspose(filters, size, strides=2, padding='same',\n",
        "                                      kernel_initializer=initializer, use_bias=False))\n",
        "    result.add(layers.BatchNormalization())\n",
        "    if apply_dropout:\n",
        "        result.add(layers.Dropout(0.5))\n",
        "    result.add(layers.ReLU())\n",
        "    return result\n",
        "\n",
        "def build_generator():\n",
        "    inputs = layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, CHANNELS])\n",
        "\n",
        "    # Encoder\n",
        "    down_stack = [\n",
        "        downsample(64, 4, apply_batchnorm=False), # (bs, 128, 128, 64)\n",
        "        downsample(128, 4), # (bs, 64, 64, 128)\n",
        "        downsample(256, 4), # (bs, 32, 32, 256)\n",
        "        downsample(512, 4), # (bs, 16, 16, 512)\n",
        "        downsample(512, 4), # (bs, 8, 8, 512)\n",
        "        downsample(512, 4), # (bs, 4, 4, 512)\n",
        "        downsample(512, 4), # (bs, 2, 2, 512)\n",
        "        downsample(512, 4), # (bs, 1, 1, 512)\n",
        "    ]\n",
        "\n",
        "    # Decoder\n",
        "    up_stack = [\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4, apply_dropout=True),\n",
        "        upsample(512, 4),\n",
        "        upsample(256, 4),\n",
        "        upsample(128, 4),\n",
        "        upsample(64, 4),\n",
        "    ]\n",
        "\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "    last = layers.Conv2DTranspose(CHANNELS, 4, strides=2, padding='same',\n",
        "                                  kernel_initializer=initializer, activation='tanh')\n",
        "\n",
        "    x = inputs\n",
        "    skips = []\n",
        "    for down in down_stack:\n",
        "        x = down(x)\n",
        "        skips.append(x)\n",
        "\n",
        "    skips = reversed(skips[:-1])\n",
        "\n",
        "    for up, skip in zip(up_stack, skips):\n",
        "        x = up(x)\n",
        "        x = layers.Concatenate()([x, skip])\n",
        "\n",
        "    x = last(x)\n",
        "    return keras.Model(inputs=inputs, outputs=x)\n",
        "\n",
        "def build_discriminator():\n",
        "    #   Sigmoid fonksiyonu yerine WGAN-GP kullanÄ±lmÄ±ÅŸtÄ±r.\n",
        "    initializer = tf.random_normal_initializer(0., 0.02)\n",
        "\n",
        "    inp = layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, CHANNELS], name='input_image')\n",
        "    tar = layers.Input(shape=[IMG_WIDTH, IMG_HEIGHT, CHANNELS], name='target_image')\n",
        "\n",
        "    x = layers.Concatenate()([inp, tar]) # (bs, 256, 256, channels*2)\n",
        "\n",
        "    down1 = downsample(64, 4, False)(x)\n",
        "    down2 = downsample(128, 4)(down1)\n",
        "    down3 = downsample(256, 4)(down2)\n",
        "\n",
        "    # Zero Padding ve Conv\n",
        "    zero_pad1 = layers.ZeroPadding2D()(down3)\n",
        "    conv = layers.Conv2D(512, 4, strides=1, kernel_initializer=initializer, use_bias=False)(zero_pad1)\n",
        "    batchnorm1 = layers.BatchNormalization()(conv)\n",
        "    leaky_relu = layers.LeakyReLU()(batchnorm1)\n",
        "\n",
        "    zero_pad2 = layers.ZeroPadding2D()(leaky_relu)\n",
        "\n",
        "    last = layers.Conv2D(1, 4, strides=1, kernel_initializer=initializer)(zero_pad2)\n",
        "\n",
        "    return keras.Model(inputs=[inp, tar], outputs=last)\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QdhHODSPJ9FO"
      },
      "source": [
        "### Hybrid Model Class (Pix2Pix + WGAN GP)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u9-eAhKMKGiv"
      },
      "outputs": [],
      "source": [
        "class WGAN_GP_Pix2Pix(keras.Model):\n",
        "    def __init__(self, generator, discriminator, lambda_gp=10.0, lambda_l1=100.0):\n",
        "        super(WGAN_GP_Pix2Pix, self).__init__()\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.lambda_gp = lambda_gp # Gradient Penalty aÄŸÄ±rlÄ±ÄŸÄ±\n",
        "        self.lambda_l1 = lambda_l1 # L1 (Pix2Pix) aÄŸÄ±rlÄ±ÄŸÄ±\n",
        "\n",
        "    def compile(self, d_optimizer, g_optimizer):\n",
        "        super(WGAN_GP_Pix2Pix, self).compile()\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_loss_fn = self.wasserstein_loss\n",
        "        self.g_loss_fn = self.wasserstein_loss\n",
        "        self.l1_loss_fn = tf.keras.losses.MeanAbsoluteError()\n",
        "\n",
        "    def wasserstein_loss(self, y_true, y_pred):\n",
        "        return tf.reduce_mean(y_true * y_pred)\n",
        "\n",
        "    def gradient_penalty(self, batch_size, real_images, fake_images, input_images):\n",
        "        \"\"\" GP Hesaplama: Real ve Fake arasÄ± interpolasyon \"\"\"\n",
        "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
        "        diff = fake_images - real_images\n",
        "        interpolated = real_images + alpha * diff\n",
        "\n",
        "        with tf.GradientTape() as gp_tape:\n",
        "            gp_tape.watch(interpolated)\n",
        "            # Discriminator'a hem input(LD) hem interpolasyon verilir\n",
        "            pred = self.discriminator([input_images, interpolated], training=True)\n",
        "\n",
        "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
        "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
        "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
        "        return gp\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        if isinstance(inputs, (list, tuple)):\n",
        "            inputs = inputs[0]\n",
        "        return self.generator(inputs, training=training)\n",
        "\n",
        "    def train_step(self, data):\n",
        "        # Data Loader'dan gelen veri: (input_image, target_image)\n",
        "        input_image, target_image = data\n",
        "        batch_size = tf.shape(input_image)[0]\n",
        "\n",
        "        # --- DISCRIMINATOR EÄÄ°TÄ°MÄ° ---\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_image = self.generator(input_image, training=True)\n",
        "\n",
        "            fake_pred = self.discriminator([input_image, fake_image], training=True)\n",
        "            real_pred = self.discriminator([input_image, target_image], training=True)\n",
        "\n",
        "            # Wasserstein Loss: D(fake) - D(real)\n",
        "\n",
        "            # Not: Real iÃ§in -1, Fake iÃ§in 1 gibi davranÄ±lÄ±r, formÃ¼l minimize etmek Ã¼zerine kuruludur.\n",
        "\n",
        "            d_cost = tf.reduce_mean(fake_pred) - tf.reduce_mean(real_pred)\n",
        "\n",
        "            # Gradient Penalty\n",
        "            gp = self.gradient_penalty(batch_size, target_image, fake_image, input_image)\n",
        "\n",
        "            # Toplam D Loss\n",
        "            d_loss = d_cost + (gp * self.lambda_gp)\n",
        "\n",
        "        d_grad = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
        "        self.d_optimizer.apply_gradients(zip(d_grad, self.discriminator.trainable_variables))\n",
        "\n",
        "        # --- GENERATOR EÄÄ°TÄ°MÄ° ---\n",
        "        with tf.GradientTape() as tape:\n",
        "            fake_image = self.generator(input_image, training=True)\n",
        "            fake_pred = self.discriminator([input_image, fake_image], training=True)\n",
        "\n",
        "            # G Loss (Wasserstein KÄ±smÄ±)\n",
        "            g_wgan_loss = -tf.reduce_mean(fake_pred)\n",
        "\n",
        "            # G Loss (L1 KÄ±smÄ±): Orijinal Pix2Pix yapÄ±sÄ± (GÃ¶rÃ¼ntÃ¼ benzerliÄŸi)\n",
        "            g_l1_loss = self.l1_loss_fn(target_image, fake_image) * self.lambda_l1\n",
        "\n",
        "            g_loss = g_wgan_loss + g_l1_loss\n",
        "\n",
        "        g_grad = tape.gradient(g_loss, self.generator.trainable_variables)\n",
        "        self.g_optimizer.apply_gradients(zip(g_grad, self.generator.trainable_variables))\n",
        "\n",
        "        return {\"d_loss\": d_loss, \"g_loss\": g_loss, \"g_l1\": g_l1_loss}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSAdIAaAKUmf"
      },
      "source": [
        "# (3) Compiling and Training the Model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-XyMqhFDMR32",
        "outputId": "1e184297-aa21-4900-a538-c86db51174c1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "# === GPU KONTROL ===\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "dPYPo53vKNOX",
        "outputId": "336c153b-47db-40b4-ed49-5921f016d516"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'print(\"EÄŸitim baÅŸlÄ±yor... (Eski performans, yeni kayÄ±t sistemi)\")\\n\\nhybrid_gan.fit(\\n    train_dataset,\\n    validation_data=val_dataset,\\n    epochs=50,\\n    callbacks=[GANMonitor(val_dataset)]\\n)'"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "\n",
        "# === AYARLAR ===\n",
        "LEARNING_RATE_G = 2e-4\n",
        "LEARNING_RATE_D = 2e-4\n",
        "# Batch size'Ä± yukarÄ±da ayarlamÄ±ÅŸtÄ±k zaten, burada tekrar tanÄ±mlamaya gerek yok\n",
        "\n",
        "# === 1. ADIM: KLASÃ–RLER ===\n",
        "project_root = r\"D:\\Projects\"\n",
        "results_dir = os.path.join(project_root, \"results\")\n",
        "checkpoint_dir = os.path.join(project_root, \"model_checkpoints\")\n",
        "\n",
        "os.makedirs(results_dir, exist_ok=True)\n",
        "os.makedirs(checkpoint_dir, exist_ok=True)\n",
        "\n",
        "# === 2. ADIM: MODELÄ° DERLE ===\n",
        "hybrid_gan = WGAN_GP_Pix2Pix(generator=generator, discriminator=discriminator)\n",
        "\n",
        "hybrid_gan.compile(\n",
        "    d_optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_D, beta_1=0.5, beta_2=0.9),\n",
        "    g_optimizer=keras.optimizers.Adam(learning_rate=LEARNING_RATE_G, beta_1=0.5, beta_2=0.9)\n",
        ")\n",
        "\n",
        "# === 3. ADIM: SADE MONÄ°TÃ–R (SENÄ°N KODUN + KAYIT Ã–ZELLÄ°ÄÄ°) ===\n",
        "class GANMonitor(keras.callbacks.Callback):\n",
        "    def __init__(self, val_dataset, num_img=3):\n",
        "        self.val_dataset = val_dataset\n",
        "        self.num_img = num_img\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # --- A. GÃ–RSEL KAYIT (HER EPOCH) ---\n",
        "        print(f\"\\nEpoch {epoch+1} bitti. GÃ¶rseller iÅŸleniyor...\")\n",
        "\n",
        "        try:\n",
        "            # Rastgele veri Ã§ek\n",
        "            idx = np.random.randint(0, len(self.val_dataset))\n",
        "            inp, tar = self.val_dataset[idx]\n",
        "            prediction = self.model.generator(inp, training=False)\n",
        "\n",
        "            title = ['Input (LD)', 'Generated (AI)', 'Target (HD)']\n",
        "            display_count = min(self.num_img, inp.shape[0])\n",
        "\n",
        "            for i in range(display_count):\n",
        "                img_list = [inp[i], prediction[i], tar[i]]\n",
        "\n",
        "                plt.figure(figsize=(12, 4))\n",
        "                for j in range(3):\n",
        "                    plt.subplot(1, 3, j+1)\n",
        "                    plt.title(title[j])\n",
        "\n",
        "                    # --- TEK DEÄÄ°ÅÄ°KLÄ°K BURASI (BEYAZLIK GÄ°TMESÄ° Ä°Ã‡Ä°N) ---\n",
        "                    # Veriyi alÄ±p en kÃ¼Ã§Ã¼k ve en bÃ¼yÃ¼ÄŸe gÃ¶re 0-1 arasÄ±na Ã§ekiyoruz.\n",
        "                    # Bu sayede veri bozuk bile olsa ekranda gri gÃ¶zÃ¼kÃ¼r.\n",
        "                    img_data = img_list[j][:, :, 0]\n",
        "                    _min, _max = np.min(img_data), np.max(img_data)\n",
        "\n",
        "                    if _max - _min > 0:\n",
        "                        show_img = (img_data - _min) / (_max - _min)\n",
        "                    else:\n",
        "                        show_img = img_data\n",
        "\n",
        "                    plt.imshow(show_img, cmap='gray')\n",
        "                    plt.axis('off')\n",
        "\n",
        "                filename = f\"epoch_{epoch+1}_{i}.png\"\n",
        "                save_path = os.path.join(results_dir, filename)\n",
        "                plt.savefig(save_path)\n",
        "                plt.close() # Sadece figÃ¼rÃ¼ kapatÄ±r, RAM temizlemez. GÃ¼venli.\n",
        "\n",
        "            # --- B. MODEL KAYIT (HER 5 EPOCH) ---\n",
        "            if (epoch+1) % 5 == 0:\n",
        "                model_name = f\"G_epoch_{epoch+1}.h5\"\n",
        "                ckpt_path = os.path.join(checkpoint_dir, model_name)\n",
        "                self.model.generator.save_weights(ckpt_path)\n",
        "                print(f\"âœ… Model Kaydedildi: {ckpt_path}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"KayÄ±t hatasÄ±: {e}\")\n",
        "\n",
        "# === 4. ADIM: BAÅLAT ===\n",
        "\"\"\"print(\"EÄŸitim baÅŸlÄ±yor... (Eski performans, yeni kayÄ±t sistemi)\")\n",
        "\n",
        "hybrid_gan.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=50,\n",
        "    callbacks=[GANMonitor(val_dataset)]\n",
        ")\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87dd4d63"
      },
      "source": [
        "# Evaluating the Generator Model with PSNR and SSIM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1729db39",
        "outputId": "30184aa9-bba9-4aee-d9fc-93aeb70ebfbe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "En son kaydedilen model: /content/drive/MyDrive/TasarÄ±m Dersi/Projects/model_checkpoints/G_epoch_50.h5\n",
            "Model yÃ¼klenirken bir hata oluÅŸtu: 'NoneType' object has no attribute 'load_weights'\n",
            "JeneratÃ¶r modeli yÃ¼klenemediÄŸi iÃ§in deÄŸerlendirme yapÄ±lamadÄ±.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Assuming the generator and discriminator models are defined as in previous cells\n",
        "# Re-define build_generator and build_discriminator functions if they are not in scope.\n",
        "# For this task, we only need the generator.\n",
        "\n",
        "# --- 1. Load the latest Generator Model ---\n",
        "# Make sure the `build_generator` function from the previous cells is run before this cell.\n",
        "# You might need to adjust the epoch number based on your saved model.\n",
        "\n",
        "checkpoint_dir = \"/content/drive/MyDrive/TasarÄ±m Dersi/Projects/model_checkpoints\"\n",
        "\n",
        "# Find the latest saved generator model\n",
        "# Assuming your model names are like G_epoch_XX.h5\n",
        "list_of_files = glob.glob(os.path.join(checkpoint_dir, 'G_epoch_*.h5'))\n",
        "if not list_of_files:\n",
        "    print(\"HATA: KaydedilmiÅŸ bir Generator modeli bulunamadÄ±. LÃ¼tfen Ã¶nce modeli eÄŸittiÄŸinizden ve kaydettiÄŸinizden emin olun.\")\n",
        "    # Exit or handle error appropriately\n",
        "else:\n",
        "    latest_file = max(list_of_files, key=os.path.getctime)\n",
        "    print(f\"En son kaydedilen model: {latest_file}\")\n",
        "\n",
        "    # Re-build the generator architecture (it needs to be the same as trained)\n",
        "    # If 'generator' object from `yuyDVrxlJ6BC` is still in memory, you can skip this line\n",
        "    # from build_generator import build_generator # If you saved build_generator in a separate file\n",
        "    if 'generator' not in locals():\n",
        "        generator = build_generator()\n",
        "\n",
        "    # Load weights into the generator\n",
        "    try:\n",
        "        generator.load_weights(latest_file)\n",
        "        print(\"JeneratÃ¶r modeli baÅŸarÄ±yla yÃ¼klendi.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Model yÃ¼klenirken bir hata oluÅŸtu: {e}\")\n",
        "        generator = None # Set to None to prevent further errors if loading fails\n",
        "\n",
        "\n",
        "if generator:\n",
        "    # --- 2. Calculate PSNR and SSIM on the validation dataset ---\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "\n",
        "    print(\"DoÄŸrulama seti Ã¼zerinde PSNR ve SSIM hesaplanÄ±yor...\")\n",
        "\n",
        "    for batch_idx, (input_image, target_image) in enumerate(val_dataset):\n",
        "        # Generate prediction\n",
        "        predicted_image = generator(input_image, training=False)\n",
        "\n",
        "        # Ensure images are in the correct format (float32) and range for PSNR/SSIM\n",
        "        # PSNR and SSIM typically expect values in [0, 1] or [-1, 1].\n",
        "        # Our normalization was to [-1, 1], so we'll use that range for max_val.\n",
        "\n",
        "        # Calculate PSNR\n",
        "        batch_psnr = tf.image.psnr(target_image, predicted_image, max_val=2.0) # Max value is 1 - (-1) = 2\n",
        "        psnr_scores.extend(batch_psnr.numpy().tolist())\n",
        "\n",
        "        # Calculate SSIM\n",
        "        # SSIM requires a channel dimension if it's not already there (which it should be for our images)\n",
        "        batch_ssim = tf.image.ssim(target_image, predicted_image, max_val=2.0) # Max value is 1 - (-1) = 2\n",
        "        ssim_scores.extend(batch_ssim.numpy().tolist())\n",
        "\n",
        "        if batch_idx % 100 == 0: # Print progress\n",
        "            print(f\"  Ä°ÅŸlenen batch: {batch_idx}/{len(val_dataset)}\")\n",
        "\n",
        "    # --- 3. Report Average Scores ---\n",
        "    avg_psnr = np.mean(psnr_scores)\n",
        "    avg_ssim = np.mean(ssim_scores)\n",
        "\n",
        "    print(f\"\\nOrtalama PSNR: {avg_psnr:.4f}\")\n",
        "    print(f\"Ortalama SSIM: {avg_ssim:.4f}\")\n",
        "else:\n",
        "    print(\"JeneratÃ¶r modeli yÃ¼klenemediÄŸi iÃ§in deÄŸerlendirme yapÄ±lamadÄ±.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LjgE97lotdkK"
      },
      "source": [
        "###Â Evaluate 2 (Alternate)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "0288a25f4c624208ab019d93be718e1e",
            "65efff93f7ce47ea84a9545421a0cd0b"
          ]
        },
        "id": "TNBI54uMsKaB",
        "outputId": "7ddb1c57-2249-4897-84b2-91787dfa0c19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "YÃ¼klenen Model: G_epoch_50.h5\n",
            "âœ… Model hazÄ±r.\n",
            "\n",
            "Veriler RAM'e yÃ¼kleniyor (YaklaÅŸÄ±k 40-50 sn sÃ¼rer)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0288a25f4c624208ab019d93be718e1e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/2548 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Veri HazÄ±r. Toplam GÃ¶rÃ¼ntÃ¼: 2548\n",
            "\n",
            "Analiz BaÅŸlÄ±yor (CPU Modu - AdÄ±m AdÄ±m)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "65efff93f7ce47ea84a9545421a0cd0b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/51 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "========================================\n",
            "ğŸ“Š FÄ°NAL SONUÃ‡LAR\n",
            "========================================\n",
            "Ortalama PSNR: 41.7306 dB (Â±5.8375)\n",
            "Ortalama SSIM: 0.9411 (Â±0.0452)\n",
            "========================================\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# === 1. MODELÄ° HAZIRLA ===\n",
        "if 'checkpoint_dir' not in locals():\n",
        "    # Colab yolu\n",
        "    checkpoint_dir = \"/content/drive/MyDrive/TasarÄ±m Dersi/Projects/model_checkpoints\"\n",
        "\n",
        "# Modeli bul ve yÃ¼kle\n",
        "list_of_files = glob.glob(os.path.join(checkpoint_dir, 'G_epoch_*.h5'))\n",
        "if not list_of_files:\n",
        "    raise ValueError(\"Model dosyasÄ± bulunamadÄ±!\")\n",
        "\n",
        "latest_file = max(list_of_files, key=os.path.getctime)\n",
        "print(f\"YÃ¼klenen Model: {os.path.basename(latest_file)}\")\n",
        "\n",
        "generator = build_generator()\n",
        "generator.load_weights(latest_file)\n",
        "print(\"âœ… Model hazÄ±r.\")\n",
        "\n",
        "# === 2. VERÄ°YÄ° RAM'E Ã‡EK (HÄ±zlÄ± YÃ¶ntem) ===\n",
        "val_files_A = val_dataset.files_A\n",
        "val_files_B = val_dataset.files_B\n",
        "\n",
        "print(f\"\\nVeriler RAM'e yÃ¼kleniyor (YaklaÅŸÄ±k 40-50 sn sÃ¼rer)...\")\n",
        "all_inputs = []\n",
        "all_targets = []\n",
        "\n",
        "for fA, fB in tqdm(zip(val_files_A, val_files_B), total=len(val_files_A)):\n",
        "    try:\n",
        "        imgA = np.load(fA)\n",
        "        imgB = np.load(fB)\n",
        "\n",
        "        # Boyut dÃ¼zeltme\n",
        "        if imgA.ndim == 2: imgA = np.expand_dims(imgA, axis=-1)\n",
        "        if imgB.ndim == 2: imgB = np.expand_dims(imgB, axis=-1)\n",
        "\n",
        "        all_inputs.append(imgA)\n",
        "        all_targets.append(imgB)\n",
        "    except:\n",
        "        continue\n",
        "\n",
        "X_val = np.array(all_inputs)\n",
        "Y_val = np.array(all_targets)\n",
        "print(f\"âœ… Veri HazÄ±r. Toplam GÃ¶rÃ¼ntÃ¼: {len(X_val)}\")\n",
        "\n",
        "# === 3. GÃœVENLÄ° HESAPLAMA (Safe Mode) ===\n",
        "# CPU'yu yormamak iÃ§in: Tahmin et -> Hesapla -> Unut yÃ¶ntemini kullanÄ±yoruz.\n",
        "print(\"\\nAnaliz BaÅŸlÄ±yor (CPU Modu - AdÄ±m AdÄ±m)...\")\n",
        "\n",
        "psnr_scores = []\n",
        "ssim_scores = []\n",
        "\n",
        "# Ä°ÅŸlemciyi kilitlememek iÃ§in 50'ÅŸerli paketler halinde yapÄ±yoruz\n",
        "batch_size = 50\n",
        "total_samples = len(X_val)\n",
        "\n",
        "for i in tqdm(range(0, total_samples, batch_size)):\n",
        "    # 1. KÃ¼Ã§Ã¼k bir parÃ§a al\n",
        "    batch_input = X_val[i : i + batch_size]\n",
        "    batch_target = Y_val[i : i + batch_size]\n",
        "\n",
        "    # 2. Sadece bu parÃ§a iÃ§in tahmin yap (Training=False)\n",
        "    batch_pred = generator(batch_input, training=False)\n",
        "\n",
        "    # 3. Sadece bu parÃ§a iÃ§in PSNR/SSIM hesapla\n",
        "    # (Veriler -1 ile 1 arasÄ±nda olduÄŸu iÃ§in max_val=2.0)\n",
        "    batch_psnr = tf.image.psnr(batch_target, batch_pred, max_val=2.0).numpy()\n",
        "    batch_ssim = tf.image.ssim(batch_target, batch_pred, max_val=2.0).numpy()\n",
        "\n",
        "    # 4. SonuÃ§larÄ± listeye ekle\n",
        "    psnr_scores.extend(batch_psnr)\n",
        "    ssim_scores.extend(batch_ssim)\n",
        "\n",
        "# === 4. SONUÃ‡LARI YAZDIR ===\n",
        "avg_psnr = np.mean(psnr_scores)\n",
        "std_psnr = np.std(psnr_scores)\n",
        "avg_ssim = np.mean(ssim_scores)\n",
        "std_ssim = np.std(ssim_scores)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(f\"ğŸ“Š FÄ°NAL SONUÃ‡LAR\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Ortalama PSNR: {avg_psnr:.4f} dB (Â±{std_psnr:.4f})\")\n",
        "print(f\"Ortalama SSIM: {avg_ssim:.4f} (Â±{std_ssim:.4f})\")\n",
        "print(\"=\"*40)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluating the Model with New Dataset"
      ],
      "metadata": {
        "id": "JZdOKuJ7Sk18"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydicom"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Huwz6AgXhpPe",
        "outputId": "809e5c6f-d969-4c0c-cc89-31c2183c8275"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pydicom\n",
            "  Downloading pydicom-3.0.1-py3-none-any.whl.metadata (9.4 kB)\n",
            "Downloading pydicom-3.0.1-py3-none-any.whl (2.4 MB)\n",
            "\u001b[?25l   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[91mâ•¸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m131.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m62.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydicom\n",
            "Successfully installed pydicom-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pydicom\n",
        "from pathlib import Path\n",
        "\n",
        "# ==================== AYARLAR ====================\n",
        "phantomx_base_dir = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/phantomx_abdomen_pelvis_dataset\"\n",
        "output_dir = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output\"\n",
        "\n",
        "phantoms = [\"D55-01\", \"D55-02\"]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PhantomX DICOM â†’ NPY DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ (3 Seviyeli YapÄ±)\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"ğŸ“‚ Ana dizin: {phantomx_base_dir}\")\n",
        "print(f\"ğŸ“‚ Ã‡Ä±ktÄ± dizini: {output_dir}\")\n",
        "\n",
        "# Dizin kontrolÃ¼\n",
        "if not os.path.exists(phantomx_base_dir):\n",
        "    print(f\"\\nâŒ HATA: Ana dizin bulunamadÄ±!\")\n",
        "    print(f\"   Kontrol edin: {phantomx_base_dir}\")\n",
        "    exit()\n",
        "\n",
        "# ==================== KLASÃ–R YAPISINI ANLA ====================\n",
        "\n",
        "def klasor_yapisini_incele(base_dir, phantoms):\n",
        "    \"\"\"3 seviyeli klasÃ¶r yapÄ±sÄ±nÄ± inceler\"\"\"\n",
        "\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ“‹ KLASÃ–R YAPISI ANALÄ°ZÄ°\")\n",
        "    print(\"=\"*80)\n",
        "\n",
        "    klasor_bilgileri = {}\n",
        "\n",
        "    for phantom in phantoms:\n",
        "        phantom_yolu = os.path.join(base_dir, phantom)\n",
        "\n",
        "        if not os.path.exists(phantom_yolu):\n",
        "            print(f\"\\nâŒ {phantom} klasÃ¶rÃ¼ bulunamadÄ±!\")\n",
        "            continue\n",
        "\n",
        "        print(f\"\\n{'â”€'*80}\")\n",
        "        print(f\"ğŸ”¬ Phantom: {phantom}\")\n",
        "        print(f\"{'â”€'*80}\")\n",
        "\n",
        "        # 1. Seviye: Doz klasÃ¶rleri (40, 300)\n",
        "        doz_klasorleri = {}\n",
        "\n",
        "        for doz_adi in sorted(os.listdir(phantom_yolu)):\n",
        "            doz_yolu = os.path.join(phantom_yolu, doz_adi)\n",
        "\n",
        "            if not os.path.isdir(doz_yolu):\n",
        "                continue\n",
        "\n",
        "            print(f\"\\n  ğŸ“ Doz KlasÃ¶rÃ¼: {doz_adi}\")\n",
        "\n",
        "            # Doz bilgisini belirle\n",
        "            if doz_adi == \"40\":\n",
        "                doz_tipi = \"1.4 CTDIvol (DÃ¼ÅŸÃ¼k Doz)\"\n",
        "            elif doz_adi == \"300\":\n",
        "                doz_tipi = \"10.5 CTDIvol (YÃ¼ksek Doz)\"\n",
        "            else:\n",
        "                doz_tipi = \"Bilinmeyen\"\n",
        "\n",
        "            print(f\"      â†’ {doz_tipi}\")\n",
        "\n",
        "            # 2. Seviye: RekonstrÃ¼ksiyon metodu klasÃ¶rleri\n",
        "            rekon_klasorleri = []\n",
        "\n",
        "            for rekon_adi in sorted(os.listdir(doz_yolu)):\n",
        "                rekon_yolu = os.path.join(doz_yolu, rekon_adi)\n",
        "\n",
        "                if not os.path.isdir(rekon_yolu):\n",
        "                    continue\n",
        "\n",
        "                # DICOM sayÄ±sÄ±nÄ± say\n",
        "                dicom_sayisi = len([f for f in os.listdir(rekon_yolu) if f.endswith('.dcm')])\n",
        "\n",
        "                # RekonstrÃ¼ksiyon tipini belirle\n",
        "                if \"FBP\" in rekon_adi.upper():\n",
        "                    rekon_tip = \"FBP (Filtered Back Projection)\"\n",
        "                elif \"AIDR\" in rekon_adi.upper():\n",
        "                    rekon_tip = \"AIDR3D (Iterative Reconstruction)\"\n",
        "                elif \"AICE\" in rekon_adi.upper() or \"ICE\" in rekon_adi.upper():\n",
        "                    rekon_tip = \"AiCE (AI-based Reconstruction)\"\n",
        "                else:\n",
        "                    rekon_tip = \"Bilinmeyen\"\n",
        "\n",
        "                rekon_klasorleri.append({\n",
        "                    'ad': rekon_adi,\n",
        "                    'yol': rekon_yolu,\n",
        "                    'tip': rekon_tip,\n",
        "                    'dicom_sayisi': dicom_sayisi\n",
        "                })\n",
        "\n",
        "                print(f\"      â””â”€â”€ {rekon_tip}\")\n",
        "                print(f\"          KlasÃ¶r: {rekon_adi}\")\n",
        "                print(f\"          DICOM: {dicom_sayisi} dosya\")\n",
        "\n",
        "            doz_klasorleri[doz_adi] = {\n",
        "                'yol': doz_yolu,\n",
        "                'tip': doz_tipi,\n",
        "                'rekonstruksiyonlar': rekon_klasorleri\n",
        "            }\n",
        "\n",
        "        klasor_bilgileri[phantom] = doz_klasorleri\n",
        "\n",
        "    return klasor_bilgileri\n",
        "\n",
        "# KlasÃ¶r yapÄ±sÄ±nÄ± incele\n",
        "klasor_bilgileri = klasor_yapisini_incele(phantomx_base_dir, phantoms)\n",
        "\n",
        "if not klasor_bilgileri:\n",
        "    print(\"\\nâŒ HiÃ§bir phantom klasÃ¶rÃ¼ bulunamadÄ±!\")\n",
        "    exit()\n",
        "\n",
        "# ==================== KULLANICIDAN SEÃ‡Ä°M AL ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ¯ REKONSTRÃœKSÄ°YON METODU SEÃ‡Ä°MÄ°\")\n",
        "print(\"=\"*80)\n",
        "print(\"\\nâš ï¸  Ã–NERÄ°: FBP metodu seÃ§in (model eÄŸitiminde kullanÄ±ldÄ±ysa)\")\n",
        "print(\"   â€¢ FBP: Standart, ham gÃ¼rÃ¼ltÃ¼ iÃ§erir (en yaygÄ±n)\")\n",
        "print(\"   â€¢ AIDR3D: Iterative, kÄ±smen gÃ¼rÃ¼ltÃ¼ azaltÄ±lmÄ±ÅŸ\")\n",
        "print(\"   â€¢ AiCE: AI-based, yoÄŸun gÃ¼rÃ¼ltÃ¼ azaltma yapÄ±lmÄ±ÅŸ\")\n",
        "\n",
        "# Hangi rekonstrÃ¼ksiyon metodunu kullanacaÄŸÄ±z?\n",
        "print(\"\\nHangi rekonstrÃ¼ksiyon metodunu kullanmak istersiniz?\")\n",
        "print(\"  [1] FBP (Ã–nerilen)\")\n",
        "print(\"  [2] AIDR3D\")\n",
        "print(\"  [3] AiCE\")\n",
        "\n",
        "rekon_secim = input(\"\\nSeÃ§iminiz (1/2/3): \").strip()\n",
        "\n",
        "rekon_map = {\n",
        "    \"1\": \"FBP\",\n",
        "    \"2\": \"AIDR\",\n",
        "    \"3\": \"AICE\"\n",
        "}\n",
        "\n",
        "if rekon_secim not in rekon_map:\n",
        "    print(\"âŒ GeÃ§ersiz seÃ§im! VarsayÄ±lan olarak FBP kullanÄ±lacak.\")\n",
        "    rekon_secim = \"1\"\n",
        "\n",
        "secilen_rekon = rekon_map[rekon_secim]\n",
        "print(f\"\\nâœ… SeÃ§ilen metod: {secilen_rekon}\")\n",
        "\n",
        "# ==================== FONKSÄ°YONLAR ====================\n",
        "\n",
        "def rekonstruksiyon_klasoru_bul(rekon_klasorleri, aranan_tip):\n",
        "    \"\"\"Belirtilen rekonstrÃ¼ksiyon tipine ait klasÃ¶rÃ¼ bulur\"\"\"\n",
        "    for klasor in rekon_klasorleri:\n",
        "        if aranan_tip.upper() in klasor['ad'].upper():\n",
        "            return klasor\n",
        "    return None\n",
        "\n",
        "def dicom_klasoru_oku(klasor_yolu, klasor_adi):\n",
        "    \"\"\"DICOM dosyalarÄ±nÄ± okur ve 3D numpy array olarak dÃ¶ndÃ¼rÃ¼r\"\"\"\n",
        "    print(f\"      ğŸ“‚ {klasor_adi}\")\n",
        "\n",
        "    dicom_dosyalari = []\n",
        "\n",
        "    for dosya in os.listdir(klasor_yolu):\n",
        "        if dosya.endswith('.dcm'):\n",
        "            dosya_yolu = os.path.join(klasor_yolu, dosya)\n",
        "            dicom_dosyalari.append(dosya_yolu)\n",
        "\n",
        "    if not dicom_dosyalari:\n",
        "        print(f\"      âŒ DICOM dosyasÄ± bulunamadÄ±!\")\n",
        "        return None\n",
        "\n",
        "    print(f\"      âœ“ {len(dicom_dosyalari)} DICOM dosyasÄ± bulundu\")\n",
        "\n",
        "    # DICOM dosyalarÄ±nÄ± oku ve sÄ±rala\n",
        "    dicom_veriler = []\n",
        "    hata_sayisi = 0\n",
        "\n",
        "    for dosya in sorted(dicom_dosyalari):\n",
        "       try:\n",
        "            ds = pydicom.dcmread(dosya)\n",
        "\n",
        "            # Pixel array'i al\n",
        "            pixel_array = ds.pixel_array.astype(np.float32)\n",
        "\n",
        "            # --- DÃœZELTME BAÅLANGIÃ‡: HU DÃ¶nÃ¼ÅŸÃ¼mÃ¼ ---\n",
        "            intercept = ds.RescaleIntercept if 'RescaleIntercept' in ds else 0\n",
        "            slope = ds.RescaleSlope if 'RescaleSlope' in ds else 1\n",
        "\n",
        "            # Ham veriyi HU birimine Ã§evir\n",
        "            pixel_array = pixel_array * slope + intercept\n",
        "            # --- DÃœZELTME BÄ°TÄ°Å ---\n",
        "\n",
        "            # Sonra direkt clip ve normalize iÅŸlemlerine geÃ§\n",
        "            pixel_array = np.clip(pixel_array, -1000, 1000)\n",
        "\n",
        "            # Z koordinatÄ± veya InstanceNumber kullan\n",
        "            if hasattr(ds, 'ImagePositionPatient'):\n",
        "                slice_location = float(ds.ImagePositionPatient[2])\n",
        "            elif hasattr(ds, 'SliceLocation'):\n",
        "                slice_location = float(ds.SliceLocation)\n",
        "            elif hasattr(ds, 'InstanceNumber'):\n",
        "                slice_location = float(ds.InstanceNumber)\n",
        "            else:\n",
        "                slice_location = len(dicom_veriler)\n",
        "\n",
        "            dicom_veriler.append((slice_location, pixel_array))\n",
        "\n",
        "        except Exception as e:\n",
        "            hata_sayisi += 1\n",
        "            if hata_sayisi <= 3:\n",
        "                print(f\"      âš  Hata ({os.path.basename(dosya)}): {e}\")\n",
        "\n",
        "    if hata_sayisi > 3:\n",
        "        print(f\"      âš  Toplam {hata_sayisi} dosya okunamadÄ±\")\n",
        "\n",
        "    if not dicom_veriler:\n",
        "        print(f\"      âŒ HiÃ§bir DICOM dosyasÄ± okunamadÄ±!\")\n",
        "        return None\n",
        "\n",
        "    # Z koordinatÄ±na gÃ¶re sÄ±rala\n",
        "    dicom_veriler.sort(key=lambda x: x[0])\n",
        "\n",
        "    # 3D array oluÅŸtur\n",
        "    volume_3d = np.stack([slice_data for _, slice_data in dicom_veriler])\n",
        "\n",
        "    print(f\"      âœ“ Volume: {volume_3d.shape}\")\n",
        "    print(f\"      âœ“ HU aralÄ±ÄŸÄ±: [{volume_3d.min():.1f}, {volume_3d.max():.1f}]\")\n",
        "\n",
        "    return volume_3d\n",
        "\n",
        "def normalizasyon(volume, min_hu=-1000, max_hu=1000):\n",
        "    \"\"\"HU deÄŸerlerini [-1, 1] aralÄ±ÄŸÄ±na normalize eder\"\"\"\n",
        "    volume = np.clip(volume, min_hu, max_hu)\n",
        "    volume = (volume - min_hu) / (max_hu - min_hu)  # [0, 1]\n",
        "    volume = volume * 2 - 1  # [-1, 1]\n",
        "    return volume.astype(np.float32)\n",
        "\n",
        "# ==================== DÃ–NÃœÅTÃœRMEYÄ° BAÅLAT ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"ğŸ”„ DÃ–NÃœÅTÃœRME Ä°ÅLEMÄ°\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "basarili_donusumler = []\n",
        "\n",
        "for phantom_name, doz_klasorleri in klasor_bilgileri.items():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ”¬ Phantom: {phantom_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # 40 = dÃ¼ÅŸÃ¼k doz (INPUT)\n",
        "    # 300 = yÃ¼ksek doz (TARGET)\n",
        "\n",
        "    if \"40\" not in doz_klasorleri or \"300\" not in doz_klasorleri:\n",
        "        print(f\"  âŒ Gerekli doz klasÃ¶rleri bulunamadÄ± (40 veya 300 eksik)\")\n",
        "        continue\n",
        "\n",
        "    # INPUT: 40 klasÃ¶rÃ¼nden seÃ§ilen rekonstrÃ¼ksiyon\n",
        "    input_rekonlar = doz_klasorleri[\"40\"][\"rekonstruksiyonlar\"]\n",
        "    input_rekon = rekonstruksiyon_klasoru_bul(input_rekonlar, secilen_rekon)\n",
        "\n",
        "    # TARGET: 300 klasÃ¶rÃ¼nden seÃ§ilen rekonstrÃ¼ksiyon\n",
        "    target_rekonlar = doz_klasorleri[\"300\"][\"rekonstruksiyonlar\"]\n",
        "    target_rekon = rekonstruksiyon_klasoru_bul(target_rekonlar, secilen_rekon)\n",
        "\n",
        "    if not input_rekon or not target_rekon:\n",
        "        print(f\"  âŒ {secilen_rekon} rekonstrÃ¼ksiyonu bulunamadÄ±!\")\n",
        "        continue\n",
        "\n",
        "    print(f\"\\n  ğŸ“¥ INPUT (1.4 CTDIvol):\")\n",
        "    print(f\"     {input_rekon['ad']}\")\n",
        "    print(f\"  ğŸ“¤ TARGET (10.5 CTDIvol):\")\n",
        "    print(f\"     {target_rekon['ad']}\")\n",
        "\n",
        "    try:\n",
        "        # DICOM'larÄ± oku\n",
        "        print(f\"\\n  ğŸ“ DICOM dosyalarÄ± okunuyor...\")\n",
        "\n",
        "        print(f\"\\n    INPUT:\")\n",
        "        input_volume = dicom_klasoru_oku(input_rekon['yol'], input_rekon['ad'])\n",
        "\n",
        "        print(f\"\\n    TARGET:\")\n",
        "        target_volume = dicom_klasoru_oku(target_rekon['yol'], target_rekon['ad'])\n",
        "\n",
        "        if input_volume is None or target_volume is None:\n",
        "            print(f\"\\n  âŒ DICOM okuma baÅŸarÄ±sÄ±z!\")\n",
        "            continue\n",
        "\n",
        "        # Shape kontrolÃ¼\n",
        "        if input_volume.shape != target_volume.shape:\n",
        "            print(f\"\\n  âš  UYARI: Boyutlar farklÄ±!\")\n",
        "            print(f\"    Input: {input_volume.shape}\")\n",
        "            print(f\"    Target: {target_volume.shape}\")\n",
        "\n",
        "            min_slices = min(input_volume.shape[0], target_volume.shape[0])\n",
        "            min_height = min(input_volume.shape[1], target_volume.shape[1])\n",
        "            min_width = min(input_volume.shape[2], target_volume.shape[2])\n",
        "\n",
        "            input_volume = input_volume[:min_slices, :min_height, :min_width]\n",
        "            target_volume = target_volume[:min_slices, :min_height, :min_width]\n",
        "\n",
        "            print(f\"    â†’ Uyumlu hale getirildi: {input_volume.shape}\")\n",
        "\n",
        "        # Normalizasyon\n",
        "        print(f\"\\n  ğŸ”„ Normalizasyon: [-1000, 1000] HU â†’ [-1, 1]\")\n",
        "\n",
        "        input_normalized = normalizasyon(input_volume)\n",
        "        target_normalized = normalizasyon(target_volume)\n",
        "\n",
        "        print(f\"    âœ“ Input: [{input_normalized.min():.3f}, {input_normalized.max():.3f}]\")\n",
        "        print(f\"    âœ“ Target: [{target_normalized.min():.3f}, {target_normalized.max():.3f}]\")\n",
        "\n",
        "        # NPY olarak kaydet\n",
        "        print(f\"\\n  ğŸ’¾ Dosyalar kaydediliyor...\")\n",
        "        phantom_output = os.path.join(output_dir, phantom_name)\n",
        "        os.makedirs(phantom_output, exist_ok=True)\n",
        "\n",
        "        input_save_path = os.path.join(phantom_output, \"input_ldct.npy\")\n",
        "        target_save_path = os.path.join(phantom_output, \"target_ndct.npy\")\n",
        "\n",
        "        np.save(input_save_path, input_normalized)\n",
        "        np.save(target_save_path, target_normalized)\n",
        "\n",
        "        print(f\"    âœ“ {input_save_path}\")\n",
        "        print(f\"    âœ“ {target_save_path}\")\n",
        "\n",
        "        # Ä°statistikler\n",
        "        print(f\"\\n  ğŸ“Š Ä°statistikler:\")\n",
        "        print(f\"     Slice: {input_normalized.shape[0]}\")\n",
        "        print(f\"     Boyut: {input_normalized.shape[1]} x {input_normalized.shape[2]}\")\n",
        "        print(f\"     Dosya: {input_normalized.nbytes / (1024**2):.2f} MB (her biri)\")\n",
        "\n",
        "        basarili_donusumler.append(phantom_name)\n",
        "        print(f\"\\n  âœ… BaÅŸarÄ±lÄ±!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n  âŒ HATA: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n",
        "# ==================== Ã–ZET ====================\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "if basarili_donusumler:\n",
        "    print(\"âœ… DÃ–NÃœÅTÃœRME TAMAMLANDI!\")\n",
        "    print(\"=\"*80)\n",
        "    print(f\"\\nğŸ“ Ã‡Ä±ktÄ±: {output_dir}\\n\")\n",
        "    for phantom in basarili_donusumler:\n",
        "        print(f\"  {phantom}/\")\n",
        "        print(f\"    â”œâ”€â”€ input_ldct.npy  (1.4 CTDIvol - DÃ¼ÅŸÃ¼k Doz)\")\n",
        "        print(f\"    â””â”€â”€ target_ndct.npy (10.5 CTDIvol - YÃ¼ksek Doz)\")\n",
        "    print(f\"\\nğŸš€ Test kodunu Ã§alÄ±ÅŸtÄ±rabilirsiniz!\")\n",
        "else:\n",
        "    print(\"âŒ HÄ°Ã‡BÄ°R PHANTOM DÃ–NÃœÅTÃœRÃœLEMEDI!\")\n",
        "    print(\"=\"*80)\n",
        "print()"
      ],
      "metadata": {
        "id": "mzxjaNRUSsP7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daa52b7c-95d0-4c19-c337-eee9e2e0a907"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PhantomX DICOM â†’ NPY DÃ¶nÃ¼ÅŸtÃ¼rÃ¼cÃ¼ (3 Seviyeli YapÄ±)\n",
            "================================================================================\n",
            "ğŸ“‚ Ana dizin: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/phantomx_abdomen_pelvis_dataset\n",
            "ğŸ“‚ Ã‡Ä±ktÄ± dizini: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output\n",
            "\n",
            "================================================================================\n",
            "ğŸ“‹ KLASÃ–R YAPISI ANALÄ°ZÄ°\n",
            "================================================================================\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ”¬ Phantom: D55-01\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "  ğŸ“ Doz KlasÃ¶rÃ¼: 300\n",
            "      â†’ 10.5 CTDIvol (YÃ¼ksek Doz)\n",
            "      â””â”€â”€ AIDR3D (Iterative Reconstruction)\n",
            "          KlasÃ¶r: AIDR3D_FC08_300_171515.916\n",
            "          DICOM: 376 dosya\n",
            "      â””â”€â”€ AiCE (AI-based Reconstruction)\n",
            "          KlasÃ¶r: AiCE_BODY-SHARP_300_172938.900\n",
            "          DICOM: 376 dosya\n",
            "      â””â”€â”€ FBP (Filtered Back Projection)\n",
            "          KlasÃ¶r: FBP_FC08_300_171515.916\n",
            "          DICOM: 376 dosya\n",
            "\n",
            "  ğŸ“ Doz KlasÃ¶rÃ¼: 40\n",
            "      â†’ 1.4 CTDIvol (DÃ¼ÅŸÃ¼k Doz)\n",
            "      â””â”€â”€ AIDR3D (Iterative Reconstruction)\n",
            "          KlasÃ¶r: AIDR3D_FC08_40_152622.890\n",
            "          DICOM: 376 dosya\n",
            "      â””â”€â”€ AiCE (AI-based Reconstruction)\n",
            "          KlasÃ¶r: AiCE_BODY-SHARP_40_152622.890\n",
            "          DICOM: 376 dosya\n",
            "      â””â”€â”€ FBP (Filtered Back Projection)\n",
            "          KlasÃ¶r: FBP_FC08_40_152622.890\n",
            "          DICOM: 376 dosya\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ”¬ Phantom: D55-02\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "  ğŸ“ Doz KlasÃ¶rÃ¼: 300\n",
            "      â†’ 10.5 CTDIvol (YÃ¼ksek Doz)\n",
            "      â””â”€â”€ AIDR3D (Iterative Reconstruction)\n",
            "          KlasÃ¶r: 49_120_300_FC08_AIDR3D_194917.334\n",
            "          DICOM: 363 dosya\n",
            "      â””â”€â”€ FBP (Filtered Back Projection)\n",
            "          KlasÃ¶r: 51_120_300_FC08_FBP_194917.334\n",
            "          DICOM: 363 dosya\n",
            "      â””â”€â”€ AiCE (AI-based Reconstruction)\n",
            "          KlasÃ¶r: 55_120_300_BODY-SHARP_AICE_174115.432\n",
            "          DICOM: 363 dosya\n",
            "\n",
            "  ğŸ“ Doz KlasÃ¶rÃ¼: 40\n",
            "      â†’ 1.4 CTDIvol (DÃ¼ÅŸÃ¼k Doz)\n",
            "      â””â”€â”€ AIDR3D (Iterative Reconstruction)\n",
            "          KlasÃ¶r: 35_120_40_FC08_AIDR3D_174115.413\n",
            "          DICOM: 363 dosya\n",
            "      â””â”€â”€ FBP (Filtered Back Projection)\n",
            "          KlasÃ¶r: 38_120_40_FC08_FBP_174115.413\n",
            "          DICOM: 363 dosya\n",
            "      â””â”€â”€ AiCE (AI-based Reconstruction)\n",
            "          KlasÃ¶r: 7_120_40_BODY-SHARP_AICE_174115.413\n",
            "          DICOM: 363 dosya\n",
            "\n",
            "================================================================================\n",
            "ğŸ¯ REKONSTRÃœKSÄ°YON METODU SEÃ‡Ä°MÄ°\n",
            "================================================================================\n",
            "\n",
            "âš ï¸  Ã–NERÄ°: FBP metodu seÃ§in (model eÄŸitiminde kullanÄ±ldÄ±ysa)\n",
            "   â€¢ FBP: Standart, ham gÃ¼rÃ¼ltÃ¼ iÃ§erir (en yaygÄ±n)\n",
            "   â€¢ AIDR3D: Iterative, kÄ±smen gÃ¼rÃ¼ltÃ¼ azaltÄ±lmÄ±ÅŸ\n",
            "   â€¢ AiCE: AI-based, yoÄŸun gÃ¼rÃ¼ltÃ¼ azaltma yapÄ±lmÄ±ÅŸ\n",
            "\n",
            "Hangi rekonstrÃ¼ksiyon metodunu kullanmak istersiniz?\n",
            "  [1] FBP (Ã–nerilen)\n",
            "  [2] AIDR3D\n",
            "  [3] AiCE\n",
            "\n",
            "SeÃ§iminiz (1/2/3): 1\n",
            "\n",
            "âœ… SeÃ§ilen metod: FBP\n",
            "\n",
            "================================================================================\n",
            "ğŸ”„ DÃ–NÃœÅTÃœRME Ä°ÅLEMÄ°\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¬ Phantom: D55-01\n",
            "================================================================================\n",
            "\n",
            "  ğŸ“¥ INPUT (1.4 CTDIvol):\n",
            "     FBP_FC08_40_152622.890\n",
            "  ğŸ“¤ TARGET (10.5 CTDIvol):\n",
            "     FBP_FC08_300_171515.916\n",
            "\n",
            "  ğŸ“ DICOM dosyalarÄ± okunuyor...\n",
            "\n",
            "    INPUT:\n",
            "      ğŸ“‚ FBP_FC08_40_152622.890\n",
            "      âœ“ 376 DICOM dosyasÄ± bulundu\n",
            "      âœ“ Volume: (376, 512, 512)\n",
            "      âœ“ HU aralÄ±ÄŸÄ±: [-1000.0, 1000.0]\n",
            "\n",
            "    TARGET:\n",
            "      ğŸ“‚ FBP_FC08_300_171515.916\n",
            "      âœ“ 376 DICOM dosyasÄ± bulundu\n",
            "      âœ“ Volume: (376, 512, 512)\n",
            "      âœ“ HU aralÄ±ÄŸÄ±: [-1000.0, 1000.0]\n",
            "\n",
            "  ğŸ”„ Normalizasyon: [-1000, 1000] HU â†’ [-1, 1]\n",
            "    âœ“ Input: [0.024, 1.000]\n",
            "    âœ“ Target: [0.024, 1.000]\n",
            "\n",
            "  ğŸ’¾ Dosyalar kaydediliyor...\n",
            "    âœ“ /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output/D55-01/input_ldct.npy\n",
            "    âœ“ /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output/D55-01/target_ndct.npy\n",
            "\n",
            "  ğŸ“Š Ä°statistikler:\n",
            "     Slice: 376\n",
            "     Boyut: 512 x 512\n",
            "     Dosya: 376.00 MB (her biri)\n",
            "\n",
            "  âœ… BaÅŸarÄ±lÄ±!\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¬ Phantom: D55-02\n",
            "================================================================================\n",
            "\n",
            "  ğŸ“¥ INPUT (1.4 CTDIvol):\n",
            "     38_120_40_FC08_FBP_174115.413\n",
            "  ğŸ“¤ TARGET (10.5 CTDIvol):\n",
            "     51_120_300_FC08_FBP_194917.334\n",
            "\n",
            "  ğŸ“ DICOM dosyalarÄ± okunuyor...\n",
            "\n",
            "    INPUT:\n",
            "      ğŸ“‚ 38_120_40_FC08_FBP_174115.413\n",
            "      âœ“ 363 DICOM dosyasÄ± bulundu\n",
            "      âœ“ Volume: (363, 512, 512)\n",
            "      âœ“ HU aralÄ±ÄŸÄ±: [-1000.0, 1000.0]\n",
            "\n",
            "    TARGET:\n",
            "      ğŸ“‚ 51_120_300_FC08_FBP_194917.334\n",
            "      âœ“ 363 DICOM dosyasÄ± bulundu\n",
            "      âœ“ Volume: (363, 512, 512)\n",
            "      âœ“ HU aralÄ±ÄŸÄ±: [-1000.0, 1000.0]\n",
            "\n",
            "  ğŸ”„ Normalizasyon: [-1000, 1000] HU â†’ [-1, 1]\n",
            "    âœ“ Input: [0.024, 1.000]\n",
            "    âœ“ Target: [0.024, 1.000]\n",
            "\n",
            "  ğŸ’¾ Dosyalar kaydediliyor...\n",
            "    âœ“ /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output/D55-02/input_ldct.npy\n",
            "    âœ“ /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output/D55-02/target_ndct.npy\n",
            "\n",
            "  ğŸ“Š Ä°statistikler:\n",
            "     Slice: 363\n",
            "     Boyut: 512 x 512\n",
            "     Dosya: 363.00 MB (her biri)\n",
            "\n",
            "  âœ… BaÅŸarÄ±lÄ±!\n",
            "\n",
            "================================================================================\n",
            "âœ… DÃ–NÃœÅTÃœRME TAMAMLANDI!\n",
            "================================================================================\n",
            "\n",
            "ğŸ“ Ã‡Ä±ktÄ±: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output\n",
            "\n",
            "  D55-01/\n",
            "    â”œâ”€â”€ input_ldct.npy  (1.4 CTDIvol - DÃ¼ÅŸÃ¼k Doz)\n",
            "    â””â”€â”€ target_ndct.npy (10.5 CTDIvol - YÃ¼ksek Doz)\n",
            "  D55-02/\n",
            "    â”œâ”€â”€ input_ldct.npy  (1.4 CTDIvol - DÃ¼ÅŸÃ¼k Doz)\n",
            "    â””â”€â”€ target_ndct.npy (10.5 CTDIvol - YÃ¼ksek Doz)\n",
            "\n",
            "ğŸš€ Test kodunu Ã§alÄ±ÅŸtÄ±rabilirsiniz!\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import cv2\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from skimage.metrics import peak_signal_noise_ratio as psnr\n",
        "from tqdm import tqdm\n",
        "\n",
        "# ==================== AYARLAR ====================\n",
        "phantomx_processed_dir = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output\"\n",
        "checkpoint_dir = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Projects/model_checkpoints\"\n",
        "output_dir = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Results2\"\n",
        "\n",
        "phantoms = [\"D55-01\", \"D55-02\"]\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"PhantomX Model Test ve DeÄŸerlendirme\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"ğŸ“‚ Processed data: {phantomx_processed_dir}\")\n",
        "print(f\"ğŸ§  Model checkpoints: {checkpoint_dir}\")\n",
        "print(f\"ğŸ“Š Results output: {output_dir}\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# ==================== FONKSÄ°YONLAR ====================\n",
        "\n",
        "def load_latest_generator(checkpoint_dir, generator_model):\n",
        "    \"\"\"En yÃ¼ksek epoch numaralÄ± generator modelini yÃ¼kler\"\"\"\n",
        "    list_of_files = glob.glob(os.path.join(checkpoint_dir, 'G_epoch_*.h5'))\n",
        "\n",
        "    if not list_of_files:\n",
        "        print(\"âŒ HATA: KaydedilmiÅŸ generator modeli bulunamadÄ±!\")\n",
        "        print(f\"   Kontrol edilen dizin: {checkpoint_dir}\")\n",
        "        print(f\"   Aranan format: G_epoch_*.h5\")\n",
        "\n",
        "        # KlasÃ¶rdeki tÃ¼m dosyalarÄ± gÃ¶ster\n",
        "        if os.path.exists(checkpoint_dir):\n",
        "            all_files = os.listdir(checkpoint_dir)\n",
        "            if all_files:\n",
        "                print(f\"\\n   KlasÃ¶rdeki dosyalar:\")\n",
        "                for f in sorted(all_files)[:10]:  # Ä°lk 10 dosyayÄ± gÃ¶ster\n",
        "                    print(f\"   - {f}\")\n",
        "        return None\n",
        "\n",
        "    # Epoch numarasÄ±na gÃ¶re sÄ±rala ve en bÃ¼yÃ¼ÄŸÃ¼nÃ¼ al\n",
        "    def get_epoch_number(filepath):\n",
        "        \"\"\"Dosya adÄ±ndan epoch numarasÄ±nÄ± Ã§Ä±kar\"\"\"\n",
        "        basename = os.path.basename(filepath)\n",
        "        # G_epoch_50.h5 -> 50\n",
        "        try:\n",
        "            epoch_str = basename.split('_')[-1].replace('.h5', '')\n",
        "            return int(epoch_str)\n",
        "        except:\n",
        "            return 0\n",
        "\n",
        "    latest_file = max(list_of_files, key=get_epoch_number)\n",
        "    epoch_num = get_epoch_number(latest_file)\n",
        "\n",
        "    print(f\"\\nğŸ“¦ Model bulundu ve yÃ¼kleniyor...\")\n",
        "    print(f\"   Dosya: {os.path.basename(latest_file)}\")\n",
        "    print(f\"   Epoch: {epoch_num}\")\n",
        "    print(f\"   Toplam {len(list_of_files)} checkpoint bulundu\")\n",
        "\n",
        "    try:\n",
        "        generator_model.load_weights(latest_file)\n",
        "        print(\"âœ… Model baÅŸarÄ±yla yÃ¼klendi!\")\n",
        "        return generator_model\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Model yÃ¼kleme hatasÄ±: {e}\")\n",
        "        print(f\"\\nğŸ’¡ Model mimarisi ile checkpoint uyumsuz olabilir.\")\n",
        "        print(f\"   LÃ¼tfen generator modelinin doÄŸru tanÄ±mlandÄ±ÄŸÄ±ndan emin olun.\")\n",
        "        return None\n",
        "\n",
        "def calculate_metrics_3d(target, predicted):\n",
        "    \"\"\"3D volume iÃ§in PSNR ve SSIM hesaplar\"\"\"\n",
        "    psnr_scores = []\n",
        "    ssim_scores = []\n",
        "\n",
        "    # Her slice iÃ§in ayrÄ± ayrÄ± hesapla\n",
        "    for i in range(target.shape[0]):\n",
        "        target_slice = target[i]\n",
        "        pred_slice = predicted[i]\n",
        "\n",
        "        # PSNR hesaplama (data_range=2.0 Ã§Ã¼nkÃ¼ [-1, 1] aralÄ±ÄŸÄ±nda)\n",
        "        psnr_val = psnr(target_slice, pred_slice, data_range=2.0)\n",
        "        psnr_scores.append(psnr_val)\n",
        "\n",
        "        # SSIM hesaplama\n",
        "        ssim_val = ssim(target_slice, pred_slice, data_range=2.0)\n",
        "        ssim_scores.append(ssim_val)\n",
        "\n",
        "    return np.array(psnr_scores), np.array(ssim_scores)\n",
        "\n",
        "def visualize_results(input_vol, target_vol, predicted_vol, phantom_name, output_dir, num_slices=5):\n",
        "    \"\"\"SonuÃ§larÄ± gÃ¶rselleÅŸtirir ve kaydeder\"\"\"\n",
        "    total_slices = input_vol.shape[0]\n",
        "    slice_indices = np.linspace(0, total_slices-1, num_slices, dtype=int)\n",
        "\n",
        "    fig, axes = plt.subplots(num_slices, 4, figsize=(16, 4*num_slices))\n",
        "\n",
        "    for idx, slice_num in enumerate(slice_indices):\n",
        "        # Input (LDCT)\n",
        "        axes[idx, 0].imshow(input_vol[slice_num], cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[idx, 0].set_title(f'Input (LDCT)\\nSlice {slice_num}/{total_slices}', fontsize=10)\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        # Target (NDCT)\n",
        "        axes[idx, 1].imshow(target_vol[slice_num], cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[idx, 1].set_title(f'Target (NDCT)\\nSlice {slice_num}/{total_slices}', fontsize=10)\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        # Predicted\n",
        "        axes[idx, 2].imshow(predicted_vol[slice_num], cmap='gray', vmin=-1, vmax=1)\n",
        "        axes[idx, 2].set_title(f'Predicted\\nSlice {slice_num}/{total_slices}', fontsize=10)\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "        # Difference map\n",
        "        diff = np.abs(target_vol[slice_num] - predicted_vol[slice_num])\n",
        "        im = axes[idx, 3].imshow(diff, cmap='hot', vmin=0, vmax=0.5)\n",
        "        axes[idx, 3].set_title(f'Absolute Error\\nSlice {slice_num}/{total_slices}', fontsize=10)\n",
        "        axes[idx, 3].axis('off')\n",
        "\n",
        "        # Colorbar for error map\n",
        "        plt.colorbar(im, ax=axes[idx, 3], fraction=0.046, pad=0.04)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Kaydet\n",
        "    save_path = os.path.join(output_dir, f\"{phantom_name}_comparison.png\")\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"  ğŸ’¾ GÃ¶rselleÅŸtirme kaydedildi: {os.path.basename(save_path)}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_metrics_distribution(psnr_scores, ssim_scores, phantom_name, output_dir):\n",
        "    \"\"\"Metrik daÄŸÄ±lÄ±mlarÄ±nÄ± Ã§izer\"\"\"\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "    # PSNR histogram\n",
        "    axes[0].hist(psnr_scores, bins=30, color='steelblue', edgecolor='black', alpha=0.7)\n",
        "    axes[0].axvline(np.mean(psnr_scores), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Ortalama: {np.mean(psnr_scores):.2f} dB')\n",
        "    axes[0].set_xlabel('PSNR (dB)', fontsize=12)\n",
        "    axes[0].set_ylabel('Slice SayÄ±sÄ±', fontsize=12)\n",
        "    axes[0].set_title(f'{phantom_name} - PSNR DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    # SSIM histogram\n",
        "    axes[1].hist(ssim_scores, bins=30, color='forestgreen', edgecolor='black', alpha=0.7)\n",
        "    axes[1].axvline(np.mean(ssim_scores), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Ortalama: {np.mean(ssim_scores):.4f}')\n",
        "    axes[1].set_xlabel('SSIM', fontsize=12)\n",
        "    axes[1].set_ylabel('Slice SayÄ±sÄ±', fontsize=12)\n",
        "    axes[1].set_title(f'{phantom_name} - SSIM DaÄŸÄ±lÄ±mÄ±', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Kaydet\n",
        "    save_path = os.path.join(output_dir, f\"{phantom_name}_metrics_distribution.png\")\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"  ğŸ’¾ Metrik daÄŸÄ±lÄ±mÄ± kaydedildi: {os.path.basename(save_path)}\")\n",
        "    plt.close()\n",
        "\n",
        "def plot_slice_metrics(psnr_scores, ssim_scores, phantom_name, output_dir):\n",
        "    \"\"\"Slice bazÄ±nda metrik deÄŸiÅŸimini gÃ¶sterir\"\"\"\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(14, 8))\n",
        "\n",
        "    slice_numbers = np.arange(len(psnr_scores))\n",
        "\n",
        "    # PSNR over slices\n",
        "    axes[0].plot(slice_numbers, psnr_scores, color='steelblue', linewidth=1.5, alpha=0.7)\n",
        "    axes[0].axhline(np.mean(psnr_scores), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Ortalama: {np.mean(psnr_scores):.2f} dB')\n",
        "    axes[0].fill_between(slice_numbers, psnr_scores, alpha=0.3, color='steelblue')\n",
        "    axes[0].set_xlabel('Slice NumarasÄ±', fontsize=12)\n",
        "    axes[0].set_ylabel('PSNR (dB)', fontsize=12)\n",
        "    axes[0].set_title(f'{phantom_name} - Slice BazÄ±nda PSNR', fontsize=14, fontweight='bold')\n",
        "    axes[0].legend()\n",
        "    axes[0].grid(alpha=0.3)\n",
        "\n",
        "    # SSIM over slices\n",
        "    axes[1].plot(slice_numbers, ssim_scores, color='forestgreen', linewidth=1.5, alpha=0.7)\n",
        "    axes[1].axhline(np.mean(ssim_scores), color='red', linestyle='--', linewidth=2,\n",
        "                    label=f'Ortalama: {np.mean(ssim_scores):.4f}')\n",
        "    axes[1].fill_between(slice_numbers, ssim_scores, alpha=0.3, color='forestgreen')\n",
        "    axes[1].set_xlabel('Slice NumarasÄ±', fontsize=12)\n",
        "    axes[1].set_ylabel('SSIM', fontsize=12)\n",
        "    axes[1].set_title(f'{phantom_name} - Slice BazÄ±nda SSIM', fontsize=14, fontweight='bold')\n",
        "    axes[1].legend()\n",
        "    axes[1].grid(alpha=0.3)\n",
        "\n",
        "    plt.tight_layout()\n",
        "\n",
        "    # Kaydet\n",
        "    save_path = os.path.join(output_dir, f\"{phantom_name}_slice_metrics.png\")\n",
        "    plt.savefig(save_path, dpi=150, bbox_inches='tight')\n",
        "    print(f\"  ğŸ’¾ Slice metrikleri kaydedildi: {os.path.basename(save_path)}\")\n",
        "    plt.close()\n",
        "\n",
        "# ==================== MODEL YÃœKLEME ====================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ”§ MODEL HAZIRLANIYOR\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Generator objesi kontrolÃ¼\n",
        "if 'generator' not in dir():\n",
        "    print(\"\\nâš ï¸  UYARI: 'generator' objesi bulunamadÄ±!\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"ğŸ“ GENERATOR MODELÄ°NÄ° TANIMLAYIN\")\n",
        "    print(\"=\"*80)\n",
        "    print(\"\\nLÃ¼tfen test kodunu Ã§alÄ±ÅŸtÄ±rmadan Ã–NCE aÅŸaÄŸÄ±daki kodu Ã§alÄ±ÅŸtÄ±rÄ±n:\\n\")\n",
        "    print(\"```python\")\n",
        "    print(\"# SeÃ§enek 1: EÄŸer build_generator fonksiyonunuz varsa\")\n",
        "    print(\"generator = build_generator()\")\n",
        "    print(\"\")\n",
        "    print(\"# SeÃ§enek 2: EÄŸer model tanÄ±mÄ± baÅŸka bir yerdeyse\")\n",
        "    print(\"from your_model_file import build_generator\")\n",
        "    print(\"generator = build_generator()\")\n",
        "    print(\"\")\n",
        "    print(\"# SeÃ§enek 3: Manuel olarak tanÄ±mlayÄ±n\")\n",
        "    print(\"# generator = tf.keras.models.Sequential([...])\")\n",
        "    print(\"```\")\n",
        "    print(\"\\n\" + \"=\"*80)\n",
        "    print(\"âŒ Test durduruluyor. Ã–nce generator'Ä± tanÄ±mlayÄ±n ve bu kodu tekrar Ã§alÄ±ÅŸtÄ±rÄ±n.\")\n",
        "    print(\"=\"*80)\n",
        "else:\n",
        "    print(\"âœ… Generator objesi bellekte bulundu.\")\n",
        "    print(f\"   Model tipi: {type(generator)}\")\n",
        "\n",
        "    # Model aÄŸÄ±rlÄ±klarÄ±nÄ± yÃ¼kle\n",
        "    generator = load_latest_generator(checkpoint_dir, generator)\n",
        "\n",
        "    if generator is None:\n",
        "        print(\"\\nâŒ Generator modeli yÃ¼klenemedi. Test durduruluyor.\")\n",
        "        print(\"\\nğŸ’¡ OlasÄ± nedenler:\")\n",
        "        print(\"   1. Checkpoint dosyasÄ± bulunamadÄ±\")\n",
        "        print(\"   2. Model mimarisi checkpoint ile uyumsuz\")\n",
        "        print(\"   3. Dosya yolu hatalÄ±\")\n",
        "        print(f\"\\n   Kontrol edin: {checkpoint_dir}\")\n",
        "        exit()\n",
        "\n",
        "# ==================== PHANTOM TESTLERÄ° ====================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ§ª PHANTOM TESTLERÄ° BAÅLIYOR\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "all_results = {}\n",
        "\n",
        "for phantom_name in phantoms:\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ”¬ Phantom: {phantom_name}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    phantom_dir = os.path.join(phantomx_processed_dir, phantom_name)\n",
        "\n",
        "    # NPY dosyalarÄ±nÄ± yÃ¼kle\n",
        "    input_path = os.path.join(phantom_dir, \"input_ldct.npy\")\n",
        "    target_path = os.path.join(phantom_dir, \"target_ndct.npy\")\n",
        "\n",
        "    if not os.path.exists(input_path) or not os.path.exists(target_path):\n",
        "        print(f\"âŒ NPY dosyalarÄ± bulunamadÄ±: {phantom_dir}\")\n",
        "        print(f\"   Kontrol edilen dosyalar:\")\n",
        "        print(f\"   - {input_path}\")\n",
        "        print(f\"   - {target_path}\")\n",
        "        continue\n",
        "\n",
        "    print(f\"ğŸ“‚ Veri yÃ¼kleniyor...\")\n",
        "    input_volume = np.load(input_path)\n",
        "    target_volume = np.load(target_path)\n",
        "\n",
        "    print(f\"  âœ“ Input shape: {input_volume.shape}\")\n",
        "    print(f\"  âœ“ Target shape: {target_volume.shape}\")\n",
        "    print(f\"  âœ“ Input range: [{input_volume.min():.3f}, {input_volume.max():.3f}]\")\n",
        "    print(f\"  âœ“ Target range: [{target_volume.min():.3f}, {target_volume.max():.3f}]\")\n",
        "\n",
        "    # Model input boyutunu kontrol et\n",
        "    model_input_shape = generator.input_shape  # (None, H, W, C)\n",
        "    expected_h = model_input_shape[1]\n",
        "    expected_w = model_input_shape[2]\n",
        "\n",
        "    actual_h = input_volume.shape[1]\n",
        "    actual_w = input_volume.shape[2]\n",
        "\n",
        "    print(f\"\\n  ğŸ” Boyut kontrolÃ¼:\")\n",
        "    print(f\"     Model beklenen boyut: {expected_h}x{expected_w}\")\n",
        "    print(f\"     Veri boyutu: {actual_h}x{actual_w}\")\n",
        "\n",
        "    # EÄŸer boyutlar uyuÅŸmuyorsa resize et\n",
        "    needs_resize = (actual_h != expected_h) or (actual_w != expected_w)\n",
        "    original_input = None\n",
        "    original_target = None\n",
        "\n",
        "    if needs_resize:\n",
        "        print(f\"\\n  ğŸ”„ GÃ¶rÃ¼ntÃ¼ler resize ediliyor: {actual_h}x{actual_w} â†’ {expected_h}x{expected_w}\")\n",
        "\n",
        "        from tensorflow.keras.preprocessing.image import array_to_img, img_to_array\n",
        "        import cv2\n",
        "\n",
        "        # Input volume'u resize et\n",
        "        input_resized = np.zeros((input_volume.shape[0], expected_h, expected_w), dtype=np.float32)\n",
        "        for i in range(input_volume.shape[0]):\n",
        "            input_resized[i] = cv2.resize(input_volume[i], (expected_w, expected_h),\n",
        "                                         interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Target volume'u resize et\n",
        "        target_resized = np.zeros((target_volume.shape[0], expected_h, expected_w), dtype=np.float32)\n",
        "        for i in range(target_volume.shape[0]):\n",
        "            target_resized[i] = cv2.resize(target_volume[i], (expected_w, expected_h),\n",
        "                                          interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Orijinal boyutlarÄ± sakla (sonra tekrar bÃ¼yÃ¼tmek iÃ§in)\n",
        "        original_input = input_volume\n",
        "        original_target = target_volume\n",
        "\n",
        "        input_volume = input_resized\n",
        "        target_volume = target_resized\n",
        "\n",
        "        print(f\"     âœ“ Resize tamamlandÄ±: {input_volume.shape}\")\n",
        "\n",
        "    # Model iÃ§in tahmin yap\n",
        "    num_slices = input_volume.shape[0]\n",
        "    predicted_volume = np.zeros_like(input_volume)\n",
        "\n",
        "    print(f\"\\nğŸ”® Model tahminleri yapÄ±lÄ±yor ({num_slices} slice)...\")\n",
        "\n",
        "    # Progress bar iÃ§in\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    for i in tqdm(range(num_slices), desc=f\"  {phantom_name}\"):\n",
        "        # Slice'Ä± al ve reshape et: (1, H, W, 1)\n",
        "        input_slice = input_volume[i:i+1, :, :]\n",
        "        input_slice = np.expand_dims(input_slice, axis=-1)  # Channel dimension ekle\n",
        "\n",
        "        # Tahmin yap\n",
        "        predicted_slice = generator.predict(input_slice, verbose=0)\n",
        "\n",
        "        # Geri Ã§evir: (H, W)\n",
        "        predicted_volume[i] = predicted_slice[0, :, :, 0]\n",
        "\n",
        "    print(f\"  âœ… TÃ¼m tahminler tamamlandÄ±!\")\n",
        "\n",
        "    # EÄŸer resize yaptÄ±ysak, tahminleri orijinal boyuta geri getir\n",
        "    if needs_resize:\n",
        "        print(f\"\\n  ğŸ”„ Tahminler orijinal boyuta geri getiriliyor: {expected_h}x{expected_w} â†’ {actual_h}x{actual_w}\")\n",
        "\n",
        "        predicted_original_size = np.zeros_like(original_input, dtype=np.float32)\n",
        "        for i in range(predicted_volume.shape[0]):\n",
        "            predicted_original_size[i] = cv2.resize(predicted_volume[i], (actual_w, actual_h),\n",
        "                                                    interpolation=cv2.INTER_LINEAR)\n",
        "\n",
        "        # Metrikleri hesaplamak iÃ§in orijinal boyutlarÄ± kullan\n",
        "        input_for_metrics = original_input\n",
        "        target_for_metrics = original_target\n",
        "        predicted_for_metrics = predicted_original_size\n",
        "\n",
        "        # GÃ¶rselleÅŸtirme iÃ§in resize edilmiÅŸ versiyonlarÄ± kullan (daha hÄ±zlÄ±)\n",
        "        input_for_viz = input_volume\n",
        "        target_for_viz = target_volume\n",
        "        predicted_for_viz = predicted_volume\n",
        "\n",
        "        print(f\"     âœ“ Geri getirme tamamlandÄ±: {predicted_original_size.shape}\")\n",
        "    else:\n",
        "        input_for_metrics = input_volume\n",
        "        target_for_metrics = target_volume\n",
        "        predicted_for_metrics = predicted_volume\n",
        "\n",
        "        input_for_viz = input_volume\n",
        "        target_for_viz = target_volume\n",
        "        predicted_for_viz = predicted_volume\n",
        "\n",
        "    # Metrikleri hesapla\n",
        "    print(f\"\\nğŸ“Š Metrikler hesaplanÄ±yor...\")\n",
        "    psnr_scores, ssim_scores = calculate_metrics_3d(target_for_metrics, predicted_for_metrics)\n",
        "\n",
        "    # Ä°statistikleri kaydet\n",
        "    results = {\n",
        "        'phantom': phantom_name,\n",
        "        'num_slices': num_slices,\n",
        "        'psnr_mean': np.mean(psnr_scores),\n",
        "        'psnr_std': np.std(psnr_scores),\n",
        "        'psnr_min': np.min(psnr_scores),\n",
        "        'psnr_max': np.max(psnr_scores),\n",
        "        'psnr_median': np.median(psnr_scores),\n",
        "        'ssim_mean': np.mean(ssim_scores),\n",
        "        'ssim_std': np.std(ssim_scores),\n",
        "        'ssim_min': np.min(ssim_scores),\n",
        "        'ssim_max': np.max(ssim_scores),\n",
        "        'ssim_median': np.median(ssim_scores)\n",
        "    }\n",
        "\n",
        "    all_results[phantom_name] = results\n",
        "\n",
        "    # SonuÃ§larÄ± yazdÄ±r\n",
        "    print(f\"\\n{'â”€'*80}\")\n",
        "    print(f\"  ğŸ“ˆ SONUÃ‡LAR - {phantom_name}\")\n",
        "    print(f\"{'â”€'*80}\")\n",
        "    print(f\"  Toplam Slice: {num_slices}\")\n",
        "    print(f\"\\n  PSNR:\")\n",
        "    print(f\"    Ortalama: {results['psnr_mean']:.2f} dB (Â±{results['psnr_std']:.2f})\")\n",
        "    print(f\"    Medyan:   {results['psnr_median']:.2f} dB\")\n",
        "    print(f\"    Min/Max:  {results['psnr_min']:.2f} / {results['psnr_max']:.2f} dB\")\n",
        "    print(f\"\\n  SSIM:\")\n",
        "    print(f\"    Ortalama: {results['ssim_mean']:.4f} (Â±{results['ssim_std']:.4f})\")\n",
        "    print(f\"    Medyan:   {results['ssim_median']:.4f}\")\n",
        "    print(f\"    Min/Max:  {results['ssim_min']:.4f} / {results['ssim_max']:.4f}\")\n",
        "    print(f\"{'â”€'*80}\\n\")\n",
        "\n",
        "    # GÃ¶rselleÅŸtirmeler\n",
        "    print(f\"ğŸ¨ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...\")\n",
        "    visualize_results(input_for_viz, target_for_viz, predicted_for_viz,\n",
        "                     phantom_name, output_dir, num_slices=5)\n",
        "    plot_metrics_distribution(psnr_scores, ssim_scores, phantom_name, output_dir)\n",
        "    plot_slice_metrics(psnr_scores, ssim_scores, phantom_name, output_dir)\n",
        "\n",
        "    # Tahminleri kaydet (orijinal boyutta)\n",
        "    pred_save_path = os.path.join(output_dir, f\"{phantom_name}_predicted.npy\")\n",
        "    np.save(pred_save_path, predicted_for_metrics)\n",
        "    print(f\"  ğŸ’¾ Tahminler kaydedildi: {os.path.basename(pred_save_path)}\\n\")\n",
        "\n",
        "# ==================== GENEL RAPOR ====================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ğŸ“‹ GENEL DEÄERLENDIRME RAPORU\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "if len(all_results) == 0:\n",
        "    print(\"\\nâŒ HiÃ§bir phantom iÃ§in test yapÄ±lamadÄ±!\")\n",
        "else:\n",
        "    # Her phantom iÃ§in Ã¶zet\n",
        "    for phantom_name, results in all_results.items():\n",
        "        print(f\"\\nğŸ”¬ {phantom_name}:\")\n",
        "        print(f\"  ğŸ“Š PSNR: {results['psnr_mean']:.2f} Â± {results['psnr_std']:.2f} dB\")\n",
        "        print(f\"  ğŸ“Š SSIM: {results['ssim_mean']:.4f} Â± {results['ssim_std']:.4f}\")\n",
        "\n",
        "    # Ortalama sonuÃ§lar (tÃ¼m phantomlar)\n",
        "    if len(all_results) > 1:\n",
        "        avg_psnr = np.mean([r['psnr_mean'] for r in all_results.values()])\n",
        "        avg_ssim = np.mean([r['ssim_mean'] for r in all_results.values()])\n",
        "\n",
        "        print(f\"\\n{'â”€'*80}\")\n",
        "        print(f\"ğŸ“Š TÃœM PHANTOM ORTALAMASI:\")\n",
        "        print(f\"  PSNR: {avg_psnr:.2f} dB\")\n",
        "        print(f\"  SSIM: {avg_ssim:.4f}\")\n",
        "        print(f\"{'â”€'*80}\")\n",
        "\n",
        "    # DosyalarÄ± listele\n",
        "    print(f\"\\nğŸ“ Ã‡IKTI DOSYALARI ({output_dir}):\")\n",
        "    for phantom_name in all_results.keys():\n",
        "        print(f\"\\n  {phantom_name}/\")\n",
        "        print(f\"    â”œâ”€â”€ {phantom_name}_comparison.png          (GÃ¶rsel karÅŸÄ±laÅŸtÄ±rma)\")\n",
        "        print(f\"    â”œâ”€â”€ {phantom_name}_metrics_distribution.png (Metrik daÄŸÄ±lÄ±mlarÄ±)\")\n",
        "        print(f\"    â”œâ”€â”€ {phantom_name}_slice_metrics.png       (Slice bazÄ±nda trend)\")\n",
        "        print(f\"    â””â”€â”€ {phantom_name}_predicted.npy           (Tahmin edilen volume)\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"âœ… TEST TAMAMLANDI!\")\n",
        "print(\"=\" * 80)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4d1IDjV_Qn2R",
        "outputId": "49c3f3c4-ed94-4699-ff9a-1eeb99d53d8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "PhantomX Model Test ve DeÄŸerlendirme\n",
            "================================================================================\n",
            "ğŸ“‚ Processed data: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Output\n",
            "ğŸ§  Model checkpoints: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Projects/model_checkpoints\n",
            "ğŸ“Š Results output: /content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Results2\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸ”§ MODEL HAZIRLANIYOR\n",
            "================================================================================\n",
            "âœ… Generator objesi bellekte bulundu.\n",
            "   Model tipi: <class 'keras.src.models.functional.Functional'>\n",
            "\n",
            "ğŸ“¦ Model bulundu ve yÃ¼kleniyor...\n",
            "   Dosya: G_epoch_50.h5\n",
            "   Epoch: 50\n",
            "   Toplam 10 checkpoint bulundu\n",
            "âœ… Model baÅŸarÄ±yla yÃ¼klendi!\n",
            "\n",
            "================================================================================\n",
            "ğŸ§ª PHANTOM TESTLERÄ° BAÅLIYOR\n",
            "================================================================================\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¬ Phantom: D55-01\n",
            "================================================================================\n",
            "ğŸ“‚ Veri yÃ¼kleniyor...\n",
            "  âœ“ Input shape: (376, 512, 512)\n",
            "  âœ“ Target shape: (376, 512, 512)\n",
            "  âœ“ Input range: [0.024, 1.000]\n",
            "  âœ“ Target range: [0.024, 1.000]\n",
            "\n",
            "  ğŸ” Boyut kontrolÃ¼:\n",
            "     Model beklenen boyut: 256x256\n",
            "     Veri boyutu: 512x512\n",
            "\n",
            "  ğŸ”„ GÃ¶rÃ¼ntÃ¼ler resize ediliyor: 512x512 â†’ 256x256\n",
            "     âœ“ Resize tamamlandÄ±: (376, 256, 256)\n",
            "\n",
            "ğŸ”® Model tahminleri yapÄ±lÄ±yor (376 slice)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  D55-01: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 376/376 [00:42<00:00,  8.85it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  âœ… TÃ¼m tahminler tamamlandÄ±!\n",
            "\n",
            "  ğŸ”„ Tahminler orijinal boyuta geri getiriliyor: 256x256 â†’ 512x512\n",
            "     âœ“ Geri getirme tamamlandÄ±: (376, 512, 512)\n",
            "\n",
            "ğŸ“Š Metrikler hesaplanÄ±yor...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  ğŸ“ˆ SONUÃ‡LAR - D55-01\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  Toplam Slice: 376\n",
            "\n",
            "  PSNR:\n",
            "    Ortalama: 36.76 dB (Â±1.20)\n",
            "    Medyan:   37.09 dB\n",
            "    Min/Max:  30.59 / 37.49 dB\n",
            "\n",
            "  SSIM:\n",
            "    Ortalama: 0.9682 (Â±0.0258)\n",
            "    Medyan:   0.9744\n",
            "    Min/Max:  0.8157 / 0.9787\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ¨ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...\n",
            "  ğŸ’¾ GÃ¶rselleÅŸtirme kaydedildi: D55-01_comparison.png\n",
            "  ğŸ’¾ Metrik daÄŸÄ±lÄ±mÄ± kaydedildi: D55-01_metrics_distribution.png\n",
            "  ğŸ’¾ Slice metrikleri kaydedildi: D55-01_slice_metrics.png\n",
            "  ğŸ’¾ Tahminler kaydedildi: D55-01_predicted.npy\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ”¬ Phantom: D55-02\n",
            "================================================================================\n",
            "ğŸ“‚ Veri yÃ¼kleniyor...\n",
            "  âœ“ Input shape: (363, 512, 512)\n",
            "  âœ“ Target shape: (363, 512, 512)\n",
            "  âœ“ Input range: [0.024, 1.000]\n",
            "  âœ“ Target range: [0.024, 1.000]\n",
            "\n",
            "  ğŸ” Boyut kontrolÃ¼:\n",
            "     Model beklenen boyut: 256x256\n",
            "     Veri boyutu: 512x512\n",
            "\n",
            "  ğŸ”„ GÃ¶rÃ¼ntÃ¼ler resize ediliyor: 512x512 â†’ 256x256\n",
            "     âœ“ Resize tamamlandÄ±: (363, 256, 256)\n",
            "\n",
            "ğŸ”® Model tahminleri yapÄ±lÄ±yor (363 slice)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  D55-02: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 363/363 [00:32<00:00, 11.02it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  âœ… TÃ¼m tahminler tamamlandÄ±!\n",
            "\n",
            "  ğŸ”„ Tahminler orijinal boyuta geri getiriliyor: 256x256 â†’ 512x512\n",
            "     âœ“ Geri getirme tamamlandÄ±: (363, 512, 512)\n",
            "\n",
            "ğŸ“Š Metrikler hesaplanÄ±yor...\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  ğŸ“ˆ SONUÃ‡LAR - D55-02\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "  Toplam Slice: 363\n",
            "\n",
            "  PSNR:\n",
            "    Ortalama: 36.79 dB (Â±1.38)\n",
            "    Medyan:   37.15 dB\n",
            "    Min/Max:  28.26 / 38.78 dB\n",
            "\n",
            "  SSIM:\n",
            "    Ortalama: 0.9688 (Â±0.0258)\n",
            "    Medyan:   0.9732\n",
            "    Min/Max:  0.7700 / 0.9796\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ¨ GÃ¶rselleÅŸtirmeler oluÅŸturuluyor...\n",
            "  ğŸ’¾ GÃ¶rselleÅŸtirme kaydedildi: D55-02_comparison.png\n",
            "  ğŸ’¾ Metrik daÄŸÄ±lÄ±mÄ± kaydedildi: D55-02_metrics_distribution.png\n",
            "  ğŸ’¾ Slice metrikleri kaydedildi: D55-02_slice_metrics.png\n",
            "  ğŸ’¾ Tahminler kaydedildi: D55-02_predicted.npy\n",
            "\n",
            "\n",
            "================================================================================\n",
            "ğŸ“‹ GENEL DEÄERLENDIRME RAPORU\n",
            "================================================================================\n",
            "\n",
            "ğŸ”¬ D55-01:\n",
            "  ğŸ“Š PSNR: 36.76 Â± 1.20 dB\n",
            "  ğŸ“Š SSIM: 0.9682 Â± 0.0258\n",
            "\n",
            "ğŸ”¬ D55-02:\n",
            "  ğŸ“Š PSNR: 36.79 Â± 1.38 dB\n",
            "  ğŸ“Š SSIM: 0.9688 Â± 0.0258\n",
            "\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "ğŸ“Š TÃœM PHANTOM ORTALAMASI:\n",
            "  PSNR: 36.77 dB\n",
            "  SSIM: 0.9685\n",
            "â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
            "\n",
            "ğŸ“ Ã‡IKTI DOSYALARI (/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Results2):\n",
            "\n",
            "  D55-01/\n",
            "    â”œâ”€â”€ D55-01_comparison.png          (GÃ¶rsel karÅŸÄ±laÅŸtÄ±rma)\n",
            "    â”œâ”€â”€ D55-01_metrics_distribution.png (Metrik daÄŸÄ±lÄ±mlarÄ±)\n",
            "    â”œâ”€â”€ D55-01_slice_metrics.png       (Slice bazÄ±nda trend)\n",
            "    â””â”€â”€ D55-01_predicted.npy           (Tahmin edilen volume)\n",
            "\n",
            "  D55-02/\n",
            "    â”œâ”€â”€ D55-02_comparison.png          (GÃ¶rsel karÅŸÄ±laÅŸtÄ±rma)\n",
            "    â”œâ”€â”€ D55-02_metrics_distribution.png (Metrik daÄŸÄ±lÄ±mlarÄ±)\n",
            "    â”œâ”€â”€ D55-02_slice_metrics.png       (Slice bazÄ±nda trend)\n",
            "    â””â”€â”€ D55-02_predicted.npy           (Tahmin edilen volume)\n",
            "\n",
            "================================================================================\n",
            "âœ… TEST TAMAMLANDI!\n",
            "================================================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Error Handling"
      ],
      "metadata": {
        "id": "TItYogxM8bIx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pydicom\n",
        "import os\n",
        "import glob\n",
        "\n",
        "# YollarÄ±nÄ±zÄ± buraya tanÄ±mlayÄ±n\n",
        "mayo_ornek = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Hata ayÄ±klama/1-0001.dcm\" # Mayo'dan rastgele bir dosya\n",
        "phantom_ornek = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/phantomx_abdomen_pelvis_dataset/D55-01/40/FBP_FC08_40_152622.890/IM-0025-0333.dcm\" # PhantomX'ten rastgele bir dosya (veya NPY Ã¶ncesi hali)\n",
        "\n",
        "def check_dicom_header(path, name):\n",
        "    if not os.path.exists(path): return\n",
        "    ds = pydicom.dcmread(path)\n",
        "    print(f\"--- {name} ---\")\n",
        "    print(f\"Intercept: {ds.RescaleIntercept if 'RescaleIntercept' in ds else 'YOK'}\")\n",
        "    print(f\"Slope:     {ds.RescaleSlope if 'RescaleSlope' in ds else 'YOK'}\")\n",
        "    raw_min = ds.pixel_array.min()\n",
        "    raw_max = ds.pixel_array.max()\n",
        "    print(f\"Raw Min/Max: {raw_min} / {raw_max}\")\n",
        "\n",
        "    # GerÃ§ek HU deÄŸerlerini hesapla\n",
        "    slope = ds.RescaleSlope if 'RescaleSlope' in ds else 1\n",
        "    intercept = ds.RescaleIntercept if 'RescaleIntercept' in ds else 0\n",
        "    hu_min = raw_min * slope + intercept\n",
        "    hu_max = raw_max * slope + intercept\n",
        "    print(f\"GerÃ§ek HU Min/Max: {hu_min} / {hu_max}\\n\")\n",
        "\n",
        "check_dicom_header(mayo_ornek, \"MAYO CLINIC\")\n",
        "check_dicom_header(phantom_ornek, \"PHANTOMX\")"
      ],
      "metadata": {
        "id": "gjuw5WX98gsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pydicom\n",
        "import os\n",
        "from google.colab import drive\n",
        "\n",
        "# 1. Google Drive'Ä± BaÄŸla (EÄŸer baÄŸlÄ± deÄŸilse pencere aÃ§Ä±lÄ±r)\n",
        "if not os.path.exists('/content/drive'):\n",
        "    print(\"Google Drive baÄŸlanÄ±yor...\")\n",
        "    drive.mount('/content/drive')\n",
        "else:\n",
        "    print(\"âœ… Google Drive zaten baÄŸlÄ±.\")\n",
        "\n",
        "# 2. Dosya YollarÄ± (Sizin verdiÄŸiniz yollar)\n",
        "mayo_ornek = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/Hata ayÄ±klama/1-0001.dcm\"\n",
        "phantom_ornek = r\"/content/drive/MyDrive/Dersler/TasarÄ±m Dersi/Phantomx/archive/phantomx_abdomen_pelvis_dataset/D55-01/40/FBP_FC08_40_152622.890/IM-0025-0333.dcm\"\n",
        "\n",
        "def check_dicom_header(path, name):\n",
        "    print(f\"\\nğŸ” {name} aranÄ±yor...\")\n",
        "\n",
        "    # --- KONTROL KISMI ---\n",
        "    if not os.path.exists(path):\n",
        "        print(f\"âŒ HATA: Dosya bulunamadÄ±!\")\n",
        "        print(f\"   Aranan Yol: {path}\")\n",
        "\n",
        "        # KlasÃ¶r var mÄ± diye bakalÄ±m (Dosya adÄ± mÄ± yanlÄ±ÅŸ?)\n",
        "        folder = os.path.dirname(path)\n",
        "        if os.path.exists(folder):\n",
        "            print(f\"   âœ… KlasÃ¶r mevcut: {folder}\")\n",
        "            print(f\"   ğŸ“‚ KlasÃ¶r iÃ§indeki ilk 3 dosya:\")\n",
        "            files = os.listdir(folder)\n",
        "            for f in files[:3]: print(f\"      - {f}\")\n",
        "        else:\n",
        "            print(f\"   âŒ KlasÃ¶r de bulunamadÄ±. Yol tamamen yanlÄ±ÅŸ olabilir.\")\n",
        "        return\n",
        "    # ---------------------\n",
        "\n",
        "    ds = pydicom.dcmread(path)\n",
        "    print(f\"âœ… Dosya bulundu. Analiz ediliyor...\")\n",
        "    print(f\"-\"*30)\n",
        "\n",
        "    # Header Bilgileri\n",
        "    intercept = ds.RescaleIntercept if 'RescaleIntercept' in ds else 0\n",
        "    slope = ds.RescaleSlope if 'RescaleSlope' in ds else 1\n",
        "\n",
        "    print(f\"Rescale Intercept: {intercept}\")\n",
        "    print(f\"Rescale Slope:     {slope}\")\n",
        "\n",
        "    raw_min = ds.pixel_array.min()\n",
        "    raw_max = ds.pixel_array.max()\n",
        "    print(f\"Raw Pixel Min/Max: {raw_min} / {raw_max}\")\n",
        "\n",
        "    # GerÃ§ek HU\n",
        "    hu_min = raw_min * slope + intercept\n",
        "    hu_max = raw_max * slope + intercept\n",
        "    print(f\"GerÃ§ek HU Min/Max: {hu_min:.1f} / {hu_max:.1f}\")\n",
        "\n",
        "    # YORUM\n",
        "    if intercept == -1024:\n",
        "        print(\"ğŸ’¡ YORUM: Bu veri 'Offset' kullanÄ±yor. (Tipik Mayo/Siemens/GE)\")\n",
        "        print(\"   EÄŸitimde bu -1024'Ã¼ hesaba katmadÄ±ysanÄ±z model hatalÄ± Ã¶ÄŸrendi.\")\n",
        "    elif intercept == 0:\n",
        "        print(\"ğŸ’¡ YORUM: Bu veri zaten HU'ya yakÄ±n veya farklÄ± formatta.\")\n",
        "\n",
        "# Analizi BaÅŸlat\n",
        "check_dicom_header(mayo_ornek, \"MAYO CLINIC\")\n",
        "check_dicom_header(phantom_ornek, \"PHANTOMX\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hh2PHDmLAd7G",
        "outputId": "7f6b141d-cc6c-41c7-da45-d016ce3ffaf0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive baÄŸlanÄ±yor...\n",
            "Mounted at /content/drive\n",
            "\n",
            "ğŸ” MAYO CLINIC aranÄ±yor...\n",
            "âœ… Dosya bulundu. Analiz ediliyor...\n",
            "------------------------------\n",
            "Rescale Intercept: -0.9999866485596\n",
            "Rescale Slope:     0.00032043788815\n",
            "Raw Pixel Min/Max: 202 / 61026\n",
            "GerÃ§ek HU Min/Max: -0.9 / 18.6\n",
            "\n",
            "ğŸ” PHANTOMX aranÄ±yor...\n",
            "âœ… Dosya bulundu. Analiz ediliyor...\n",
            "------------------------------\n",
            "Rescale Intercept: 0\n",
            "Rescale Slope:     1\n",
            "Raw Pixel Min/Max: -2048 / 1521\n",
            "GerÃ§ek HU Min/Max: -2048.0 / 1521.0\n",
            "ğŸ’¡ YORUM: Bu veri zaten HU'ya yakÄ±n veya farklÄ± formatta.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "TItYogxM8bIx"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}