{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# External Test: PhantomX Dataset\n",
                "\n",
                "Bu notebook, eƒüitim verisinden tamamen baƒüƒ±msƒ±z olan PhantomX datasetinde modeli test eder.\n",
                "\n",
                "**PhantomX Dataset:**\n",
                "- 2 phantom (D55-01, D55-02)\n",
                "- Her biri i√ßin d√º≈ü√ºk doz (40 mAs) ve y√ºksek doz (300 mAs) g√∂r√ºnt√ºler\n",
                "- 3 farklƒ± rekonstr√ºksiyon: FBP, AIDR3D, AiCE\n",
                "\n",
                "---\n",
                "\n",
                "## ƒ∞≈ü Akƒ±≈üƒ±\n",
                "\n",
                "1. PhantomX DICOM ‚Üí NPY d√∂n√º≈ü√ºm√º\n",
                "2. Model ile denoising\n",
                "3. PSNR/SSIM hesaplama\n",
                "4. G√∂rsel kar≈üƒ±la≈ütƒ±rma"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "!pip install pydicom -q"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "from google.colab import drive\n",
                "drive.mount('/content/drive')"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Model'i Y√ºkle"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "%run 01_model_architecture.ipynb"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import os\n",
                "import glob\n",
                "\n",
                "checkpoint_dir = \"/content/drive/MyDrive/Tasarƒ±m Dersi/Projects/model_checkpoints\"\n",
                "\n",
                "list_of_files = glob.glob(os.path.join(checkpoint_dir, 'G_epoch_*.h5'))\n",
                "latest_file = max(list_of_files, key=os.path.getctime)\n",
                "print(f\"Y√ºklenen Model: {os.path.basename(latest_file)}\")\n",
                "\n",
                "generator = build_generator()\n",
                "generator.load_weights(latest_file)\n",
                "print(\"‚úÖ Model hazƒ±r.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. PhantomX DICOM ‚Üí NPY D√∂n√º≈ü√ºm√º\n",
                "\n",
                "Mayo dataset ile aynƒ± preprocessing adƒ±mlarƒ±nƒ± uyguluyoruz:\n",
                "- HU d√∂n√º≈ü√ºm√º\n",
                "- [-1000, 1000] clip\n",
                "- [-1, 1] normalizasyon"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import numpy as np\n",
                "import pydicom\n",
                "\n",
                "# Ayarlar\n",
                "phantomx_base_dir = \"/content/drive/MyDrive/Dersler/Tasarƒ±m Dersi/Phantomx/archive/phantomx_abdomen_pelvis_dataset\"\n",
                "output_dir = \"/content/drive/MyDrive/Dersler/Tasarƒ±m Dersi/Phantomx/archive/Output\"\n",
                "\n",
                "phantoms = [\"D55-01\", \"D55-02\"]\n",
                "\n",
                "print(\"=\"*80)\n",
                "print(\"PhantomX DICOM ‚Üí NPY D√∂n√º≈üt√ºr√ºc√º\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def dicom_klasoru_oku(klasor_yolu, klasor_adi):\n",
                "    \"\"\"DICOM dosyalarƒ±nƒ± okur ve 3D numpy array olarak d√∂nd√ºr√ºr\"\"\"\n",
                "    print(f\"      üìÇ {klasor_adi}\")\n",
                "\n",
                "    dicom_dosyalari = []\n",
                "\n",
                "    for dosya in os.listdir(klasor_yolu):\n",
                "        if dosya.endswith('.dcm'):\n",
                "            dosya_yolu = os.path.join(klasor_yolu, dosya)\n",
                "            dicom_dosyalari.append(dosya_yolu)\n",
                "\n",
                "    if not dicom_dosyalari:\n",
                "        print(f\"      ‚ùå DICOM dosyasƒ± bulunamadƒ±!\")\n",
                "        return None\n",
                "\n",
                "    print(f\"      ‚úì {len(dicom_dosyalari)} DICOM dosyasƒ± bulundu\")\n",
                "\n",
                "    # DICOM dosyalarƒ±nƒ± oku ve sƒ±rala\n",
                "    dicom_veriler = []\n",
                "\n",
                "    for dosya in sorted(dicom_dosyalari):\n",
                "        try:\n",
                "            ds = pydicom.dcmread(dosya)\n",
                "\n",
                "            pixel_array = ds.pixel_array.astype(np.float32)\n",
                "\n",
                "            # HU D√∂n√º≈ü√ºm√º\n",
                "            intercept = ds.RescaleIntercept if 'RescaleIntercept' in ds else 0\n",
                "            slope = ds.RescaleSlope if 'RescaleSlope' in ds else 1\n",
                "            pixel_array = pixel_array * slope + intercept\n",
                "\n",
                "            # Clip\n",
                "            pixel_array = np.clip(pixel_array, -1000, 1000)\n",
                "\n",
                "            # Z koordinatƒ±\n",
                "            if hasattr(ds, 'ImagePositionPatient'):\n",
                "                slice_location = float(ds.ImagePositionPatient[2])\n",
                "            elif hasattr(ds, 'SliceLocation'):\n",
                "                slice_location = float(ds.SliceLocation)\n",
                "            elif hasattr(ds, 'InstanceNumber'):\n",
                "                slice_location = float(ds.InstanceNumber)\n",
                "            else:\n",
                "                slice_location = len(dicom_veriler)\n",
                "\n",
                "            dicom_veriler.append((slice_location, pixel_array))\n",
                "\n",
                "        except Exception as e:\n",
                "            continue\n",
                "\n",
                "    # Z koordinatƒ±na g√∂re sƒ±rala\n",
                "    dicom_veriler.sort(key=lambda x: x[0])\n",
                "\n",
                "    # 3D array olu≈ütur\n",
                "    volume_3d = np.stack([slice_data for _, slice_data in dicom_veriler])\n",
                "\n",
                "    print(f\"      ‚úì Volume: {volume_3d.shape}\")\n",
                "\n",
                "    return volume_3d\n",
                "\n",
                "\n",
                "def normalizasyon(volume, min_hu=-1000, max_hu=1000):\n",
                "    \"\"\"HU deƒüerlerini [-1, 1] aralƒ±ƒüƒ±na normalize eder\"\"\"\n",
                "    volume = np.clip(volume, min_hu, max_hu)\n",
                "    volume = (volume - min_hu) / (max_hu - min_hu)  # [0, 1]\n",
                "    volume = volume * 2 - 1  # [-1, 1]\n",
                "    return volume.astype(np.float32)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Phantom Test\n",
                "\n",
                "Her phantom i√ßin:\n",
                "1. D√º≈ü√ºk doz g√∂r√ºnt√ºleri al\n",
                "2. Model ile iyile≈ütir\n",
                "3. Y√ºksek doz referans ile kar≈üƒ±la≈ütƒ±r"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import tensorflow as tf\n",
                "import cv2\n",
                "import matplotlib.pyplot as plt\n",
                "\n",
                "# FBP rekonstr√ºksiyon kullanƒ±yoruz (model bununla eƒüitildi)\n",
                "rekon_secim = \"FBP\"\n",
                "\n",
                "for phantom_name in phantoms:\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"üî¨ Phantom: {phantom_name}\")\n",
                "    print('='*80)\n",
                "    \n",
                "    phantom_path = os.path.join(phantomx_base_dir, phantom_name)\n",
                "    \n",
                "    # Low dose (40) ve High dose (300) klas√∂rlerini bul\n",
                "    low_dose_path = None\n",
                "    high_dose_path = None\n",
                "    \n",
                "    for dose_folder in os.listdir(phantom_path):\n",
                "        dose_path = os.path.join(phantom_path, dose_folder)\n",
                "        if not os.path.isdir(dose_path):\n",
                "            continue\n",
                "        \n",
                "        # FBP klas√∂r√ºn√º bul\n",
                "        for rekon_folder in os.listdir(dose_path):\n",
                "            if rekon_secim.upper() in rekon_folder.upper():\n",
                "                if dose_folder == \"40\":\n",
                "                    low_dose_path = os.path.join(dose_path, rekon_folder)\n",
                "                elif dose_folder == \"300\":\n",
                "                    high_dose_path = os.path.join(dose_path, rekon_folder)\n",
                "    \n",
                "    if not low_dose_path or not high_dose_path:\n",
                "        print(f\"  ‚ùå Klas√∂rler bulunamadƒ±!\")\n",
                "        continue\n",
                "    \n",
                "    # DICOM oku\n",
                "    print(f\"\\n  üì• Low Dose okuyuyor...\")\n",
                "    low_volume = dicom_klasoru_oku(low_dose_path, os.path.basename(low_dose_path))\n",
                "    \n",
                "    print(f\"\\n  üì§ High Dose okuyuyor...\")\n",
                "    high_volume = dicom_klasoru_oku(high_dose_path, os.path.basename(high_dose_path))\n",
                "    \n",
                "    if low_volume is None or high_volume is None:\n",
                "        continue\n",
                "    \n",
                "    # Normalizasyon\n",
                "    low_norm = normalizasyon(low_volume)\n",
                "    high_norm = normalizasyon(high_volume)\n",
                "    \n",
                "    # Resize (model 256x256 bekliyor)\n",
                "    low_resized = np.array([cv2.resize(s, (256, 256)) for s in low_norm])\n",
                "    high_resized = np.array([cv2.resize(s, (256, 256)) for s in high_norm])\n",
                "    \n",
                "    # Channel ekle\n",
                "    low_input = low_resized[..., np.newaxis]\n",
                "    high_target = high_resized[..., np.newaxis]\n",
                "    \n",
                "    # Inference\n",
                "    print(f\"\\n  üîÑ Model √ßalƒ±≈ütƒ±rƒ±lƒ±yor...\")\n",
                "    predictions = generator(low_input, training=False)\n",
                "    \n",
                "    # PSNR/SSIM\n",
                "    psnr = tf.image.psnr(high_target, predictions, max_val=2.0).numpy()\n",
                "    ssim = tf.image.ssim(high_target, predictions, max_val=2.0).numpy()\n",
                "    \n",
                "    print(f\"\\n  üìä Sonu√ßlar:\")\n",
                "    print(f\"     PSNR: {np.mean(psnr):.4f} dB (¬±{np.std(psnr):.4f})\")\n",
                "    print(f\"     SSIM: {np.mean(ssim):.4f} (¬±{np.std(ssim):.4f})\")\n",
                "    \n",
                "    # √ñrnek g√∂rsel\n",
                "    mid_slice = len(low_input) // 2\n",
                "    \n",
                "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
                "    \n",
                "    axes[0].imshow(low_input[mid_slice, :, :, 0], cmap='gray')\n",
                "    axes[0].set_title('Low Dose (Input)')\n",
                "    axes[0].axis('off')\n",
                "    \n",
                "    axes[1].imshow(predictions[mid_slice, :, :, 0], cmap='gray')\n",
                "    axes[1].set_title('AI Enhanced')\n",
                "    axes[1].axis('off')\n",
                "    \n",
                "    axes[2].imshow(high_target[mid_slice, :, :, 0], cmap='gray')\n",
                "    axes[2].set_title('High Dose (Reference)')\n",
                "    axes[2].axis('off')\n",
                "    \n",
                "    plt.suptitle(f'{phantom_name} - Slice {mid_slice}', fontsize=14)\n",
                "    plt.tight_layout()\n",
                "    plt.savefig(f'{phantom_name}_comparison.png', dpi=150)\n",
                "    plt.show()"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.9.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}